{"attributes":{"backlinks":[],"path":"/Users/lorenz/.julia/packages/Surrogates/zwNvG/docs/src/tutorials.md","title":"tutorials"},"tag":"document","children":[{"attributes":{},"tag":"md","children":[{"attributes":{},"tag":"h2","children":["Surrogates 101"],"type":"node"},{"attributes":{},"tag":"p","children":["Let's start with something easy to get our hands dirty. I want to build a surrogate for ",{"attributes":{},"tag":"math","children":["f(x) = \\log(x) \\cdot x^2+x^3"],"type":"node"},". Let's choose the radial basis surrogate."],"type":"node"},{"attributes":{"lang":"@example"},"tag":"codeblock","children":["using Surrogates\nf = x -> log(x)*x^2+x^3\nlb = 1.0\nub = 10.0\nx = sample(50,lb,ub,SobolSample())\ny = f.(x)\nmy_radial_basis = RadialBasis(x,y,lb,ub)\n\n#I want an approximation at 5.4\napprox = my_radial_basis(5.4)\n"],"type":"node"},{"attributes":{},"tag":"p","children":["Let's now see an example in 2D."],"type":"node"},{"attributes":{"lang":"@example"},"tag":"codeblock","children":["using Surrogates\nusing LinearAlgebra\nf = x -> x[1]*x[2]\nlb = [1.0,2.0]\nub = [10.0,8.5]\nx = sample(50,lb,ub,SobolSample())\ny = f.(x)\nmy_radial_basis = RadialBasis(x,y,lb,ub)\n\n#I want an approximation at (1.0,1.4)\napprox = my_radial_basis((1.0,1.4))\n"],"type":"node"},{"attributes":{},"tag":"h2","children":["Kriging standard error"],"type":"node"},{"attributes":{},"tag":"p","children":["Let's now use the Kriging surrogate, which is a single-output Gaussian process. This surrogate has a nice feature: not only does it approximate the solution at a point, it also calculates the standard error at such point. Let's see an example:"],"type":"node"},{"attributes":{"lang":"@example kriging"},"tag":"codeblock","children":["using Surrogates\nf = x -> exp(x)*x^2+x^3\nlb = 0.0\nub = 10.0\nx = sample(100,lb,ub,UniformSample())\ny = f.(x)\np = 1.9\nmy_krig = Kriging(x,y,lb,ub,p=p)\n\n#I want an approximation at 5.4\napprox = my_krig(5.4)\n\n#I want to find the standard error at 5.4\nstd_err = std_error_at_point(my_krig,5.4)\n"],"type":"node"},{"attributes":{},"tag":"p","children":["Let's now optimize the Kriging surrogate using Lower confidence bound method, this is just a one-liner:"],"type":"node"},{"attributes":{"lang":"@example kriging"},"tag":"codeblock","children":["surrogate_optimize(f,LCBS(),lb,ub,my_krig,UniformSample())\n"],"type":"node"},{"attributes":{},"tag":"p","children":["Surrogate optimization methods have two purposes: they both sample the space in unknown regions and look for the minima at the same time."],"type":"node"},{"attributes":{},"tag":"h2","children":["Lobachevsky integral"],"type":"node"},{"attributes":{},"tag":"p","children":["The Lobachevsky surrogate has the nice feature of having a closed formula for its integral, which is something that other surrogates are missing. Let's compare it with QuadGK:"],"type":"node"},{"attributes":{"lang":"@example"},"tag":"codeblock","children":["using Surrogates\nusing QuadGK\nobj = x -> 3*x + log(x)\na = 1.0\nb = 4.0\nx = sample(2000,a,b,SobolSample())\ny = obj.(x)\nalpha = 2.0\nn = 6\nmy_loba = LobachevskySurrogate(x,y,a,b,alpha=alpha,n=n)\n\n#1D integral\nint_1D = lobachevsky_integral(my_loba,a,b)\nint = quadgk(obj,a,b)\nint_val_true = int[1]-int[2]\n@assert int_1D â‰ˆ int_val_true\n"],"type":"node"},{"attributes":{},"tag":"h2","children":["Example of NeuralSurrogate"],"type":"node"},{"attributes":{},"tag":"p","children":["Basic example of fitting a neural network on a simple function of two variables."],"type":"node"},{"attributes":{"lang":"@example"},"tag":"codeblock","children":["using Surrogates\nusing Flux\nusing Statistics\n\nf = x -> x[1]^2 + x[2]^2\nbounds = Float32[-1.0, -1.0], Float32[1.0, 1.0]\n# Flux models are in single precision by default.\n# Thus, single precision will also be used here for our training samples.\n\nx_train = sample(100, bounds..., SobolSample())\ny_train = f.(x_train)\n\n# Perceptron with one hidden layer of 20 neurons.\nmodel = Chain(Dense(2, 20, relu), Dense(20, 1))\nloss(x, y) = Flux.mse(model(x), y)\n\n# Training of the neural network\nlearning_rate = 0.1\noptimizer = Descent(learning_rate)  # Simple gradient descent. See Flux documentation for other options.\nn_epochs = 50\nsgt = NeuralSurrogate(x_train, y_train, bounds..., model=model, loss=loss, opt=optimizer, n_echos=n_epochs)\n\n# Testing the new model\nx_test = sample(30, bounds..., SobolSample())\ntest_error = mean(abs2, sgt(x)[1] - f(x) for x in x_test)\n"],"type":"node"}],"type":"node"}],"type":"node"}