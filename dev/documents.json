[{"doctype":"documentation","id":"references/ExponentialUtilities._expv_ee","title":"_expv_ee","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.compute_B!","title":"compute_B!","text":""},{"doctype":"documentation","id":"references/Surrogates.LinearStructure","title":"LinearStructure","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.has_op","title":"has_op","text":""},{"doctype":"documentation","id":"references/SciMLBase.responsible_map","title":"responsible_map","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.calculate_step!","title":"calculate_step!","text":""},{"doctype":"documentation","id":"references/Optimization.DEFAULT_DATA","title":"DEFAULT_DATA","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.pass","title":"pass","text":""},{"doctype":"documentation","id":"references/Surrogates.AbstractSurrogate","title":"AbstractSurrogate","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.check_parameters","title":"check_parameters","text":""},{"doctype":"documentation","id":"references/LabelledArrays.unwrap_maybe","title":"unwrap_maybe","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.isvarkind","title":"isvarkind","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.intlog2","title":"intlog2","text":""},{"doctype":"documentation","id":"references/MethodOfLines.half_range","title":"half_range","text":""},{"doctype":"documentation","id":"references/SciMLOperators.AbstractMatrixFreeOperator","title":"AbstractMatrixFreeOperator","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/NonlinearSolve.SciMLSolution","title":"SciMLSolution","text":""},{"doctype":"documentation","id":"references/Catalyst.reset_networkproperties!","title":"reset_networkproperties!","text":"Clears the cache of various properties like the netstoichiometry matrix Use if such properties need to be recalculated for some reason"},{"doctype":"document","id":"ModelingToolkit/basics/Variable_metadata.md","title":"Symbolic metadata","text":"Distributions d Normal m dist d m m Symbolic metadata It is possible to add metadata to symbolic variables The following information can be added note it's possible to extend this to user-defined metadata as well Input or output Designate a variable as either an input or an output using the following Bounds Bounds are useful when parameters are to be optimized or to express intervals of uncertainty Mark input as a disturbance Indicate that an input is not available for control i.e it's a disturbance input Mark parameter as tunable Indicate that a parameter can be automatically tuned by automatic control tuning apps Probability distributions A probability distribution may be associated with a parameter to indicate either uncertainty about it's value or as a prior distribution for Bayesian optimization Additional functions For systems that contain parameters with metadata like described above have some additional functions defined for convenience In the example below we define a system with tunable parameters and extract bounds vectors Index Docstrings"},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs","title":"BipartiteGraphs","text":""},{"doctype":"documentation","id":"references/Optimization.decompose_trace","title":"decompose_trace","text":""},{"doctype":"documentation","id":"references/SciMLBase.UJacobianWrapper","title":"UJacobianWrapper","text":""},{"doctype":"documentation","id":"references/Catalyst.dependants","title":"dependants","text":"See documentation for  dependents "},{"doctype":"documentation","id":"references/RecursiveArrayTools.recursivefill!","title":"recursivefill!","text":"b AbstractArray T N a A recursive  fill  function"},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.is_neg_one","title":"is_neg_one","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.compute_v!","title":"compute_v!","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.arnoldi!","title":"arnoldi!","text":"Non-allocating version of  arnoldi "},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.Tail1","title":"Tail1","text":""},{"doctype":"document","id":"DiffEqSensitivity/neural_ode/neural_ode_flux.md","title":"Neural Ordinary Differential Equations with Flux.train!","text":"OrdinaryDiffEq Flux Optim Plots u0 Float32 datasize tspan trueODEfunc du u p t true_A du u true_A t range tspan tspan length datasize prob trueODEfunc u0 tspan ode_data Array prob Tsit5 saveat t dudt2 x x Dense tanh Dense p re Flux destructure dudt2 dudt u p t re p u prob dudt u0 tspan predict_n_ode Array prob Tsit5 u0 u0 p p saveat t loss_n_ode pred predict_n_ode loss sum abs2 ode_data pred loss loss_n_ode cb doplot pred predict_n_ode display sum abs2 ode_data pred pl scatter t ode_data label scatter! pl t pred label display plot pl cb data Iterators repeated res1 Flux train! loss_n_ode Flux u0 p data ADAM cb cb p re Flux destructure OrdinaryDiffEq GalacticOptim GalacticFlux GalacticOptimJL Plots u0 Float32 datasize tspan trueODEfunc du u p t true_A du u true_A t range tspan tspan length datasize prob trueODEfunc u0 tspan ode_data Array prob Tsit5 saveat t dudt2 x x Dense tanh Dense p re Flux destructure dudt2 dudt u p t re p u prob dudt u0 tspan θ u0 p predict_n_ode θ Array prob Tsit5 u0 θ p θ end saveat t loss_n_ode θ pred predict_n_ode θ loss sum abs2 ode_data pred loss pred loss_n_ode θ cb θ l pred doplot display l pl scatter t ode_data label scatter! pl t pred label display plot pl cb θ loss_n_ode θ adtype GalacticOptim optf GalacticOptim loss_neuralode adtype optprob GalacticOptim optfunc prob_neuralode p result_neuralode GalacticOptim optprob ADAM cb callback maxiters optprob2 optprob u0 result_neuralode u result_neuralode2 GalacticOptim optprob2 LBFGS cb callback allow_f_increases Neural Ordinary Differential Equations with Flux.train All of the tools of DiffEqSensitivity.jl can be used with Flux.jl A lot of the examples have been written to use  FastChain  and  sciml_train  but in all cases this can be changed to the  Chain  and  Flux.train  workflow Using Flux  Chain  neural networks with Flux.train This should work almost automatically by using  solve  Here is an example of optimizing  u0  and  p  Using Flux  Chain  neural networks with GalacticOptim Flux neural networks can be used with GalacitcOptim.jl by using the  Flux.destructure  function In this case if  dudt  is a Flux chain then returns  p  which is the vector of parameters for the chain and  re  which is a function  re(p  that reconstructs the neural network with new parameters  p  Using this function we can thus build our neural differential equations in an explicit parameter style Let's use this to build and train a neural ODE from scratch In this example we will optimize both the neural network parameters  p  and the input initial condition  u0  Notice that GalacticOptim.jl works on a vector input so we have to concatenate  u0  and  p  and then in the loss function split to the pieces Notice that the advantage of this format is that we can use Optim's optimizers like  LBFGS  with a full  Chain  object for all of Flux's neural networks like convolutional neural networks"},{"doctype":"documentation","id":"references/SciMLBase.AbstractIntegralProblem","title":"AbstractIntegralProblem","text":"DocStringExtensions.TypeDefinition Base for types which define integrals suitable for quadrature"},{"doctype":"documentation","id":"references/MethodOfLines.params","title":"params","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.GlobalScope","title":"GlobalScope","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.difference_vars","title":"difference_vars","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.firststep!","title":"firststep!","text":"Compute the first step of Arnoldi or Lanczos iteration Compute the first step of Arnoldi or Lanczos iteration of augmented system"},{"doctype":"documentation","id":"references/DiffEqSensitivity.AbstractForwardSensitivityAlgorithm","title":"AbstractForwardSensitivityAlgorithm","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.build_torn_function","title":"build_torn_function","text":""},{"doctype":"documentation","id":"references/Catalyst.numparams","title":"numparams","text":""},{"doctype":"document","id":"Optimization/optimization_packages/nlopt.md","title":"NLopt.jl","text":"Pkg Pkg add rosenbrock x p p x p x x x0 zeros p f rosenbrock prob f x0 p lb ub sol prob NLopt LN_NELDERMEAD rosenbrock x p p x p x x x0 zeros p f rosenbrock prob f x0 p lb ub sol prob NLopt LD_LBFGS rosenbrock x p p x p x x x0 zeros p f rosenbrock prob f x0 p lb ub sol prob NLopt GN_DIRECT rosenbrock x p p x p x x x0 zeros p f rosenbrock prob f x0 p lb ub sol prob NLopt G_MLSL_LDS local_method NLopt LD_LBFGS local_maxiters NLopt.jl NLopt  is Julia package interfacing to the free/open-source  NLopt library  which implements many optimization methods both global and local  NLopt Documentation  Installation OptimizationNLopt.jl To use this package install the OptimizationNLopt package Methods NLopt.jl  algorithms are chosen either via  NLopt.Opt(:algname nstates  where nstates is the number of states to be optimized but preferably via  NLopt.AlgorithmName  where AlgorithmName can be one of the following NLopt.GN_DIRECT NLopt.GN_DIRECT_L NLopt.GN_DIRECT_L_RAND NLopt.GN_DIRECT_NOSCAL NLopt.GN_DIRECT_L_NOSCAL NLopt.GN_DIRECT_L_RAND_NOSCAL NLopt.GN_ORIG_DIRECT NLopt.GN_ORIG_DIRECT_L NLopt.GD_STOGO NLopt.GD_STOGO_RAND NLopt.LD_LBFGS_NOCEDAL NLopt.LD_LBFGS NLopt.LN_PRAXIS NLopt.LD_VAR1 NLopt.LD_VAR2 NLopt.LD_TNEWTON NLopt.LD_TNEWTON_RESTART NLopt.LD_TNEWTON_PRECOND NLopt.LD_TNEWTON_PRECOND_RESTART NLopt.GN_CRS2_LM NLopt.GN_MLSL NLopt.GD_MLSL NLopt.GN_MLSL_LDS NLopt.GD_MLSL_LDS NLopt.LD_MMA NLopt.LN_COBYLA NLopt.LN_NEWUOA NLopt.LN_NEWUOA_BOUND NLopt.LN_NELDERMEAD NLopt.LN_SBPLX NLopt.LN_AUGLAG NLopt.LD_AUGLAG NLopt.LN_AUGLAG_EQ NLopt.LD_AUGLAG_EQ NLopt.LN_BOBYQA NLopt.GN_ISRES NLopt.AUGLAG NLopt.AUGLAG_EQ NLopt.G_MLSL NLopt.G_MLSL_LDS NLopt.LD_SLSQP NLopt.LD_CCSAQ NLopt.GN_ESCH NLopt.GN_AGS See the  NLopt Documentation  for more details on each optimizer Beyond the common arguments the following optimizer parameters can be set as  kwargs  stopval xtol_rel xtol_abs constrtol_abs initial_step population vector_storage Local Optimizer Derivative-Free Derivative-free optimizers are optimizers that can be used even in cases where no derivatives or automatic differentiation is specified While they tend to be less efficient than derivative-based optimizers they can be easily applied to cases where defining derivatives is difficult Note that while these methods do not support general constraints all support bounds constraints via  lb  and  ub  in the  OptimizationProblem  NLopt  derivative-free optimizers are NLopt.LN_PRAXIS NLopt.LN_COBYLA NLopt.LN_NEWUOA NLopt.LN_NEWUOA_BOUND NLopt.LN_NELDERMEAD NLopt.LN_SBPLX NLopt.LN_AUGLAG NLopt.LN_AUGLAG_EQ NLopt.LN_BOBYQA The Rosenbrock function can optimized using the  NLopt.LN_NELDERMEAD  as follows Gradient-Based Gradient-based optimizers are optimizers which utilise the gradient information based on derivatives defined or automatic differentiation NLopt  gradient-based optimizers are NLopt.LD_LBFGS_NOCEDAL NLopt.LD_LBFGS NLopt.LD_VAR1 NLopt.LD_VAR2 NLopt.LD_TNEWTON NLopt.LD_TNEWTON_RESTART NLopt.LD_TNEWTON_PRECOND NLopt.LD_TNEWTON_PRECOND_RESTART NLopt.LD_MMA NLopt.LD_AUGLAG NLopt.LD_AUGLAG_EQ NLopt.LD_SLSQP NLopt.LD_CCSAQ The Rosenbrock function can optimized using  NLopt.LD_LBFGS  as follows Global Optimizer Without Constraint Equations The following algorithms in  NLopt  are performing global optimization on problems without constraint equations However lower and upper constraints set by  lb  and  ub  in the  OptimizationProblem  are required NLopt  global optimizers which fall into this category are NLopt.GN_DIRECT NLopt.GN_DIRECT_L NLopt.GN_DIRECT_L_RAND NLopt.GN_DIRECT_NOSCAL NLopt.GN_DIRECT_L_NOSCAL NLopt.GN_DIRECT_L_RAND_NOSCAL NLopt.GD_STOGO NLopt.GD_STOGO_RAND NLopt.GN_CRS2_LM NLopt.GN_MLSL NLopt.GD_MLSL NLopt.GN_MLSL_LDS NLopt.GD_MLSL_LDS NLopt.G_MLSL NLopt.G_MLSL_LDS NLopt.GN_ESCH The Rosenbrock function can optimized using  NLopt.GN_DIRECT  as follows Algorithms such as  NLopt.G_MLSL  or  NLopt.G_MLSL_LDS  also require a local optimiser to be selected which via the  local_method  argument of  solve  The Rosenbrock function can optimized using  NLopt.G_MLSL_LDS  with  NLopt.LN_NELDERMEAD  as the local optimizer The local optimizer maximum iterations are set via  local_maxiters  With Constraint Equations The following algorithms in  NLopt  are performing global optimization on problems with constraint equations However lower and upper constraints set by  lb  and  ub  in the  OptimizationProblem  are required  note Constraints with NLopt Equality and inequality equation support for  NLopt  via  Optimization  is not supported directly However you can use the MOI wrapper to use constraints with NLopt optimisers NLopt  global optimizers which fall into this category are NLopt.GN_ORIG_DIRECT NLopt.GN_ORIG_DIRECT_L NLopt.GN_ISRES NLopt.GN_AGS"},{"doctype":"documentation","id":"references/ModelingToolkit.AbstractTimeDependentSystem","title":"AbstractTimeDependentSystem","text":""},{"doctype":"documentation","id":"references/SciMLBase.diffeq_to_arrays","title":"diffeq_to_arrays","text":""},{"doctype":"documentation","id":"references/SciMLBase.promote_tspan","title":"promote_tspan","text":"Convert the  tspan  field of a  DEProblem  to a  tmin tmax  tuple where both elements are of the same type If  tspan  is a function returns it as-is"},{"doctype":"documentation","id":"references/Catalyst.reactionparams","title":"reactionparams","text":"Given a  ReactionSystem  return a vector of all parameters defined within the system and any subsystems that are of type  ReactionSystem  To get the parameters in the system and all subsystems including non ReactionSystem  subsystems use  parameters(network  Notes If  ModelingToolkit.get_systems(network  is non-empty will allocate"},{"doctype":"documentation","id":"references/SciMLBase.AbstractNoTimeSolution","title":"AbstractNoTimeSolution","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/DiffEqSensitivity.ODEBacksolveSensitivityFunction","title":"ODEBacksolveSensitivityFunction","text":""},{"doctype":"documentation","id":"references/SciMLBase.DiscreteProblem","title":"DiscreteProblem","text":"Defines a discrete dynamical system problem Documentation Page https://diffeq.sciml.ai/stable/types/discrete_types Mathematical Specification of a Discrete Problem To define an Discrete Problem you simply need to give the function  f  and the initial condition  u_0  which define a function map u_{n+1  f(u_{n},p,t_{n+1 f  should be specified as  f(un,p,t  or in-place as  f(unp1,un,p,t  and  u_0  should be an AbstractArray or number whose geometry matches the desired geometry of  u  Note that we are not limited to numbers or vectors for  u₀  one is allowed to provide  u₀  as arbitrary matrices  higher dimension tensors as well  u_{n+1  only depends on the previous iteration  u_{n  and  t_{n+1  The default  t_{n+1  of  FunctionMap  is  t_n  t_0  n*dt  with  dt=1  being the default For continuous-time Markov chains this is the time at which the change is occuring Note that if the discrete solver is set to have  scale_by_time=true  then the problem is interpreted as the map u_{n+1  u_n  dt f(u_{n},p,t_{n+1 Problem Type Constructors DiscreteProblem{isinplace}(f::ODEFunction,u0,tspan,p=NullParameters();kwargs   Defines the discrete problem with the specified functions DiscreteProblem{isinplace}(f,u0,tspan,p=NullParameters();kwargs   Defines the discrete problem with the specified functions DiscreteProblem{isinplace}(u0,tspan,p=NullParameters();kwargs   Defines the discrete problem with the identity map Parameters are optional and if not given then a  NullParameters  singleton will be used which will throw nice errors if you try to index non-existent parameters Any extra keyword arguments are passed on to the solvers For example if you set a  callback  in the problem then that  callback  will be added in every solve call For specifying Jacobians and mass matrices see the DiffEqFunctions  performance_overloads page Fields f  The function in the map u0  The initial condition tspan  The timespan for the problem p  The parameters for the problem Defaults to  NullParameters kwargs  The keyword arguments passed onto the solves Note About Timing Note that if no  dt  and not  tstops  is given it's assumed that  dt=1  and thus  tspan=(0,n  will solve for  n  iterations If in the solver  dt  is given then the number of iterations will change And if  tstops  is not empty the solver will revert to the standard behavior of fixed timestep methods which is step to each tstop Defines a discrete problem with the specified functions DocStringExtensions.MethodSignatures Define a discrete problem with the identity map"},{"doctype":"documentation","id":"references/ModelingToolkit.define_vars","title":"define_vars","text":""},{"doctype":"documentation","id":"references/Surrogates._forward_pass_1d","title":"_forward_pass_1d","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.apply_inv_pivot_rows!","title":"apply_inv_pivot_rows!","text":""},{"doctype":"document","id":"ModelingToolkit/systems/ODESystem.md","title":"ODESystem","text":"jacobian_sparsity ODESystem System Constructors Composition and Accessor Functions get_eqs(sys  or  equations(sys  The equations that define the ODE get_states(sys  or  states(sys  The set of states in the ODE get_ps(sys  or  parameters(sys  The parameters of the ODE get_iv(sys  The independent variable of the ODE Transformations Analyses Applicable Calculation and Generation Functions Standard Problem Constructors Torn Problem Constructors"},{"doctype":"documentation","id":"references/QuasiMonteCarlo.free_dimensions","title":"free_dimensions","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.flowvar","title":"flowvar","text":""},{"doctype":"documentation","id":"references/Catalyst.get_reactants","title":"get_reactants","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractDiffEqFunction","title":"AbstractDiffEqFunction","text":"DocStringExtensions.TypeDefinition Base for types defining differential equation functions"},{"doctype":"document","id":"Optimization/tutorials/minibatch.md","title":"Minibatch examples","text":"OptimizationOptimJL OrdinaryDiffEq newtons_cooling du u p t temp u k temp_m p du dT k temp temp_m true_sol du u p t true_p log newtons_cooling du u true_p t dudt_ u p t ann u p u callback p l pred doplot display l doplot pl scatter t ode_data label scatter! pl t pred label display plot pl u0 Float32 datasize tspan t range tspan tspan length datasize true_prob true_sol u0 tspan ode_data Array true_prob Tsit5 saveat t ann tanh tanh pp ann prob dudt_ u0 tspan pp predict_adjoint fullp time_batch Array prob Tsit5 p fullp saveat time_batch loss_adjoint fullp batch time_batch pred predict_adjoint fullp time_batch sum abs2 batch pred pred k train_loader Flux Data DataLoader ode_data t batchsize k numEpochs l1 loss_adjoint pp train_loader data train_loader data optfun θ p batch time_batch loss_adjoint θ batch time_batch optprob optfun pp IterTools ncycle res1 optprob ADAM ncycle train_loader numEpochs callback callback res1 minimum l1 Minibatch examples Note This example uses the OptimizationOptimJL.jl package See the Optim.jl page  optim for details on the installation and usage"},{"doctype":"documentation","id":"references/Integrals.v_inf","title":"v_inf","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.torn_system_with_nlsolve_jacobian_sparsity","title":"torn_system_with_nlsolve_jacobian_sparsity","text":""},{"doctype":"documentation","id":"references/NeuralPDE.build_loss_function","title":"build_loss_function","text":""},{"doctype":"documentation","id":"references/SciMLBase.__has_syms","title":"__has_syms","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.numericnstoich","title":"numericnstoich","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.Cos2Windowing","title":"Cos2Windowing","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.cpp_bridge","title":"cpp_bridge","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.DEFAULT_LINSOLVE","title":"DEFAULT_LINSOLVE","text":""},{"doctype":"documentation","id":"references/Catalyst.numreactionparams","title":"numreactionparams","text":"Return the total number of parameters within the given  ReactionSystem  and subsystems that are  ReactionSystem s Notes If there are no subsystems this will be fast As this calls  reactionparams  it can be slow and will allocate if there are any subsystems"},{"doctype":"documentation","id":"references/ModelingToolkit.instream_rt","title":"instream_rt","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.validate","title":"validate","text":"Returns true iff units of equations are valid"},{"doctype":"documentation","id":"references/ModelingToolkit.SystemStructures.linear_subsys_adjmat","title":"linear_subsys_adjmat","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_eqs","title":"get_eqs","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.renormalize!","title":"renormalize!","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.has_continuous_callback","title":"has_continuous_callback","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.MultiDimDirectionalBC","title":"MultiDimDirectionalBC","text":""},{"doctype":"documentation","id":"references/SciMLBase.has_reinit","title":"has_reinit","text":""},{"doctype":"documentation","id":"references/Surrogates.thinplateRadial","title":"thinplateRadial","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.SystemStructures.TransformationState","title":"TransformationState","text":""},{"doctype":"documentation","id":"references/SciMLOperators.has_expmv!","title":"has_expmv!","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.NeuralSDE","title":"NeuralSDE","text":"model1 model2 tspan nbrown alg nothing args sensealg kwargs model1 model2 tspan nbrown alg nothing args sensealg kwargs Constructs a neural stochastic differential equation neural SDE Arguments model1  A Chain or FastChain neural network that defines the drift function model2  A Chain or FastChain neural network that defines the diffusion function Should output a matrix that is nbrown x size(x,1 tspan  The timespan to be solved on nbrown  The number of Brownian processes alg  The algorithm used to solve the ODE Defaults to  nothing  i.e the default algorithm from DifferentialEquations.jl sensealg  The choice of differentiation algorthm used in the backpropogation Defaults to using reverse-mode automatic differentiation via Tracker.jl kwargs  Additional arguments splatted to the ODE solver See the  Common Solver Arguments  documentation for more details"},{"doctype":"documentation","id":"references/ModelingToolkit.equation_dependencies","title":"equation_dependencies","text":"sys variables sys β γ κ η t S t I t R t rate₁ β S I rate₂ γ I t affect₁ S S I I affect₂ I I R R j₁ ConstantRateJump rate₁ affect₁ j₂ VariableRateJump rate₂ affect₂ jumpsys j₁ j₂ t S I R β γ jumpsys jumpsys variables jumpsys Given an  AbstractSystem  calculate for each equation the variables it depends on Notes Variables that are not in  variables  are filtered out get_variables  is used to determine the variables within a given equation returns a  Vector{Vector{Variable  mapping the index of an equation to the  variables  it depends on Example"},{"doctype":"document","id":"LinearSolve/tutorials/linear.md","title":"Solving Linear Systems in Julia","text":"A rand b rand prob A b sol prob sol u sol prob Solving Linear Systems in Julia A linear system Au=b is specified by defining an  AbstractMatrix   A  or by providing a matrix-free operator for performing  A*x  operations via the function  A(u,p,t  out-of-place and  A(du,u,p,t  for in-place For the sake of simplicity this tutorial will only showcase concrete matrices The following defines a matrix and a  LinearProblem  which is subsequently solved by the default linear solver Note that  solve(prob  is equivalent to  solve(prob,nothing  where  nothing  denotes the choice of the default linear solver This is equivalent to the Julia built-in  A\\b  where the solution is recovered via  sol.u  The power of this package comes into play when changing the algorithms For example  IterativeSolvers.jl  has some nice methods like GMRES which can be faster in some cases With LinearSolve.jl there is one interface and changing linear solvers is simply the switch of the algorithm choice Thus a package which uses LinearSolve.jl simply needs to allow the user to pass in an algorithm struct and all wrapped linear solvers are immediately available as tweaks to the general algorithm"},{"doctype":"documentation","id":"references/LinearSolve.set_b","title":"set_b","text":"DocStringExtensions.MethodSignatures"},{"doctype":"documentation","id":"references/SciMLOperators.issquare","title":"issquare","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.outputs","title":"outputs","text":"Return all variables that mare marked as outputs See also  unbound_outputs  See also  bound_outputs   unbound_outputs"},{"doctype":"documentation","id":"references/DiffEqFlux.QuarticKernel","title":"QuarticKernel","text":""},{"doctype":"document","id":"DiffEqSensitivity/neural_ode/mnist_neural_ode.md","title":"[GPU-based MNIST Neural ODE Classifier]( mnist)","text":"DifferentialEquations NNlib MLDataUtils Printf Flux Losses logitcrossentropy Flux Data DataLoader MLDatasets CUDA CUDA allowscalar loadmnist batchsize bs train_split onehot labels_raw convertlabel LabelEnc OneOfK labels_raw LabelEnc NativeLabels collect imgs labels_raw MNIST traindata x_data Float32 reshape imgs size imgs size imgs size imgs y_data onehot labels_raw x_train y_train x_test y_test stratifiedobs x_data y_data p train_split DataLoader gpu collect x_train y_train batchsize batchsize shuffle DataLoader gpu collect x_test y_test batchsize batchsize shuffle bs train_split train_dataloader test_dataloader loadmnist bs train_split down Flux Dense tanh gpu nn Dense tanh Dense tanh Dense tanh gpu nn_ode nn Tsit5 save_everystep reltol abstol save_start gpu fc Dense gpu DiffEqArray_to_Array x xarr gpu x reshape xarr size xarr model down nn_ode DiffEqArray_to_Array fc gpu img lab train_dataloader data train_dataloader data x_d down img x_m model img classify x argmax eachcol x accuracy model data n_batches total_correct total i x y enumerate collect data i n_batches target_class classify cpu y predicted_class classify cpu model x total_correct sum target_class predicted_class total length target_class total_correct total accuracy model train_dataloader loss x y logitcrossentropy model x y loss img lab opt ADAM iter cb iter iter train_accuracy accuracy model train_dataloader test_accuracy accuracy model test_dataloader n_batches length test_dataloader iter train_accuracy test_accuracy Flux train! loss Flux down nn_ode p fc train_dataloader opt cb cb DifferentialEquations NNlib MLDataUtils Printf Flux Losses logitcrossentropy Flux Data DataLoader MLDatasets CUDA CUDA allowscalar loadmnist batchsize bs train_split onehot labels_raw convertlabel LabelEnc OneOfK labels_raw LabelEnc NativeLabels collect imgs labels_raw MNIST traindata x_data Float32 reshape imgs size imgs size imgs size imgs y_data onehot labels_raw x_train y_train x_test y_test stratifiedobs x_data y_data p train_split DataLoader gpu collect x_train y_train batchsize batchsize shuffle DataLoader gpu collect x_test y_test batchsize batchsize shuffle down Flux Dense tanh gpu nn Dense tanh Dense tanh Dense tanh gpu nn_ode nn Tsit5 save_everystep reltol abstol save_start gpu fc Dense gpu DiffEqArray_to_Array x xarr gpu x reshape xarr size xarr model down nn_ode DiffEqArray_to_Array fc gpu img lab train_dataloader data train_dataloader data x_d down img x_m model img m_no_ode down nn fc gpu x_m m_no_ode img classify x argmax eachcol x accuracy model data n_batches total_correct total i x y enumerate collect data i n_batches target_class classify cpu y predicted_class classify cpu model x total_correct sum target_class predicted_class total length target_class total_correct total accuracy m train_dataloader loss x y logitcrossentropy model x y loss img lab opt ADAM cb iter iter train_accuracy accuracy model train_dataloader test_accuracy accuracy model test_dataloader n_batches length test_dataloader iter train_accuracy test_accuracy Flux train! loss Flux down nn_ode p fc zip x_train y_train opt cb cb GPU-based MNIST Neural ODE Classifier  mnist Training a classifier for  MNIST  using a neural ordinary differential equation  NN-ODE  on  GPUs  with  Minibatching  Step-by-step description below Step-by-Step Description Load Packages GPU A good trick used here ensures that only optimized kernels are called when using the GPU Additionally the  gpu  function is shown as a way to translate models and data over to the GPU Note that this function is CPU-safe so if the GPU is disabled or unavailable this code will fallback to the CPU Load MNIST Dataset into Minibatches The preprocessing is done in  loadmnist  where the raw MNIST data is split into features  x_train  and labels  y_train  by specifying batchsize  bs  The function  convertlabel  will then transform the current labels  labels_raw  from numbers 0 to 9  LabelEnc.NativeLabels(collect(0:9  into one hot encoding  LabelEnc.OneOfK  Features are reshaped into format  Height Width Color BatchSize  or in this case  28 28 1 128  meaning that every minibatch will contain 128 images with a single color channel of 28x28 pixels The entire dataset of 60,000 images is split into the train and test dataset ensuring a balanced ratio of labels These splits are then passed to Flux's DataLoader This automatically minibatches both the images and labels Additionally it allows us to shuffle the train dataset in each epoch while keeping the order of the test data the same and then loaded from main Layers The Neural Network requires passing inputs sequentially through multiple layers We use  Chain  which allows inputs to functions to come from previous layer and sends the outputs to the next Four different sets of layers are used here down  This layer downsamples our images into a 20 dimensional feature vector It takes a 28 x 28 image flattens it and then passes it through a fully connected layer with  tanh  activation nn  A 3 layers Deep Neural Network Chain with  tanh  activation which is used to model our differential equation nn_ode  ODE solver layer fc  The final fully connected layer which maps our learned feature vector to the probability of the feature vector of belonging to a particular class  gpu  An utility function which transfers our model to GPU if it is available Array Conversion When using  NeuralODE  this function converts the ODESolution's  DiffEqArray  to a Matrix CuArray and reduces the matrix from 3 to 2 dimensions for use in the next layer For CPU If this function does not automatically fallback to CPU when no GPU is present we can change  gpu(x  to  Array(x  Build Topology Next we connect all layers together in a single chain There are a few things we can do to examine the inner workings of our neural network This can also be built without the NN-ODE by replacing  nn-ode  with a simple  nn  Prediction To convert the classification back into readable numbers we use  classify  which returns the prediction by taking the arg max of the output for each column of the minibatch Accuracy We then evaluate the accuracy on  n_batches  at a time through the entire network Training Parameters Once we have our model we can train our neural network by backpropagation using  Flux.train  This function requires  Loss   Optimizer  and  Callback  functions Loss Cross Entropy  is the loss function computed here which applies a  Softmax  operation on the final output of our model  logitcrossentropy  takes in the prediction from our model  model(x  and compares it to actual output  y  Optimizer ADAM  is specified here as our optimizer with a  learning rate of 0.05  CallBack This callback function is used to print both the training and testing accuracy after 10 training iterations Train To train our model we select the appropriate trainable parameters of our network with  params  In our case backpropagation is required for  down   nn_ode  and  fc  Notice that the parameters for Neural ODE is given by  nn_ode.p  Expected Output"},{"doctype":"document","id":"NonlinearSolve/solvers/BracketingSolvers.md","title":"Bracketing Solvers","text":"Bracketing Solvers solve(prob::NonlinearProblem,alg;kwargs Solves for  f(u)=0  in the problem defined by  prob  using the algorithm  alg  If no algorithm is given a default algorithm will be chosen This page is solely focused on the bracketing methods for scalar nonlinear equations Recommended Methods Falsi  can have a faster convergence and is discretely differentiable but is less stable than  Bisection  Full List of Methods NonlinearSolve.jl Falsi  A non-allocating regula falsi method Bisection  A common bisection method"},{"doctype":"documentation","id":"references/DiffEqFlux.CosBasis","title":"CosBasis","text":"n Constructs a cosine basis of the form cos(x cos(2 x  cos(n x Arguments n  number of terms in the cosine expansion"},{"doctype":"documentation","id":"references/RecursiveArrayTools.unpack_args_voa","title":"unpack_args_voa","text":""},{"doctype":"documentation","id":"references/Optimization.default_chunk_size","title":"default_chunk_size","text":""},{"doctype":"document","id":"ModelingToolkit/tutorials/spring_mass.md","title":"Component-Based Modeling a Spring-Mass System","text":"Component-Based Modeling a Spring-Mass System In this tutorial we will build a simple component-based model of a spring-mass system A spring-mass system consists of one or more masses connected by springs  Hooke's law  gives the force exerted by a spring when it is extended or compressed by a given distance This specifies a differential-equation system where the acceleration of the masses is specified using the forces acting on them Copy-Paste Example Explanation Building the components For each component we use a Julia function that returns an  ODESystem  At the top we define the fundamental properties of a  Mass  it has a mass  m  a position  pos  and a velocity  v  We also define that the velocity is the rate of change of position with respect to time Note that this is an incompletely specified  ODESystem  It cannot be simulated on its own since the equations for the velocity  v[1:2](t  are unknown Notice the addition of a  name  keyword This allows us to generate different masses with different names A  Mass  can now be constructed as Or using the  named  helper macro Next we build the spring component It is characterised by the spring constant  k  and the length  l  of the spring when no force is applied to it The state of a spring is defined by its current length and direction We now define functions that help construct the equations for a mass-spring system First the  connect_spring  function connects a  spring  between two positions  a  and  b  Note that  a  and  b  can be the  pos  of a  Mass  or just a fixed position such as  0 0  In that sense the length of the spring  x  is given by the length of the vector  dir  joining  a  and  b  Lastly we define the  spring_force  function that takes a  spring  and returns the force exerted by this spring To create our system we will first create the components a mass and a spring This is done as follows We can now create the equations describing this system by connecting  spring  to  mass  and a fixed point Finally we can build the model using these equations and components We can take a look at the equations in the model using the  equations  function The states of this model are And the parameters of this model are Simplifying and solving this system This system can be solved directly as a DAE using  one of the DAE solvers from DifferentialEquations.jl  However we can symbolically simplify the system first beforehand Running  structural_simplify  eliminates unnecessary variables from the model to give the leanest numerical representation of the system We are left with only 4 equations involving 4 state variables  mass.pos[1   mass.pos[2   mass.v[1   mass.v[2  We can solve the system by converting it to an  ODEProblem  Some observed variables are not expanded by default To view the complete equations one can do This is done as follows What if we want the timeseries of a different variable That information is not lost Instead  structural_simplify  simply changes state variables into  observed  variables These are explicit algebraic equations which can be used to reconstruct the required variables on the fly This leads to dramatic computational savings since implicitly solving an ODE scales as O(n^3 so fewer states are significantly better We can access these variables using the solution object For example let's retrieve the x-position of the mass over time We can also plot the path of the mass"},{"doctype":"documentation","id":"references/ModelingToolkit.convert_system","title":"convert_system","text":"DocStringExtensions.TypedMethodSignatures Convert a  NonlinearSystem  to an  ODESystem  or converts an  ODESystem  to a new  ODESystem  with a different independent variable"},{"doctype":"documentation","id":"references/ModelingToolkit.get_cset_sv","title":"get_cset_sv","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.NoiseFunction","title":"NoiseFunction","text":"f t exp t W f f out t out exp t W f noise_prototype rand This allows you to use any arbitrary function  W(t  as a  NoiseProcess  This will use the function lazily only caching values required to minimize function calls but not store the entire noise array This requires an initial time point  t0  in the domain of  W  A second function is needed if the desired SDE algorithm requires multiple processes Additionally one can use an in-place function  W(out1,out2,t  for more efficient generation of the arrays for multi-dimensional processes When the in-place version is used without a dispatch for the out-of-place version the  noise_prototype  needs to be set NoiseFunction Example The  NoiseFunction  is pretty simple pass a function As a silly example we can use  exp  as a noise process by doing If it's multi-dimensional and an in-place function is used the  noise_prototype  must be given For example This allows you to put arbitrarily weird noise into SDEs and RODEs Have fun"},{"doctype":"documentation","id":"references/SciMLBase.HermiteInterpolation","title":"HermiteInterpolation","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/Catalyst.get_reactions","title":"get_reactions","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.PDESystem","title":"PDESystem","text":"x t u Dxx Differential x Dtt Differential t Dt Differential t C eq Dtt u t x C Dxx u t x bcs u t u t u x x x Dt u x domains t x pde_system eq bcs domains t x u DocStringExtensions.TypeDefinition A system of partial differential equations Fields DocStringExtensions.TypeFields(false Example"},{"doctype":"documentation","id":"references/MethodOfLines.differential_order","title":"differential_order","text":"return list of differential orders in the equation"},{"doctype":"documentation","id":"references/NonlinearSolve.jacobian!","title":"jacobian!","text":""},{"doctype":"documentation","id":"references/ParameterizedFunctions.findreplace","title":"findreplace","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.NonlinearProblemExpr","title":"NonlinearProblemExpr","text":"Generates a Julia expression for a NonlinearProblem from a NonlinearSystem and allows for automatically symbolically calculating numerical enhancements"},{"doctype":"documentation","id":"references/GlobalSensitivity.sample_matrices","title":"sample_matrices","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.fixpoint_sub","title":"fixpoint_sub","text":""},{"doctype":"documentation","id":"references/SciMLBase.has_colorvec","title":"has_colorvec","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.alg_cache","title":"alg_cache","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timeseries_steps_quantile","title":"timeseries_steps_quantile","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.asgraph","title":"asgraph","text":"eqdeps vtois digr odesys Dict s i i s enumerate odesys sys variables sys variablestoids Dict convert Variable v i i v enumerate variables digr odesys Convert a collection of equation dependencies for example as returned by  equation_dependencies  to a  BipartiteGraph  Notes vtois  should provide a  Dict  like mapping from each  Variable  dependency in  eqdeps  to the integer idx of the variable to use in the graph Example Continuing the example started in  equation_dependencies Convert an  AbstractSystem  to a  BipartiteGraph  mapping the index of equations to the indices of variables they depend on Notes Defaults for kwargs creating a mapping from  equations(sys  to  states(sys  they depend on variables  should provide the list of variables to use for generating the dependency graph variablestoids  should provide  Dict  like mapping from a  Variable  to its  Int  index within  variables  Example Continuing the example started in  equation_dependencies"},{"doctype":"documentation","id":"references/NonlinearSolve.@add_kwonly","title":"@add_kwonly","text":"Define keyword-only version of the  function_definition  expands to"},{"doctype":"documentation","id":"references/Surrogates.SurrogateOptimizationAlgorithm","title":"SurrogateOptimizationAlgorithm","text":""},{"doctype":"documentation","id":"references/ParameterizedFunctions.bad_derivative","title":"bad_derivative","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.substitute_aliases","title":"substitute_aliases","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.ComposedMultiDimBC","title":"ComposedMultiDimBC","text":""},{"doctype":"document","id":"ModelingToolkit/tutorials/parameter_identifiability.md","title":"Parameter Identifiability in ODE Models","text":"Pkg Pkg add StructuralIdentifiability t x4 t x5 t x6 t x7 t y1 t y2 t k5 k6 k7 k8 k9 k10 D Differential t eqs D x4 k5 x4 k6 x4 D x5 k5 x4 k6 x4 k7 x5 k8 x5 x6 D x6 k7 x5 k8 x5 x6 k9 x6 k10 x6 k10 D x7 k9 x6 k10 x6 k10 measured_quantities y1 x4 y2 x5 de eqs t name Biohydrogenation local_id_all assess_local_identifiability de measured_quantities measured_quantities p to_check k5 k7 k10 k9 k5 k6 local_id_some assess_local_identifiability de measured_quantities measured_quantities funcs_to_check to_check p Parameter Identifiability in ODE Models Ordinary differential equations are commonly used for modeling real-world processes The problem of parameter identifiability is one of the key design challenges for mathematical models A parameter is said to be  identifiable  if one can recover its value from experimental data  Structural  identifiability is a theoretical property of a model that answers this question In this tutorial we will show how to use  StructuralIdentifiability.jl  with  ModelingToolkit.jl  to assess identifiability of parameters in ODE models The theory behind  StructuralIdentifiability.jl  is presented in paper   We will start by illustrating  local identifiability  in which a parameter is known up to  finitely many values  and then proceed to determining  global identifiability  that is which parameters can be identified  uniquely  To install  StructuralIdentifiability.jl  simply run The package has a standalone data structure for ordinary differential equations but is also compatible with  ODESystem  type from  ModelingToolkit.jl  Local Identifiability Input System We will consider the following model begin{cases}\n\\frac{d\\,x_4}{d\\,t   frac{k_5 x_4}{k_6  x_4},\\\\\n\\frac{d\\,x_5}{d\\,t  frac{k_5 x_4}{k_6  x_4  frac{k_7 x_5}{(k_8  x_5  x_6)},\\\\\n\\frac{d\\,x_6}{d\\,t  frac{k_7 x_5}{(k_8  x_5  x_6  frac{k_9  x_6  k_{10  x_6 k_{10}},\\\\\n\\frac{d\\,x_7}{d\\,t  frac{k_9  x_6  k_{10  x_6 k_{10}},\\\\\ny_1  x_4,\\\\\ny_2  x_5\\end{cases This model describes the biohydrogenation  process  with unknown initial conditions Using the  ODESystem  object To define the ode system in Julia we use  ModelingToolkit.jl  We first define the parameters variables differential equations and the output equations After that we are ready to check the system for local identifiability We can see that all states except  x_7  and all parameters are locally identifiable with probability 0.99 Let's try to check specific parameters and their combinations Notice that in this case everything except the state variable  x_7  is locally identifiable including combinations such as  k_{10}/k_9 k_5+k_6 Global Identifiability In this part tutorial let us cover an example problem of querying the ODE for globally identifiable parameters Input System Let us consider the following four-dimensional model with two outputs begin{cases}\nx_1'(t  b  x_1(t  frac{1  c  x_4(t)},\\\\\nx_2'(t  alpha  x_1(t  beta  x_2(t),\\\\\nx_3'(t  gamma  x_2(t  delta  x_3(t),\\\\\nx_4'(t  sigma  x_4(t  frac{(\\gamma x_2(t  delta x_3(t x_3(t)},\\\\\ny(t  x_1(t)\n\\end{cases We will run a global identifiability check on this enzyme dynamics  model We will use the default settings the probability of correctness will be  p=0.99  and we are interested in identifiability of all possible parameters Global identifiability needs information about local identifiability first but the function we chose here will take care of that extra step for us Note  as of writing this tutorial UTF-symbols such as Greek characters are not supported by one of the project's dependencies see  this issue  We can see that only parameters  a g  are unidentifiable and everything else can be uniquely recovered Let us consider the same system but with two inputs and we will try to find out identifiability with probability  0.9  for parameters  c  and  b  Both parameters  b c  are globally identifiable with probability  0.9  in this case R Munoz-Tamayo L Puillet J.B Daniel D Sauvant O Martin M Taghipoor P Blavy  Review To be or not to be an identifiable model Is this a relevant question in animal science modelling  Animal Vol 12 4 701-712 2018 The model is the ODE system 3 in Supplementary Material 2 initial conditions are assumed to be unknown Moate P.J Boston R.C Jenkins T.C and Lean I.J  Kinetics of Ruminal Lipolysis of Triacylglycerol and Biohydrogenationof Long-Chain Fatty Acids New Insights from Old Data  Journal of Dairy Science 91 731–742 2008 Goodwin B.C  Oscillatory behavior in enzymatic control processes  Advances in Enzyme Regulation Vol 3 C 425-437 1965 Dong R Goodbrake C Harrington H A  Pogudin G  Computing input-output projections of dynamical models with applications to structural identifiability  arXiv preprint arXiv:2111.00991"},{"doctype":"documentation","id":"references/LinearSolve.InvPreconditioner","title":"InvPreconditioner","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.iscontinuous","title":"iscontinuous","text":""},{"doctype":"documentation","id":"references/GlobalSensitivity.Morris","title":"Morris","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.candidate_pow_minus","title":"candidate_pow_minus","text":""},{"doctype":"documentation","id":"references/SciMLBase.NoiseProblem","title":"NoiseProblem","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/NeuralPDE.KeywordArgWarn","title":"KeywordArgWarn","text":""},{"doctype":"document","id":"NeuralPDE/solvers/nnrode.md","title":"Random Ordinary Differential Equation Specialized Physics-Informed Neural Solver","text":"Random Ordinary Differential Equation Specialized Physics-Informed Neural Solver TODO"},{"doctype":"documentation","id":"references/SciMLBase.IntegralSolution","title":"IntegralSolution","text":"DocStringExtensions.TypeDefinition Representation of the solution to an quadrature integral_lb^ub f(x dx defined by a IntegralProblem Fields u  the representation of the optimization's solution resid  the residual of the solver alg  the algorithm type used by the solver retcode  the return code from the solver Used to determine whether the solver solved successfully  sol.retcode  Success  whether it terminated due to a user-defined callback  sol.retcode  Terminated  or whether it exited due to an error For more details see the return code section of the DifferentialEquations.jl documentation chi  the variance estimate of the estimator from Monte Carlo quadrature methods"},{"doctype":"documentation","id":"references/ParameterizedFunctions","title":"ParameterizedFunctions","text":"DocStringExtensions.Readme"},{"doctype":"documentation","id":"references/ModelingToolkit.assemble_vrj_expr","title":"assemble_vrj_expr","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.has_connector_type","title":"has_connector_type","text":""},{"doctype":"document","id":"PolyChaos/numerical_integration.md","title":"[Numerical Integration]( NumericalIntegration)","text":"Numerical Integration  NumericalIntegration The goal of this tutorial is to solve an integral using Gauss quadrature I  int_{0}^{1 f(t mathrm{d t approx sum_{k=1}^n w_k f(t_k where we choose  f(t  sin(t  and  n  5  Make sure to check out this tutorial  QuadratureRules too Variant 0 with negligible numerical errors Variant 1 Let us  now solve the same problem while elaborating what is going on under the hood At first we load the package by calling Now we define a measure specifically the uniform measure  mathrm{d}\\lambda(t  w(t mathrm{d t  with the weight  w  defined as   w mathcal{W  0,1 rightarrow mathbb{R quad w(t  1 This measure can be defined using the composite type  Uniform01Measure  Next we need to compute the quadrature rule relative to the uniform measure To do this we use the composite type  Quad  This creates a quadrature rule  quadRule_1  relative to the measure  measure  The function  nw  prints the nodes and weights To solve the integral we call  integrate Revisiting Variant 0 Why is the error from variant 0 so much smaller It's because the quadrature rule for variant 0 is based on the recurrence coefficients of the polynomials that are orthogonal relative to the measure  measure  Let's take a closer look First we compute the orthogonal polynomials using the composite type  OrthoPoly  and we set the keyword  addQuadrature  to  false  Note how  op  has a field  EmptyQuad  i.e we computed no quadrature rule The resulting system of orthogonal polynomials is characterized by its recursion coefficients  alpha beta  which can be extracted with the function  coeffs  Now the quadrature rule can be constructed based on  op  and the integral be solved Comparison We see that the different variants provide slightly different results with  variant0  and  variant0_revisited  being the same and more accurate than  variant1  The increased accuracy is based on the fact that for  variant0  and  variant0_revisited  the quadrature rules are based on the recursion coefficients of the underlying orthogonal polynomials The quadrature for  variant1  is based on an general-purpose method that can be significantly less accurate see also the next tutorial  QuadratureRules"},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.coef","title":"coef","text":""},{"doctype":"documentation","id":"references/PolyChaos._checkConsistency","title":"_checkConsistency","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_callback","title":"get_callback","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.is_proper","title":"is_proper","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.phi","title":"phi","text":"Compute the scalar phi functions for all orders up to k The phi functions are defined as varphi_0(z  exp(z),\\quad varphi_{k+1}(z  frac{\\varphi_k(z  1}{z Instead of using the recurrence relation which is numerically unstable a formula given by Sidje is used Sidje R B 1998 Expokit a software package for computing matrix exponentials ACM Transactions on Mathematical Software TOMS 24(1 130-156 Theorem 1 Compute the matrix phi functions for all orders up to k  k   1 The phi functions are defined as varphi_0(z  exp(z),\\quad varphi_{k+1}(z  frac{\\varphi_k(z  1}{z Calls  phiv_dense  on each of the basis vectors to obtain the answer If A is  Diagonal  instead calls the scalar  phi  on each diagonal element and the return values are also  Diagonal s"},{"doctype":"documentation","id":"references/NonlinearSolve.BisectionCache","title":"BisectionCache","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.sparse1","title":"sparse1","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.is_int_gt_one","title":"is_int_gt_one","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timeseries_steps_meancov","title":"timeseries_steps_meancov","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.ADJOINT_PARAMETER_COMPATABILITY_MESSAGE","title":"ADJOINT_PARAMETER_COMPATABILITY_MESSAGE","text":""},{"doctype":"document","id":"DiffEqFlux/examples/hamiltonian_nn.md","title":"Hamiltonian Neural Network","text":"DifferentialEquations Statistics Plots t range length π_32 Float32 π q_t reshape sin π_32 t p_t reshape cos π_32 t dqdt π_32 p_t dpdt π_32 q_t data cat q_t p_t dims target cat dqdt dpdt dims dataloader Flux Data DataLoader data target batchsize shuffle hnn Dense relu Dense p hnn p opt ADAM loss x y p mean hnn x p y callback println loss data target p epochs epoch epochs x y dataloader gs ReverseDiff gradient p loss x y p p Flux Optimise update! opt p gs epoch callback callback model hnn Tsit5 save_everystep save_start saveat t pred Array model data plot data data lw label plot! pred pred lw label xlabel! ylabel! t range length π_32 Float32 π q_t reshape sin π_32 t p_t reshape cos π_32 t dqdt π_32 p_t dpdt π_32 q_t data cat q_t p_t dims target cat dqdt dpdt dims dataloader Flux Data DataLoader data target batchsize shuffle hnn Dense relu Dense p hnn p opt ADAM loss x y p mean hnn x p y callback println loss data target p epochs epoch epochs x y dataloader gs ReverseDiff gradient p loss x y p p Flux Optimise update! opt p gs epoch callback callback model hnn Tsit5 save_everystep save_start saveat t pred Array model data plot data data lw label plot! pred pred lw label xlabel! ylabel! Hamiltonian Neural Network Hamiltonian Neural Networks introduced in 1 allow models to learn and respect exact conservation laws in an unsupervised manner In this example we will train a model to learn the Hamiltonian for a 1D Spring mass system This system is described by the equation m\\ddot(x  kx  0 Now we make some simplifying assumptions and assign  m  1  and  k  1  Analytically solving this equation we get  x  sin(t  Hence  q  sin(t  and  p  cos(t  Using these solutions we generate our dataset and fit the  NeuralHamiltonianDE  to learn the dynamics of this system Step by Step Explanation Data Generation The HNN predicts the gradients  dot(q dot(p  given  q p  Hence we generate the pairs  q p  using the equations given at the top Additionally to supervise the training we also generate the gradients Next we use use Flux DataLoader for automatically batching our dataset Training the HamiltonianNN We parameterize the HamiltonianNN with a small MultiLayered Perceptron HNN also works with the Fast Layers provided in DiffEqFlux HNNs are trained by optimizing the gradients of the Neural Network Zygote currently doesn't support nesting itself so we will be using ReverseDiff in the training loop to compute the gradients of the HNN Layer for Optimization Solving the ODE using trained HNN In order to visualize the learned trajectories we need to solve the ODE We will use the  NeuralHamiltonianDE  layer which is essentially a wrapper over  HamiltonianNN  layer and solves the ODE HNN Prediction Expected Output References 1 Greydanus Samuel Misko Dzamba and Jason Yosinski Hamiltonian Neural Networks Advances in Neural Information Processing Systems 32 2019 15379-15389"},{"doctype":"documentation","id":"references/Catalyst.complexgraph","title":"complexgraph","text":"Creates a Graphviz graph of the  ReactionComplex s in  rn  Reactions correspond to arrows and reaction complexes to blue circles Notes Black arrows from complexes to complexes indicate reactions whose rate is a parameter or a  Number  i.e  k A  B  Red dashed arrows from complexes to complexes indicate reactions whose rate depends on species i.e  k*C A  B  for  C  a species Requires the Graphviz jll to be installed or Graphviz to be installed and commandline accessible"},{"doctype":"documentation","id":"references/ModelingToolkit._defvaridx","title":"_defvaridx","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.JumpSysMajParamMapper","title":"JumpSysMajParamMapper","text":""},{"doctype":"documentation","id":"references/Catalyst.oderatelaw","title":"oderatelaw","text":"Given a  Reaction  return the symbolic reaction rate law used in generated ODEs for the reaction Note for a reaction defined by k*X*Y X+Z  2X  Y the expression that is returned will be  k*X(t)^2*Y(t)*Z(t  For a reaction of the form k 2X+3Y  Z the expression that is returned will be  k  X(t)^2/2  Y(t)^3/6  Notes Allocates combinatoric_ratelaw=true  uses factorial scaling factors in calculating the rate law i.e for  2S  0  at rate  k  the ratelaw would be  k*S^2/2  If  combinatoric_ratelaw=false  then the ratelaw is  k*S^2  i.e the scaling factor is ignored"},{"doctype":"document","id":"NonlinearSolve/solvers/NonlinearSystemSolvers.md","title":"[Nonlinear System Solvers]( nonlinearsystemsolvers)","text":"NLSolveJL method trust_region autodiff central store_trace extended_trace linesearch LineSearches Static linsolve x A b copyto! x A b factor one Float64 autoscale m beta one Float64 show_trace KINSOL linear_solver Dense jac_upper jac_lower userdata nothing Nonlinear System Solvers  nonlinearsystemsolvers solve(prob::NonlinearProblem,alg;kwargs Solves for  f(u)=0  in the problem defined by  prob  using the algorithm  alg  If no algorithm is given a default algorithm will be chosen This page is solely focused on the methods for nonlinear systems Recommended Methods NewtonRaphson  is a good choice for most problems It is non-allocating on static arrays and thus really well-optimized for small systems while for large systems it can make use of sparsity patterns for sparse automatic differentiation and sparse linear solving of very large systems That said as a classic Newton method its stability region can be smaller than other methods  NLSolveJL s  trust_region  method can be a good choice for high stability along with  CMINPACK  For a system which is very non-stiff i.e the condition number of the Jacobian is small or the eigenvalues of the Jacobian are within a few orders of magnitude then  NLSolveJL s  anderson  can be a good choice Full List of Methods NonlinearSolve.jl These are the core solvers NewtonRaphson(;autodiff=true,chunk_size=12,diff_type=Val{:forward},linsolve=DEFAULT_LINSOLVE  A Newton-Raphson method with swappable nonlinear solvers and autodiff methods for high performance on large and sparse systems When used on objects like static arrays this method is non-allocating SciMLNLSolve.jl This is a wrapper package for importing solvers from other packages into this interface Note that these solvers do not come by default and thus one needs to install the package before using these solvers CMINPACK  A wrapper for using the classic MINPACK method through  MINPACK.jl NLSolveJL  A wrapper for  NLsolve.jl Choices for methods in  NLSolveJL  fixedpoint  Fixed-point iteration anderson  Anderson-accelerated fixed-point iteration newton  Classical Newton method with an optional line search trust_region  Trust region Newton method the default choice For more information on these arguments consult the  NLsolve.jl documentation  Sundials.jl This is a wrapper package for the SUNDIALS C library specifically the KINSOL nonlinear solver included in that ecosystem Note that these solvers do not come by default and thus one needs to install the package before using these solvers KINSOL  The KINSOL method of the SUNDIALS C library The choices for the linear solver are Dense  A dense linear solver Band  A solver specialized for banded Jacobians If used you must set the position of the upper and lower non-zero diagonals via  jac_upper  and  jac_lower  LapackDense  A version of the dense linear solver that uses the Julia-provided OpenBLAS-linked LAPACK for multithreaded operations This will be faster than  Dense  on larger systems but has noticeable overhead on smaller 100 ODE systems LapackBand  A version of the banded linear solver that uses the Julia-provided OpenBLAS-linked LAPACK for multithreaded operations This will be faster than  Band  on larger systems but has noticeable overhead on smaller 100 ODE systems Diagonal  This method is specialized for diagonal Jacobians GMRES  A GMRES method Recommended first choice Krylov method BCG  A biconjugate gradient method PCG  A preconditioned conjugate gradient method Only for symmetric linear systems TFQMR  A TFQMR method KLU  A sparse factorization method Requires that the user specify a Jacobian The Jacobian must be set as a sparse matrix in the  ODEProblem  type"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.Tail9","title":"Tail9","text":""},{"doctype":"document","id":"Surrogates/lobachevsky.md","title":"Lobachevsky surrogate tutorial","text":"Lobachevsky surrogate tutorial Lobachevsky splines function is a function that used for univariate and multivariate scattered interpolation Introduced by Lobachevsky in 1842 to investigate errors in astronomical measurements We are going to use a Lobachevsky surrogate to optimize  f(x)=sin(x)+sin(10/3  x  First of all import  Surrogates  and  Plots  Sampling We choose to sample f in 4 points between 0 and 4 using the  sample  function The sampling points are chosen using a Sobol sequence this can be done by passing  SobolSample  to the  sample  function Building a surrogate With our sampled points we can build the Lobachevsky surrogate using the  LobachevskySurrogate  function lobachevsky_surrogate  behaves like an ordinary function which we can simply plot Alpha is the shape parameters and n specify how close you want lobachevsky function to radial basis function Optimizing Having built a surrogate we can now use it to search for minimas in our original function  f  To optimize using our surrogate we call  surrogate_optimize  method We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique In the example below it shows how to use  lobachevsky_surrogate  for higher dimension problems Lobachevsky Surrogate Tutorial ND First of all we will define the  Schaffer  function we are going to build surrogate for Notice one how its argument is a vector of numbers one for each coordinate and its output is a scalar Sampling Let's define our bounds this time we are working in two dimensions In particular we want our first dimension  x  to have bounds  0 8  and  0 8  for the second dimension We are taking 60 samples of the space using Sobol Sequences We then evaluate our function on all of the sampling points Building a surrogate Using the sampled points we build the surrogate the steps are analogous to the 1-dimensional case Optimizing With our surrogate we can now search for the minimas of the function Notice how the new sampled points which were created during the optimization process are appended to the  xys  array This is why its size changes"},{"doctype":"documentation","id":"references/ModelingToolkit.Flow","title":"Flow","text":""},{"doctype":"document","id":"DiffEqSensitivity/neural_ode/GPUs.md","title":"Neural ODEs on GPUs","text":"DifferentialEquations Lux Optim Random rng Random default_rng model_gpu Lux lux Dense tanh Lux Dense gpu p st Lux setup rng model_gpu dudt! u p t model_gpu u p st tspan tsteps u0 Float32 gpu prob_gpu dudt! u0 tspan p sol_gpu prob_gpu Tsit5 saveat tsteps prob_neuralode_gpu gpu dudt2 tspan Tsit5 saveat tsteps dudt2 Lux ActivationFunction x x Lux Dense tanh Lux Dense u0 Float32 gpu p st Lux setup rng dudt2 gpu dudt2_ u p t dudt2 u p st tspan tsteps prob_gpu dudt2_ u0 tspan p sol_gpu prob_gpu Tsit5 saveat tsteps prob_neuralode_gpu dudt2 tspan Tsit5 saveat tsteps prob_neuralode_gpu u0 p st Lux OptimizationOptimJL OrdinaryDiffEq Optim Plots CUDA Random CUDA allowscalar rng Random default_rng u0 Float32 datasize tspan tsteps range tspan tspan length datasize trueODEfunc du u p t true_A du u true_A prob_trueode trueODEfunc u0 tspan ode_data gpu prob_trueode Tsit5 saveat tsteps dudt2 Lux ActivationFunction x x Lux Dense tanh Lux Dense u0 Float32 gpu p st Lux setup rng dudt2 gpu prob_neuralode dudt2 tspan Tsit5 saveat tsteps predict_neuralode p gpu prob_neuralode u0 p st loss_neuralode p pred predict_neuralode p loss sum abs2 ode_data pred loss pred list_plots iter callback p l pred doplot list_plots iter iter list_plots iter display l plt scatter tsteps Array ode_data label scatter! plt tsteps Array pred label push! list_plots plt doplot display plot plt adtype optf x p loss_neuralode x adtype optprob optf Lux ComponentArray p result_neuralode optfunc ADAM cb callback maxiters Neural ODEs on GPUs Note that the differential equation solvers will run on the GPU if the initial condition is a GPU array Thus for example we can define a neural ODE by hand that runs on the GPU if no GPU is available the calculation defaults back to the CPU Or we could directly use the neural ODE layer function like If one is using  Lux.Chain  then the computation takes place on the GPU with  f(x,p,st  if  x   p  and  st  are on the GPU This commonly looks like or via the NeuralODE struct Neural ODE Example Here is the full neural ODE example Note that we use the  gpu  function so that the same code works on CPUs and GPUs dependent on  using CUDA "},{"doctype":"documentation","id":"references/ModelingToolkit.SystemStructures.diffvars_range","title":"diffvars_range","text":""},{"doctype":"documentation","id":"references/SciMLBase.has_observed","title":"has_observed","text":""},{"doctype":"documentation","id":"references/Surrogates.obj2_ND","title":"obj2_ND","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractRODEFunction","title":"AbstractRODEFunction","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.SimpleNoiseProcess","title":"SimpleNoiseProcess","text":"Like  NoiseProcess  but without support for adaptivity This makes it lightweight and slightly faster Warn SimpleNoiseProcess  should not be used with adaptive SDE solvers as it will lead to incorrect results t0  is the first timepoint W0  is the first value of the process Z0  is the first value of the pseudo-process This is necessary for higher order algorithms If it's not needed set to  nothing  dist  the distribution for the steps over time bridge  the bridging distribution Optional but required for adaptivity and interpolating at new values save_everystep  whether to save every step of the Brownian timeseries rng  the local RNG used for generating the random numbers reset  whether to reset the process with each solve reseed  whether to reseed the process with each solve"},{"doctype":"documentation","id":"references/DiffEqSensitivity.LSSSchur","title":"LSSSchur","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.accumulate_cost!","title":"accumulate_cost!","text":""},{"doctype":"documentation","id":"references/SciMLOperators.MatrixOperator","title":"MatrixOperator","text":"Represents a time-dependent linear operator given by an AbstractMatrix The update function is called by  update_coefficients  and is assumed to have the following signature"},{"doctype":"documentation","id":"references/LinearSolve.KrylovKitJL_GMRES","title":"KrylovKitJL_GMRES","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.AbstractBoundaryPadded3Tensor","title":"AbstractBoundaryPadded3Tensor","text":""},{"doctype":"documentation","id":"references/CommonSolve.init","title":"init","text":"iter args kwargs iter prob ProblemType alg SolverType kwargs IterType iter SolutionType Solves an equation or other mathematical problem using the algorithm specified in the arguments Generally the interface is where the keyword arguments are uniform across all choices of algorithms The  iter  type will be different for the different problem types"},{"doctype":"documentation","id":"references/ModelingToolkit.VariableConnectType","title":"VariableConnectType","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.iscomplex","title":"iscomplex","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.SystemStructures.DIFFERENTIAL_VARIABLE","title":"DIFFERENTIAL_VARIABLE","text":""},{"doctype":"documentation","id":"references/GlobalSensitivity.DGSM","title":"DGSM","text":""},{"doctype":"document","id":"DiffEqSensitivity/ode_fitting/second_order_neural.md","title":"Neural Second Order Ordinary Differential Equation","text":"DifferentialEquations Lux OptimizationFlux Random rng Random default_rng u0 Float32 du0 Float32 tspan t range tspan tspan length model Lux Lux Dense tanh Lux Dense p st Lux setup rng model ff du u p t model u p st prob ff du0 u0 tspan p predict p Array prob Tsit5 p p saveat t correct_pos Float32 transpose hcat collect end collect end loss_n_ode p pred predict p sum abs2 correct_pos pred pred data Iterators repeated opt ADAM l1 loss_n_ode p cb p l pred println l l adtype optf x p loss_n_ode x adtype optprob optf p res optprob opt cb cb maxiters Neural Second Order Ordinary Differential Equation The neural ODE focuses and finding a neural network such that u^\\prime  NN(u However in many cases in physics-based modeling the key object is not the velocity but the acceleration knowing the acceleration tells you the force field and thus the generating process for the dynamical system Thus what we want to do is find the force i.e u^{\\prime\\prime  NN(u Note that in order to be the acceleration we should divide the output of the neural network by the mass An example of training a neural network on a second order ODE is as follows"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.SimpleWienerProcess","title":"SimpleWienerProcess","text":"t0 W0 Z0 nothing kwargs t0 W0 Z0 nothing kwargs The  SimpleWienerProcess  also known as Brownian motion or the noise in the Langevin equation is the stationary process with white noise increments and a distribution  N(0,dt  The constructor is Unlike WienerProcess this uses the SimpleNoiseProcess and thus does not support adaptivity but is slightly more lightweight"},{"doctype":"documentation","id":"references/PolyChaos.build_w_gamma","title":"build_w_gamma","text":""},{"doctype":"documentation","id":"references/NeuralOperators.construct_subnet","title":"construct_subnet","text":"Construct a Chain of  Dense  layers from a given tuple of integers Input A tuple m,n,o,p of integer type numbers that each describe the width of the i-th Dense layer to Construct Output A  Flux  Chain with length of the input tuple and individual width given by the tuple elements Example"},{"doctype":"document","id":"Optimization/optimization_packages/nomad.md","title":"NOMAD.jl","text":"Pkg Pkg add rosenbrock x p p x p x x x0 zeros p f rosenbrock prob f x0 _p sol prob NOMADOpt prob f x0 _p lb ub sol prob NOMADOpt NOMAD.jl NOMAD  is Julia package interfacing to NOMAD,which is a C implementation of the Mesh Adaptive Direct Search algorithm MADS designed for difficult blackbox optimization problems These problems occur when the functions defining the objective and constraints are the result of costly computer simulations  NOMAD.jl documentation The NOMAD algorithm is called by  NOMADOpt Installation OptimizationNOMAD.jl To use this package install the OptimizationNOMAD package Global Optimizer Without Constraint Equations The method in  NOMAD  is performing global optimization on problems both with and without constraint equations Currently however linear and nonlinear constraints  defined in  Optimization  are not passed NOMAD works both with and without lower and upper boxconstraints set by  lb  and  ub  in the  OptimizationProblem  Examples The Rosenbrock function can optimized using the  NOMADOpt  with and without boxcontraints as follows"},{"doctype":"documentation","id":"references/ModelingToolkit.isconnector","title":"isconnector","text":""},{"doctype":"documentation","id":"references/MethodOfLines.generate_finite_difference_rules","title":"generate_finite_difference_rules","text":"generate_finite_difference_rules Generate a vector of finite difference rules to dictate what to replace variables in the  pde  with at the gridpoint  II  Care is taken to make sure that the rules only use points that are actually in the discretized grid by progressively up/downwinding the stencils when the gridpoint  II  is close to the boundary There is a genral catch all ruleset that uses the cartesian centered difference scheme for derivatives and simply the discretized variable at the given gridpoint for particular variables There are of course more specific schemes that are used to improve stability/speed/accuracy when particular forms are encountered in the PDE These rules are applied first to override the general ruleset Currently implemented special cases are as follows  Spherical derivatives  Nonlinear laplacian uses a half offset centered scheme for the inner derivative to improve stability  Spherical nonlinear laplacian  Up/Downwind schemes to be used for odd ordered derivatives multiplied by a coefficient downwinding when the coefficient is positive and upwinding when the coefficient is negative Please submit an issue if you know of any special cases which impact stability or accuracy that are not implemented with links to papers and/or code that demonstrates the special case"},{"doctype":"documentation","id":"references/LinearSolve.defaultalg","title":"defaultalg","text":""},{"doctype":"documentation","id":"references/CommonSolve.solve","title":"solve","text":"args kwargs prob ProblemType alg SolverType kwargs SolutionType args kwargs args kwargs Solves an equation or other mathematical problem using the algorithm specified in the arguments Generally the interface is where the keyword arguments are uniform across all choices of algorithms By default  solve  defaults to using  solve  on the iterator form i.e"},{"doctype":"documentation","id":"references/ModelingToolkit","title":"ModelingToolkit","text":"DocStringExtensions.Readme"},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.set_neighbors!","title":"set_neighbors!","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.JacVecOperator","title":"JacVecOperator","text":""},{"doctype":"documentation","id":"references/MethodOfLines.generate_bc_eqs!","title":"generate_bc_eqs!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.numerical_nlsolve","title":"numerical_nlsolve","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.calculate_tgrad","title":"calculate_tgrad","text":"sys Calculate the time gradient of a system Returns a vector of  Num  instances The result from the first call will be cached in the system object"},{"doctype":"documentation","id":"references/ModelingToolkit.has_domain","title":"has_domain","text":""},{"doctype":"documentation","id":"references/SciMLBase.cleansyms","title":"cleansyms","text":""},{"doctype":"documentation","id":"references/LabelledArrays.SLVector","title":"SLVector","text":"NamedTuple kwargs a b a b The standard constructors for  SLArray  Constructing copies with some items changed is supported by a keyword constructor whose first argument is the source and whose additional keyword arguments indicate the changes Additional examples Creates a 1D copy of v1 with corresponding items in kwargs replaced For example"},{"doctype":"documentation","id":"references/DiffEqSensitivity.build_grad_config","title":"build_grad_config","text":""},{"doctype":"documentation","id":"references/SciMLBase.SplitSDEProblem","title":"SplitSDEProblem","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/RecursiveArrayTools.recursive_mean","title":"recursive_mean","text":""},{"doctype":"documentation","id":"references/PolyChaos.calculateAffinePCE","title":"calculateAffinePCE","text":"Computes the affine PCE coefficients  x_0  and  x_1  from recurrence coefficients  \u0007lpha "},{"doctype":"documentation","id":"references/Catalyst.cache_conservationlaw_eqs!","title":"cache_conservationlaw_eqs!","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.SensitivityAlg","title":"SensitivityAlg","text":""},{"doctype":"document","id":"DiffEqSensitivity/bayesian/BayesianNODE_SGLD.md","title":"Bayesian Neural ODEs: SGLD","text":"DifferentialEquations Flux Plots StatsPlots u0 Float32 p datasize tspan tsteps tspan tspan lv u p t x y u α β γ δ p dx α x β x y dy δ x y γ y du dx dy trueodeprob lv u0 tspan p ode_data Array trueodeprob Tsit5 saveat tsteps y_train ode_data dudt tanh prob_node dudt Tsit5 saveat tsteps train_prob dudt Tsit5 saveat tsteps predict p Array train_prob u0 p loss p sum abs2 y_train predict p sgld ∇L θᵢ t a b γ ϵ a b t γ η ϵ randn size θᵢ Δθᵢ ϵ ∇L η θᵢ Δθᵢ losses Float64 grad_norm Float64 θ deepcopy prob_node p t grad gradient loss θ sgld grad θ t tmp deepcopy θ append! losses loss θ append! grad_norm sum abs2 grad append! tmp println loss θ plot losses yscale log10 plot grad_norm yscale log10 StatsPlots sampled_par sampled_loss loss p p sampled_par density sampled_loss _ i_min findmin sampled_loss plt scatter tsteps ode_data colour blue label ylim scatter! plt tsteps ode_data colour red label phase_plt scatter ode_data ode_data colour red label xlim ylim p sampled_par s prob_node u0 p plot! plt tsteps s colour blue lalpha label none plot! plt tsteps end s end colour purple lalpha label none plot! plt tsteps s colour red lalpha label none plot! plt tsteps end s end colour purple lalpha label none plot! phase_plt s s colour red lalpha label none plot! phase_plt s end s end colour purple lalpha label none plt phase_plt plot! plt seriestype vline colour green linestyle dash label bestfit prob_node u0 sampled_par i_min plot bestfit plot! plt tsteps bestfit colour black label plot! plt tsteps end bestfit end colour purple label plot! plt tsteps bestfit colour black label none plot! plt tsteps end bestfit end colour purple label none plot! phase_plt bestfit bestfit colour black label plot! phase_plt bestfit end bestfit end colour purple label savefig plt savefig phase_plt u0 Float32 p datasize tspan tsteps tspan tspan lv u p t x y u α β γ δ p dx α x β x y dy δ x y γ y du dx dy trueodeprob lv u0 tspan p ode_data Array trueodeprob Tsit5 saveat tsteps y_train ode_data dudt tanh prob_node dudt Tsit5 saveat tsteps train_prob dudt Tsit5 saveat tsteps predict p Array train_prob u0 p loss p sum abs2 y_train predict p sgld ∇L θᵢ t a b γ ϵ a b t γ η ϵ randn size θᵢ Δθᵢ ϵ ∇L η θᵢ Δθᵢ losses Float64 grad_norm Float64 θ deepcopy prob_node p t grad gradient loss θ sgld grad θ t tmp deepcopy θ append! losses loss θ append! grad_norm sum abs2 grad append! tmp println loss θ plot losses yscale log10 plot grad_norm yscale log10 StatsPlots sampled_par _ i_min findmin sampled_loss plt scatter tsteps ode_data colour blue label ylim scatter! plt tsteps ode_data colour red label phase_plt scatter ode_data ode_data colour red label xlim ylim p sampled_par s prob_node u0 p plot! plt tsteps s colour blue lalpha label none plot! plt tsteps end s end colour purple lalpha label none plot! plt tsteps s colour red lalpha label none plot! plt tsteps end s end colour purple lalpha label none plot! phase_plt s s colour red lalpha label none plot! phase_plt s end s end colour purple lalpha label none plt phase_plt plot! plt seriestype vline colour green linestyle dash label bestfit prob_node u0 sampled_par i_min plot bestfit plot! plt tsteps bestfit colour black label plot! plt tsteps end bestfit end colour purple label plot! plt tsteps bestfit colour black label none plot! plt tsteps end bestfit end colour purple label none plot! phase_plt bestfit bestfit colour black label plot! phase_plt bestfit end bestfit end colour purple label Bayesian Neural ODEs SGLD Recently Neural Ordinary Differential Equations has emerged as a powerful framework for modeling physical simulations without explicitly defining the ODEs governing the system but learning them via machine learning However the question Can Bayesian learning frameworks be integrated with Neural ODEs to robustly quantify the uncertainty in the weights of a Neural ODE remains unanswered In this tutorial a working example of the Bayesian Neural ODE SGLD sampler is shown SGLD stands for Stochastic Langevin Gradient Descent For an introduction to SGLD please refer to  Introduction to SGLD in Julia For more details regarding Bayesian Neural ODEs please refer to  Bayesian Neural Ordinary Differential Equations  Copy-Pasteable Code Before getting to the explanation here's some code to start with We will follow with a full explanation of the definition and training process Time Series Plots Contour Plots Explanation Step1 Get the data from the Lotka Volterra ODE example Step2 Define the Neural ODE architecture Note that this step potentially offers a lot of flexibility in the number of layers number of units in each layer Step3 Define the loss function for the Neural ODE Step4 Now we start integrating the Stochastic Langevin Gradient Descent(SGLD framework The SGLD Stochastic Langevin Gradient Descent sampler is seen to have a better performance than NUTS whose tutorial is also shown in a separate document Have a look at https://sebastiancallh.github.io/post/langevin for a quick introduction to SGLD Note that we sample from the last 2000 iterations Step5 Plot Retrodicted Plots Estimation and Forecasting"},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.find_solve_sequence","title":"find_solve_sequence","text":"given a set of  vars  find the groups of equations we need to solve for to obtain the solution to  vars"},{"doctype":"documentation","id":"references/RecursiveArrayTools.vecvec_to_mat","title":"vecvec_to_mat","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.EMPTY_TGRAD","title":"EMPTY_TGRAD","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.collect_terms","title":"collect_terms","text":""},{"doctype":"documentation","id":"references/SciMLBase.ratenoise_cache","title":"ratenoise_cache","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.SystemStructures.DERIVATIVE_VARIABLE","title":"DERIVATIVE_VARIABLE","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.assemble_crj","title":"assemble_crj","text":""},{"doctype":"documentation","id":"references/NeuralPDE.LogOptions","title":"LogOptions","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.StochasticTransformedFunction","title":"StochasticTransformedFunction","text":""},{"doctype":"documentation","id":"references/NeuralPDE.get_loss_function_","title":"get_loss_function_","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.empty_substitutions","title":"empty_substitutions","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.perpindex","title":"perpindex","text":""},{"doctype":"document","id":"Surrogates/InverseDistance.md","title":"InverseDistance","text":"The  Inverse Distance Surrogate  is an interpolating method and in this method the unknown points are calculated with a weighted average of the sampling points This model uses the inverse distance between the unknown and training points to predict the unknown point We do not need to fit this model because the response of an unknown point x is computed with respect to the distance between x and the training points Let's optimize following function to use Inverse Distance Surrogate f(x  sin(x  sin(x)^2  sin(x)^3  First of all we have to import these two packages  Surrogates  and  Plots  Sampling We choose to sample f in 25 points between 0 and 10 using the  sample  function The sampling points are chosen using a Low Discrepancy this can be done by passing  LowDiscrepancySample  to the  sample  function Building a Surrogate Now we will simply plot  InverseDistance  Optimizing Having built a surrogate we can now use it to search for minimas in our original function  f  To optimize using our surrogate we call  surrogate_optimize  method We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique Inverse Distance Surrogate Tutorial ND First of all we will define the  Schaffer  function we are going to build surrogate for Notice one how its argument is a vector of numbers one for each coordinate and its output is a scalar Sampling Let's define our bounds this time we are working in two dimensions In particular we want our first dimension  x  to have bounds  5 10  and  0 15  for the second dimension We are taking 60 samples of the space using Sobol Sequences We then evaluate our function on all of the sampling points Building a surrogate Using the sampled points we build the surrogate the steps are analogous to the 1-dimensional case Optimizing With our surrogate we can now search for the minimas of the function Notice how the new sampled points which were created during the optimization process are appended to the  xys  array This is why its size changes"},{"doctype":"documentation","id":"references/ExponentialUtilities.expv_timestep","title":"expv_timestep","text":"Evaluates the matrix exponentiation-vector product using time stepping u  exp(tA)b ts  is an array of time snapshots for u with  U[:,j ≈ u(ts[j   ts  can also be just one value in which case only the end result is returned and  U  is a vector The time stepping formula of Niesen  Wright is used   If the time step  tau  is not specified it is chosen according to 17 of Neisen  Wright If  adaptive==true  the time step and Krylov subsapce size adaptation scheme of Niesen  Wright is used the relative tolerance of which can be set using the keyword parameter  tol  The delta and gamma parameter of the adaptation scheme can also be adjusted Set  verbose=true  to print out the internal steps for debugging For the other keyword arguments consult  arnoldi  and  phiv  which are used internally Note that this function is just a special case of  phiv_timestep  with a more intuitive interface vector  b  instead of a n-by-1 matrix  B  Niesen J  Wright W 2009 A Krylov subspace algorithm for evaluating the φ-functions in exponential integrators arXiv preprint arXiv:0907.4631"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.GeometricBrownianBridge!","title":"GeometricBrownianBridge!","text":""},{"doctype":"documentation","id":"references/SciMLBase.add_kwonly","title":"add_kwonly","text":""},{"doctype":"documentation","id":"references/SciMLBase.PDEProblem","title":"PDEProblem","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.find_poles","title":"find_poles","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.AbstractControlSystem","title":"AbstractControlSystem","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.value_derivative","title":"value_derivative","text":"value_derivative(f x Compute  f(x d/dx f(x  in the most efficient way"},{"doctype":"documentation","id":"references/SciMLBase.AbstractAnalyticalSolution","title":"AbstractAnalyticalSolution","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.CompoundPoissonProcess","title":"CompoundPoissonProcess","text":"https://www.math.wisc.edu/~anderson/papers/AndPostleap.pdf Incorporating postleap checks in tau-leaping J Chem Phys 128 054103 2008 https://doi.org/10.1063/1.2819665"},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.tearing_assignments","title":"tearing_assignments","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.has_preface","title":"has_preface","text":""},{"doctype":"documentation","id":"references/SciMLBase.sym_to_index","title":"sym_to_index","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.ldiv_for_generated!","title":"ldiv_for_generated!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.controls","title":"controls","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.bound_outputs","title":"bound_outputs","text":"Return outputs that are bound within the system i.e internal outputs See also  bound_inputs   unbound_inputs   bound_outputs   unbound_outputs"},{"doctype":"documentation","id":"references/DiffEqSensitivity.VJPChoice","title":"VJPChoice","text":""},{"doctype":"documentation","id":"references/SciMLBase.update_coefficients","title":"update_coefficients","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.find_eq_solvables!","title":"find_eq_solvables!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.maximal_matching","title":"maximal_matching","text":"For a bipartite graph  g  construct a maximal matching of destination to source vertices subject to the constraint that vertices for which  srcfilter  or  dstfilter  return  false  may not be matched"},{"doctype":"documentation","id":"references/GlobalSensitivity.gsa_sobol_all_y_analysis","title":"gsa_sobol_all_y_analysis","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.collect_operator_variables","title":"collect_operator_variables","text":"Return a  Set  containing all variables that have Operator  op  applied to them See also  collect_differential_variables   collect_difference_variables "},{"doctype":"document","id":"DiffEqSensitivity/neural_ode/neural_gde.md","title":"Neural Graph Differential Equations","text":"GeometricFlux JLD2 SparseArrays DifferentialEquations Flux onehotbatch onecold throttle Flux Losses logitcrossentropy Statistics mean LightGraphs adjacency_matrix download download download features labels g num_nodes num_features hidden target_catg epochs train_X Float32 features train_y Float32 labels adj_mat FeaturedGraph Matrix Float32 adjacency_matrix g diffeqarray_to_array x reshape cpu x size x node GCNConv adj_mat hidden hidden Tsit5 save_everystep reltol abstol save_start model GCNConv adj_mat num_features hidden relu Dropout node diffeqarray_to_array GCNConv adj_mat hidden target_catg loss x y logitcrossentropy model x y accuracy x y mean onecold model x onecold y ps Flux model node p train_data train_X train_y opt ADAM evalcb accuracy train_X train_y i epochs Flux train! loss ps train_data opt cb throttle evalcb GeometricFlux JLD2 SparseArrays DifferentialEquations Flux onehotbatch onecold throttle Flux Losses crossentropy Statistics mean LightGraphs adjacency_matrix download download download features labels g num_nodes num_features hidden target_catg epochs train_X Float32 features train_y Float32 labels adj_mat Matrix Float32 adjacency_matrix g diffeqarray_to_array x reshape cpu x size x node GCNConv adj_mat hidden hidden Tsit5 save_everystep reltol abstol save_start model GCNConv adj_mat num_features hidden relu Dropout node diffeqarray_to_array GCNConv adj_mat hidden target_catg loss x y logitcrossentropy model x y accuracy x y mean onecold model x onecold y ps Flux model node p train_data train_X train_y opt ADAM evalcb accuracy train_X train_y i epochs Flux train! loss ps train_data opt cb throttle evalcb accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y accuracy train_X train_y Neural Graph Differential Equations This tutorial has been adapted from  here  In this tutorial we will use Graph Differential Equations GDEs to perform classification on the  CORA Dataset  We shall be using the Graph Neural Networks primitives from the package  GeometricFlux  Step by Step Explanation Load the Required Packages Load the Dataset The dataset is available in the desired format in the GeometricFlux repository We shall download the dataset from there and use the JLD2 package to load the data Model and Data Configuration The  num_nodes   target_catg  and  num_features  are defined by the data itself We shall use a shallow GNN with only 16 hidden state dimension Preprocessing the Data Convert the data to float32 and use  LightGraphs  to get the adjacency matrix from the graph  g  Neural Graph Ordinary Differential Equations Let us now define the final model We will use a single layer GNN for approximating the gradients for the neural ODE We use two additional  GCNConv  layers one to project the data to a latent space and the other to project it from the latent space to the predictions Finally a softmax layer gives us the probability of the input belonging to each target category Training Configuration Loss Function and Accuracy We shall be using the standard categorical crossentropy loss function which is used for multiclass classification tasks Model Parameters Now we extract the model parameters which we want to learn Training Data GNNs operate on an entire graph so we can't do any sort of minibatching here We need to pass the entire data in a single pass So our dataset is an array with a single tuple Optimizer For this task we will be using the  ADAM  optimizer with a learning rate of  0.01  Callback Function We also define a utility function for printing the accuracy of the model over time Training Loop Finally with the configuration ready and all the utilities defined we can use the  Flux.train  function to learn the parameters  ps  We run the training loop for  epochs  number of iterations Expected Output"},{"doctype":"documentation","id":"references/ModelingToolkit.get_substitutions_and_solved_states","title":"get_substitutions_and_solved_states","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs._vsrc","title":"_vsrc","text":""},{"doctype":"documentation","id":"references/Catalyst.get_tup_arg","title":"get_tup_arg","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.leading","title":"leading","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.decompose_rational","title":"decompose_rational","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.GeometricBrownianMotion!","title":"GeometricBrownianMotion!","text":""},{"doctype":"documentation","id":"references/SciMLBase.islinear","title":"islinear","text":""},{"doctype":"documentation","id":"references/Catalyst.indent","title":"indent","text":""},{"doctype":"documentation","id":"references/MethodOfLines.discretize_equation","title":"discretize_equation","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.Unassigned","title":"Unassigned","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.WHITE_NOISE_BRIDGE","title":"WHITE_NOISE_BRIDGE","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.expv","title":"expv","text":"Compute the matrix-exponential-vector product using Krylov A Krylov subspace is constructed using  arnoldi  and  exp  is called on the Hessenberg matrix Consult  arnoldi  for the values of the keyword arguments An alternative algorithm where an error estimate generated on-the-fly is used to terminate the Krylov iteration can be employed by setting the kwarg  mode=:error_estimate  Compute the expv product using a pre-constructed Krylov subspace"},{"doctype":"documentation","id":"references/ModelingToolkit.@named","title":"@named","text":"Rewrite  named y  foo(x  to  y  foo(x name=:y  Rewrite  named y[1:10  foo(x  to  y  map(i′->foo(x name=Symbol(:y i′ 1:10  Rewrite  named y 1:10 i  foo(x*i  to  y  map(i->foo(x*i name=Symbol(:y i 1:10  Examples"},{"doctype":"documentation","id":"references/Catalyst.subnetworks","title":"subnetworks","text":"sir SIR β S I I ν I R β ν complexes sir sir Find subnetworks corresponding to each linkage class of the reaction network Notes Requires the  incidencemat  to already be cached in  rn  by a previous call to  reactioncomplexes  For example"},{"doctype":"documentation","id":"references/SciMLBase.full_cache","title":"full_cache","text":"Returns an iterator over the cache arrays of the method This can be used to change internal values as needed"},{"doctype":"documentation","id":"references/ModelingToolkit.SystemStructures.dervars_range","title":"dervars_range","text":""},{"doctype":"documentation","id":"references/MethodOfLines.interface_errors","title":"interface_errors","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.isirreducible","title":"isirreducible","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.indent","title":"indent","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.get_coef","title":"get_coef","text":""},{"doctype":"documentation","id":"references/Catalyst.conservedequations","title":"conservedequations","text":"Calculate symbolic equations from conservation laws writing dependent variables as functions of independent variables and the conservation law constants Notes Caches the resulting equations in  rn  so will be fast on subsequent calls Examples gives"},{"doctype":"document","id":"MethodOfLines/generated/bruss_ode_eqs.md","title":"[Generated ODE system for the Brusselator Equation] ( brusssys)","text":"Differential t u t u t u t u t u t u t u t u t v t u t Differential t u t u t u t u t u t u t u t u t v t u t Differential t u t u t u t u t u t u t u t u t v t u t Differential t u t u t u t u t u t u t u t u t v t u t Differential t u t u t u t u t u t u t u t u t v t u t Differential t u t u t u t u t u t u t u t u t v t u t Differential t u t u t u t u t u t u t u t u t v t u t Differential t u t u t u t u t u t u t u t u t v t u t Differential t u t u t u t u t u t u t u t u t v t u t Differential t u t u t u t u t u t u t u t u t v t u t Differential t u t u t u t u t u t u t u t u t v t u t Differential t u t u t u t u t u t u t u t u t v t u t Differential t u t u t u t u t u t u t u t u t v t u t Differential t u t u t u t u t u t u t u t u t v t u t Differential t u t u t u t u t u t u t u t u t v t u t Differential t u t u t u t u t u t u t u t u t v t u t Differential t v t v t v t v t v t v t v t u t u t v t Differential t v t v t v t v t v t v t v t u t u t v t Differential t v t v t v t v t v t v t v t u t u t v t Differential t v t v t v t v t v t v t v t u t u t v t Differential t v t v t v t v t v t v t v t u t u t v t Differential t v t v t v t v t v t v t v t u t u t v t Differential t v t v t v t v t v t v t v t u t u t v t Differential t v t v t v t v t v t v t v t u t u t v t Differential t v t v t v t v t v t v t v t u t u t v t Differential t v t v t v t v t v t v t v t u t u t v t Differential t v t v t v t v t v t v t v t u t u t v t Differential t v t v t v t v t v t v t v t u t u t v t Differential t v t v t v t v t v t v t v t u t u t v t Differential t v t v t v t v t v t v t v t u t u t v t Differential t v t v t v t v t v t v t v t u t u t v t Differential t v t v t v t v t v t v t v t u t u t v t Generated ODE system for the Brusselator Equation   brusssys Here's the generated system of equations for the Brusselator  brusselator with  dx  dy  1/4 Equations for u Equations for v Boundary condition Equations On the call to ODEProblem this code  brusscode is generated"},{"doctype":"documentation","id":"references/SciMLBase.savevalues!","title":"savevalues!","text":"Try to save the state and time variables at the current time point or the  saveat  point by using interpolation when appropriate It returns a tuple that is  saved savedexactly  If  savevalues  saved value then  saved  is true and if  savevalues  saved at the current time point then  savedexactly  is true The saving priority/order is as follows save_on saveat force_save save_everystep"},{"doctype":"documentation","id":"references/DiffEqSensitivity.adjointdiffcache","title":"adjointdiffcache","text":"return AdjointDiffCache y"},{"doctype":"documentation","id":"references/SciMLBase.__has_Wfact","title":"__has_Wfact","text":""},{"doctype":"document","id":"DiffEqFlux/layers/SplineLayer.md","title":"Spline Layer","text":"Spline Layer Constructs a Spline Layer At a high-level it performs the following Takes as input a one-dimensional training dataset a time span a time step and an interpolation method During training adjusts the values of the function at multiples of the time-step such that the curve interpolated through these points has minimum loss on the corresponding one-dimensional dataset"},{"doctype":"documentation","id":"references/PolyChaos.AbstractOrthoPoly","title":"AbstractOrthoPoly","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractDAEFunction","title":"AbstractDAEFunction","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/ExponentialUtilities.realview","title":"realview","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.attempt","title":"attempt","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.NILSASSensitivityFunction","title":"NILSASSensitivityFunction","text":""},{"doctype":"documentation","id":"references/ModelingToolkit._validate","title":"_validate","text":""},{"doctype":"documentation","id":"references/LinearSolve._ldiv!","title":"_ldiv!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.has_torn_matching","title":"has_torn_matching","text":""},{"doctype":"documentation","id":"references/Surrogates._center_bounds","title":"_center_bounds","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.compute_a!","title":"compute_a!","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.jacNoise!","title":"jacNoise!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.merge_cb","title":"merge_cb","text":""},{"doctype":"documentation","id":"references/MethodOfLines.AbstractExtendingBoundary","title":"AbstractExtendingBoundary","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_ctrl_jac","title":"get_ctrl_jac","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.BoundaryPaddedVector","title":"BoundaryPaddedVector","text":"A vector type that extends a vector u with one ghost point at each end"},{"doctype":"document","id":"Surrogates/lp.md","title":"Lp norm function","text":"Lp norm function The Lp norm function is defined as  f(x  sqrt[p sum_{i=1}^d vert x_i vert p Let's import Surrogates and Plots Define the objective function Let's see a simple 1D case Fitting different Surrogates"},{"doctype":"document","id":"PolyChaos/scalar_products.md","title":"[Computation of Scalar Products]( ComputationOfScalarProducts)","text":"Computation of Scalar Products  ComputationOfScalarProducts By now we are able to construct orthogonal polynomials and to construct quadrature rules for a given nonnegative weight function respectively Now we combine both ideas to solve integrals involving the orthogonal polynomials langle phi_{i_1 phi_{i_2 cdots phi_{i_{m-1 phi_{i_m rangle\n int phi_{i_1}(t phi_{i_2}(t cdots phi_{i_{m-1}}(t phi_{i_m}(t w(t mathrm{d t both for the univariate and multivariate case The integrand is a polynomial possibly multivariate that can be solved exactly with the appropriate Gauss quadrature rules Note To simplify notation we drop the integration interval It is clear from the context Univariate Polynomials Classical Polynomials Let's begin with a univariate basis for some  classical  orthogonal polynomial By setting  addQuadrature  true  which is default an  n point Gauss quadrature rule is create relative to the underlying measure  opq.measure  where  n  is the number of recurrence coefficients stored in  opq.α  and  opq.β  To compute the squared norms  phi_k 2  langle phi_k phi_k  rangle\n int phi_k(t phi_k(t w(t mathrm{d t of the basis we call  computeSP2 For the general case langle phi_{i_1 phi_{i_2 cdots phi_{i_{m-1 phi_{i_m rangle\n int phi_{i_1}(t phi_{i_2}(t cdots phi_{i_{m-1}}(t phi_{i_m}(t w(t mathrm{d t there exists a type  Tensor  that requires only two arguments the  dimension   m geq 1  and an  AbstractOrthoPoly To get the desired entries  Tensor  comes with a  get  function that is called for some index  a in mathbb{N}_0^m  that has the entries  a  i_1 i_2 dots i_m  For example Or using comprehension Notice that we can cross-check the results Also  normsq  can be computed analogously in  Tensor  format Arbitrary Weights Of course the type  OrthoPoly  can be constructed for arbitrary weights  w(t  In this case we have to compute the orthogonal basis and the respective quadrature rule Let's re-work the above example by hand Now we can compute the squared norms   phi_k 2 And the tensor Let's compare the results Note The possibility to create quadrature rules for arbitrary weights should be reserved to cases different from  classical  ones Multivariate Polynomials For multivariate polynomials the syntax for  Tensor  is very much alike except that we are dealing with the type  MultiOrthoPoly  now Notice that  mT2  carries the elements of the 2-dimensional tensors for the univariate bases  opq  and  my_opq  The encoding is given by the multi-index  mop.ind To cross-check the results we can distribute the multi-index back to its univariate indices with the help of  findUnivariateIndices "},{"doctype":"documentation","id":"references/ModelingToolkit.SymScope","title":"SymScope","text":""},{"doctype":"document","id":"PolyChaos/index.md","title":"Overview","text":"julia Pkg clone Pkg dir Overview PolyChaos is a collection of numerical routines for orthogonal polynomials written in the  Julia  programming language Starting from some non-negative weight aka an absolutely continuous nonnegative measure PolyChaos allows to compute the coefficients for the monic three-term recurrence relation to evaluate the orthogonal polynomials at arbitrary points to compute the quadrature rule to compute tensors of scalar products to do all of the above in a multivariate setting aka product measures If the weight function is a probability density function PolyChaos further provides routines to compute  polynomial chaos expansions  PCEs of random variables with this very density function These routines allow to compute affine PCE coefficients for arbitrary densities to compute moments to compute the tensors of scalar products PolyChaos contains several  canonical  orthogonal polynomials such as Jacobi or Hermite polynomials For these closed-form expressions and state-of-the art quadrature rules are used whenever possible However a cornerstone principle of PolyChaos is to provide all the functionality for user-specific arbitrary weights Note What PolyChaos is not at least currently a self-contained introduction to orthogonal polynomials quadrature rules and/or polynomial chaos expansions We assume the user brings some experience to the table However over time we will focus on strengthening the tutorial charater of the package a symbolic toolbox a replacement for  FastGaussQuadrature.jl Installation The package requires  Julia 1.3  or newer In  Julia  switch to the package manager This will install PolyChaos and its dependencies Once that is done load the package That's it Let's take a look at a simple example We would like to solve the integral int_0^1 6 x^5 mathrm{d}x Exploiting the underlying uniform measure the integration can be done exactly with a 3-point quadrature rule To get going with PolyChaos check out the tutorials such as the one on numerical integration  NumericalIntegration In case you are unfamiliar with orthogonal polynomials perhaps this background information  MathematicalBackground is of help References The code base of  PolyChaos  is partially based on Walter Gautschi's  Matlab suite of programs for generating orthogonal polynomials and related quadrature rules  with much of the theory presented in his book  Orthogonal Polynomials Computation and Approximation  published in 2004 by the Oxford University Press For the theory of polynomial chaos expansion we mainly consulted T J Sullivan  Introduction to Uncertainty Quantification  Springer International Publishing Switzerland 2015 Contributing We are always looking for contributors If you are interested just get in touch tillmann dot muehlpfordt at kit dot edu Or just fork and/or star the repository Julia's package manager works nicely with Github simply install the hosted package via  Pkg.clone  and the  repository's URL  A fork is created with The fork will replace the original package Call to figure out where the package was cloned to Go to that location and figure out what branch you are on via  git branch  Citing If you found the software useful and applied it to your own research we'd appreciate a citation Add the following to your BibTeX file Of course you are more than welcome to partake in GitHub's gamification starring and forking is much appreciated Enjoy"},{"doctype":"document","id":"GlobalSensitivity/methods/dgsm.md","title":"Derivative based Global Sensitivity Measure Method","text":"crossed Bool Test Distributions samples f1 x x x x dist1 Uniform Normal Beta b f1 dist1 samples samples Derivative based Global Sensitivity Measure Method The keyword arguments for DGSM are as follows crossed  A string(True/False which act as indicator for computation of DGSM crossed indices Defaults to  false  Method Details The DGSM method takes a probability distribution for each of the parameters and samples are obtained from the distributions to create random parameter sets Derivatives of the function being analysed are then computed at the sampled parameters and specific statistics of those derivatives are used The paper by  Sobol and Kucherenko  discusses the relationship between the DGSM results  tao  and  sigma  and the Morris elementary effects and Sobol Indices API dist  Array of distribution of respective variables E.g  dist  Normal(5,6),Uniform(2,3  for two variables Example"},{"doctype":"documentation","id":"references/MethodOfLines.split_additive_terms","title":"split_additive_terms","text":""},{"doctype":"document","id":"NeuralPDE/solvers/kolmogorovbackwards_solver.md","title":"Neural Network Solvers for Kolmogorov Backwards Equations","text":"μ σ u0 tspan xspan d μ σ tspan xspan d m Dense σ Dense σ Dense m fmap cu m prob m opt sdealg ensemblealg use_gpu verbose dt dt dx dx trajectories trajectories abstol maxiters Neural Network Solvers for Kolmogorov Backwards Equations A Kolmogorov PDE is of the form  Considering S to be a solution process to the SDE then the solution to the Kolmogorov PDE is given as A Kolmogorov PDE Problem can be defined using a  SDEProblem  Here  u0  is the initial distribution of x Here we define  u(0,x  as the probability density function of  u0  μ  and  σ  are obtained from the SDE for the stochastic process above  d  represents the dimensions of  x   u0  can be defined using  Distributions.jl  Another way of defining a KolmogorovPDE is to use the  KolmogorovPDEProblem  Here  phi  is the initial condition on u(t,x when t  0  μ  and  σ  are obtained from the SDE for the stochastic process above  d  represents the dimensions of  x  To solve this problem use NNKolmogorov(chain opt  sdealg  Uses a neural network to realize a regression function which is the solution for the linear Kolmogorov Equation Here  chain  is a Flux.jl chain with a  d dimensional input and a 1-dimensional output opt  is a Flux.jl optimizer And  sdealg  is a high-order algorithm to calculate the solution for the SDE which is used to define the learning data for the problem Its default value is the classic Euler-Maruyama algorithm Using GPU for Kolmogorov Equations For running Kolmogorov Equations on a GPU there are certain aspects that are need to be taken care of Convert the model parameters to  CuArrays  using the  fmap  function given by Flux.jl Unlike other solvers we need to specify explicitly that the solver is to run on the GPU This can be done by passing the  use_gpu  true  into the solver"},{"doctype":"documentation","id":"references/SciMLBase.intervals","title":"intervals","text":""},{"doctype":"documentation","id":"references/Integrals.ReCallVJP","title":"ReCallVJP","text":""},{"doctype":"document","id":"LinearSolve/basics/FAQ.md","title":"Frequently Asked Questions","text":"LinearAlgebra Pl Diagonal weights Pr Diagonal weights A rand n n b rand n prob A b sol prob IterativeSolvers_GMRES Pl Pl Pr Pr LinearAlgebra Pl ComposePreconitioner Diagonal weights realprec Pr Diagonal weights A rand n n b rand n prob A b sol prob IterativeSolvers_GMRES Pl Pl Pr Pr Frequently Asked Questions Ask more questions How do I use IterativeSolvers solvers with a weighted tolerance vector IterativeSolvers.jl computes the norm after the application of the left precondtioner  Pl  Thus in order to use a vector tolerance  weights  one can mathematically hack the system via the following formulation If you want to use a real preconditioner under the norm  weights  then one can use  ComposePreconditioner  to apply the preconditioner after the application of the weights like as follows"},{"doctype":"documentation","id":"references/ModelingToolkit.DAEProblemExpr","title":"DAEProblemExpr","text":"Generates a Julia expression for constructing an ODEProblem from an ODESystem and allows for automatically symbolically calculating numerical enhancements"},{"doctype":"documentation","id":"references/NeuralPDE.TerminalPDEProblem","title":"TerminalPDEProblem","text":"A semilinear parabolic PDE problem with a terminal condition Consider  du/dt  l(u  f(u  where l is the nonlinear Lipschitz function Arguments g   The terminal condition for the equation f   The function f(u μ   The drift function of X from Ito's Lemma μ   The noise function of X from Ito's Lemma x0  The initial X for the problem tspan  The timespan of the problem"},{"doctype":"documentation","id":"references/DiffEqSensitivity._extract_du","title":"_extract_du","text":""},{"doctype":"documentation","id":"references/ParameterizedFunctions.@ode_def","title":"@ode_def","text":"f LotkaVolterra dx a x b x y dy c y d x y a b c d f dx a x b x y dy c y d x y a b c d Definition of the Domain-Specific Language DSL A helper macro is provided to make it easier to define a  ParameterizedFunction  and it will symbolically compute a bunch of extra functions to make the differential equation solvers run faster For example to define the previous  LotkaVolterra  you can use the following command or you can define it anonymously ode_def  uses ModelingToolkit.jl internally and returns an  ODEFunction  with the extra definitions Jacobian parameter Jacobian etc defined through the MTK symbolic tools"},{"doctype":"document","id":"ModelingToolkit/mtkitize_tutorials/modelingtoolkitize.md","title":"Automatically Accelerating ODEProblem Code","text":"Automatically Accelerating ODEProblem Code For some  DEProblem  types automatic tracing functionality is already included via the  modelingtoolkitize  function Take for example the Robertson ODE defined as an  ODEProblem  for DifferentialEquations.jl If we want to get a symbolic representation we can simply call  modelingtoolkitize  on the  prob  which will return an  ODESystem  Using this we can symbolically build the Jacobian and then rebuild the ODEProblem"},{"doctype":"documentation","id":"references/ModelingToolkit.generate_function","title":"generate_function","text":"sys dvs sys ps sys expression Val kwargs Generate a function to evaluate the system's equations"},{"doctype":"documentation","id":"references/ExponentialUtilities.exp_generic_mutable","title":"exp_generic_mutable","text":""},{"doctype":"documentation","id":"references/PolyChaos.computeSP2","title":"computeSP2","text":"Computes the  n   regular  scalar products aka 2-norms of the orthogonal polynomials namely ϕ_i\\|^2  langle phi_i,\\phi_i\\rangle quad forall i in  0,\\dots,n  Notice that only the values of  β  of the recurrence coefficients  α,β  are required The computation is based on equation 1.3.7 from Gautschi W Orthogonal Polynomials Computation and Approximation Whenever there exists an analytic expressions for  β  this function should be used The function is multiply dispatched to facilitate its use with  AbstractOrthoPoly "},{"doctype":"documentation","id":"references/SciMLBase.AbstractTimeseriesSolutionRows","title":"AbstractTimeseriesSolutionRows","text":""},{"doctype":"document","id":"LinearSolve/basics/common_solver_opts.md","title":"Common Solver Options (Keyword Arguments for Solve)","text":"Common Solver Options Keyword Arguments for Solve While many algorithms have specific arguments within their constructor the keyword arguments for  solve  are common across all of the algorithms in order to give composability These are also the options taken at  init  time The following are the options these algorithms take along with their defaults General Controls alias_A  Whether to alias the matrix  A  or use a copy by default When true algorithms like LU-factorization can be faster by reusing the memory via  lu  but care must be taken as the original input will be modified Default is  false  alias_b  Whether to alias the matrix  b  or use a copy by default When true algorithms can write and change  b  upon usage Care must be taken as the original input will be modified Default is  false  verbose  Whether to print extra information Defaults to  false  Iterative Solver Controls Error controls are not used by all algorithms Specifically direct solves always solve completely Error controls only apply to iterative solvers abstol  The absolute tolerance Defaults to  √(eps(eltype(A reltol  The relative tolerance Defaults to  √(eps(eltype(A maxiters  The number of iterations allowed Defaults to  length(prob.b Pl,Pr  The left and right preconditioners respectively For more information see the Preconditioners page  prec"},{"doctype":"documentation","id":"references/DiffEqSensitivity.split_quadratures","title":"split_quadratures","text":""},{"doctype":"documentation","id":"references/PolyChaos.rm_laguerre","title":"rm_laguerre","text":"Creates  N  recurrence coefficients for monic generalized Laguerre polynomials that are orthogonal on  0,\\infty  relative to  w(t  t^a mathrm{e}^{-t  The call  rm_laguerre(N  is the same as  rm_laguerre(N,0 "},{"doctype":"documentation","id":"references/PoissonRandom.log1pmx","title":"log1pmx","text":""},{"doctype":"documentation","id":"references/ModelingToolkit._instream_split","title":"_instream_split","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.liouville_transform","title":"liouville_transform","text":"OrdinaryDiffEq Test t α β γ δ x t y t D Differential t eqs D x α x β x y D y δ y γ x y sys eqs sys2 sys trJ u0 x y trJ prob sys2 u0 tspan p sol prob Tsit5 DocStringExtensions.TypedMethodSignatures Generates the Liouville transformed set of ODEs which is the original ODE system with a new variable  trJ  appended corresponding to the tr(Jacobian This variable is used for properties like uncertainty propagation from a given initial distribution density For example if  u'=p*u  and  p  follows a probability distribution  f(p  then the probability density of a future value with a given choice of  p  is computed by setting the inital  trJ  f(p  and the final value of  trJ  is the probability of  u(t  Example Where  sol[3  is the evolution of  trJ  over time Sources Probabilistic Robustness Analysis of F-16 Controller Performance An Optimal Transport Approach Abhishek Halder Kooktae Lee and Raktim Bhattacharya https://abhishekhalder.bitbucket.io/F16ACC2013Final.pdf"},{"doctype":"documentation","id":"references/ModelingToolkit.@connector","title":"@connector","text":""},{"doctype":"document","id":"DiffEqSensitivity/manual/direct_adjoint_sensitivities.md","title":"[Direct Adjoint Sensitivities of Differential Equations]( adjoint_sense)","text":"Direct Adjoint Sensitivities of Differential Equations  adjoint_sense First Order Adjoint Sensitivities Second Order Adjoint Sensitivities"},{"doctype":"documentation","id":"references/SciMLBase.undefined_exports","title":"undefined_exports","text":"DocStringExtensions.MethodSignatures List symbols  export ed but not actually defined"},{"doctype":"documentation","id":"references/DiffEqFlux.FastChain","title":"FastChain","text":""},{"doctype":"documentation","id":"references/MethodOfLines.LowerBoundary","title":"LowerBoundary","text":""},{"doctype":"document","id":"DiffEqFlux/layers/BasisLayers.md","title":"Classical Basis Layers","text":"Classical Basis Layers The following basis are helper functions for easily building arrays of the form f_0(x  f_(x where f is the corresponding function of the basis e.g Chebyshev Polynomials Legendre Polynomials etc"},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.ascend_dg","title":"ascend_dg","text":""},{"doctype":"document","id":"Optimization/tutorials/intro.md","title":"Basic usage","text":"rosenbrock x p p x p x x x0 zeros p prob rosenbrock x0 p OptimizationOptimJL sol prob NelderMead OptimizationBBO prob rosenbrock x0 p lb ub sol prob BBO_adaptive_de_rand_1_bin_radiuslimited ForwardDiff f rosenbrock prob f x0 p sol prob BFGS prob f x0 p lb ub sol prob Fminbox GradientDescent Basic usage In this tutorial we introduce the basics of GalcticOptim.jl by showing how to easily mix local optimizers from Optim.jl and global optimizers from BlackBoxOptim.jl on the Rosenbrock equation The simplest copy-pasteable code to get started is the following Notice that Optimization.jl is the core glue package that holds all of the common pieces but to solve the equations we need to use a solver package Here GalcticOptimJL is for  Optim.jl  and OptimizationBBO is for  BlackBoxOptim.jl  The output of the first optimization task with the  NelderMead  algorithm is given below The solution from the original solver can always be obtained via  original  We can also explore other methods in a similar way For instance the above optimization task produces the following output The examples clearly demonstrate that Optimization.jl provides an intuitive way of specifying optimization tasks and offers a relatively easy access to a wide range of optimization algorithms"},{"doctype":"documentation","id":"references/ExponentialUtilities.lanczos!","title":"lanczos!","text":"A variation of  arnoldi  that uses the Lanczos algorithm for Hermitian matrices"},{"doctype":"documentation","id":"references/ModelingToolkit.time_varying_as_func","title":"time_varying_as_func","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.varmap_to_vars","title":"varmap_to_vars","text":"DocStringExtensions.MethodSignatures Takes a list of pairs of  variables=>values  and an ordered list of variables and creates the array of values in the correct order with default values when applicable"},{"doctype":"documentation","id":"references/MethodOfLines.depvar","title":"depvar","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.terms","title":"terms","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.collect_applied_operators","title":"collect_applied_operators","text":"Return  a  Set  with all applied operators in  x  example The difference compared to  collect_operator_variables  is that  collect_operator_variables  returns the variable without the operator applied"},{"doctype":"documentation","id":"references/MethodOfLines.varmaps","title":"varmaps","text":""},{"doctype":"documentation","id":"references/Surrogates.II_tier_ranking_ND","title":"II_tier_ranking_ND","text":""},{"doctype":"documentation","id":"references/Catalyst.findvars!","title":"findvars!","text":""},{"doctype":"documentation","id":"references/MethodOfLines.buildmatrix","title":"buildmatrix","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractConstantLagSDDEProblem","title":"AbstractConstantLagSDDEProblem","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/Catalyst.species","title":"species","text":"Given a  ReactionSystem  return a vector of all species defined in the system and any subsystems that are of type  ReactionSystem  To get the variables in the system and all subsystems including non ReactionSystem  subsystems uses  states(network  Notes If  ModelingToolkit.get_systems(network  is non-empty will allocate"},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.MAX_INLINE_NLSOLVE_SIZE","title":"MAX_INLINE_NLSOLVE_SIZE","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.CLILVector","title":"CLILVector","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.SystemStructures.EquationsView","title":"EquationsView","text":""},{"doctype":"documentation","id":"references/PolyChaos.w_gaussian","title":"w_gaussian","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.calculate_weights","title":"calculate_weights","text":"Return a vector  c  such that  c⋅f.(x  approximates  f^{(n)}(x₀  for a smooth  f  The points  x  need not be evenly spaced The stencil  c  has the highest approximation order possible given values of  f  at  length(x  points More precisely if  x  has length  m  there is a function  g  such that  g(y  f(y  O(y-x₀)^{m-n  and  c⋅f.(x  g'(x₀  The algorithm is due to  Fornberg  with a  modification  to improve stability"},{"doctype":"documentation","id":"references/SciMLBase.AbstractDAEProblem","title":"AbstractDAEProblem","text":"DocStringExtensions.TypeDefinition Base for types which define DAE problems"},{"doctype":"documentation","id":"references/MethodOfLines.nparams","title":"nparams","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.adjoint_sense","title":"adjoint_sense","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.InducedCondensationGraph","title":"InducedCondensationGraph","text":"For some bipartite-graph and a topologicall sorted list of connected components represents the condensation DAG of the digraph formed by the orientation I.e this is a DAG of connected components formed by the destination vertices of some underlying bipartite graph N.B This graph does not store explicit neighbor relations of the sccs Therefor the edge multiplicity is derived from the underlying bipartite graph i.e this graph is not strict"},{"doctype":"documentation","id":"references/MethodOfLines.get_half_offset_weights_and_stencil","title":"get_half_offset_weights_and_stencil","text":"Get the weights and stencil for the inner half offset centered difference for the nonlinear laplacian for a given index and differentiating variable Does not discretize so that the weights can be used in a replacement rule TODO consider refactoring this to harmonize with centered difference Each index corresponds to the weights and index for the derivative at index i+1/2"},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.restore_x","title":"restore_x","text":""},{"doctype":"documentation","id":"references/SciMLBase.has_paramjac","title":"has_paramjac","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractJumpProblem","title":"AbstractJumpProblem","text":"DocStringExtensions.TypeDefinition Base for types which define jump problems"},{"doctype":"documentation","id":"references/QuasiMonteCarlo.RandomSample","title":"RandomSample","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.implicit_correction!","title":"implicit_correction!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.DiscreteSystem","title":"DiscreteSystem","text":"DocStringExtensions.TypeDefinition A system of difference equations Fields DocStringExtensions.TypeFields(false Example Constructs a DiscreteSystem"},{"doctype":"documentation","id":"references/MethodOfLines.cartesian_nonlinear_laplacian","title":"cartesian_nonlinear_laplacian","text":"cartesian_nonlinear_laplacian Differential(x)(expr(x)*Differential(x)(u(x Given an internal multiplying expression  expr  return the correct finite difference equation for the nonlinear laplacian at the location in the grid given by  II  The inner derivative is discretized with the half offset centered scheme giving the derivative at interpolated grid points offset by dx/2 from the regular grid The outer derivative is discretized with the centered scheme giving the nonlinear laplacian at the grid point  II  For first order returns something like this  d/dx a du/dx   a(x+1/2  u[i+1  u[i  a(x-1/2  u[i  u[i-1  dx^2 For 4th order returns something like this where  finitediff(u i  is the finite difference at the interpolated point  i  in the grid And so on"},{"doctype":"documentation","id":"references/DiffEqSensitivity.ForwardSensitivity","title":"ForwardSensitivity","text":"ForwardSensitivity  AbstractForwardSensitivityAlgorithm An implementation of continuous forward sensitivity analysis for propagating derivatives by solving the extended ODE When used within adjoint differentiation i.e via Zygote this will cause forward differentiation of the  solve  call within the reverse-mode automatic differentiation environment Constructor Keyword Arguments autodiff  Use automatic differentiation in the internal sensitivity algorithm computations Default is  true  chunk_size  Chunk size for forward mode differentiation if full Jacobians are built  autojacvec=false  and  autodiff=true  Default is  0  for automatic choice of chunk size autojacvec  Calculate the Jacobian-vector product via automatic differentiation with special seeding diff_type  The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with  autodiff=false  Further details If  autodiff=true  and  autojacvec=true  then the one chunk  J*v  forward-mode directional derivative calculation trick is used to compute the product without constructing the Jacobian via ForwardDiff.jl If  autodiff=false  and  autojacvec=true  then the numerical direction derivative trick  f(x+epsilon*v)-f(x))/epsilon  is used to compute  J*v  without constructing the Jacobian If  autodiff=true  and  autojacvec=false  then the Jacobian is constructed via chunked forward-mode automatic differentiation via ForwardDiff.jl If  autodiff=false  and  autojacvec=false  then the Jacobian is constructed via finite differences via FiniteDiff.jl SciMLProblem Support This  sensealg  only supports  ODEProblem s without callbacks events"},{"doctype":"document","id":"SciMLBase/interfaces/Algorithms.md","title":"SciMLAlgorithms","text":"prob alg kwargs SciMLAlgorithms Definition of the SciMLAlgorithm Interface SciMLAlgorithms  are defined as types which have dispatches to the function signature Algorithm-Specific Arguments Note that because the keyword arguments of  solve  are designed to be common across the whole problem type algorithms should have the algorithm-specific keyword arguments defined as part of the algorithm constructor For example  Rodas5  has a choice of  autodiff::Bool  which is not common across all ODE solvers and thus  autodiff  is a algorithm-specific keyword argument handled via  Rodas5(autodiff=true  Remake Note that  remake  is applicable to  SciMLAlgorithm  types but this is not used in the public API It's used for solvers to swap out components like ForwardDiff chunk sizes Common Algorithm Keyword Arguments Commonly used algorithm keyword arguments are Traits Abstract SciML Algorithms Concrete SciML Algorithms The concrete SciML algorithms are found in the respective solver documentations"},{"doctype":"documentation","id":"references/ModelingToolkit.collect_instream!","title":"collect_instream!","text":""},{"doctype":"documentation","id":"references/SciMLBase.interpolant","title":"interpolant","text":"Hairer Norsett Wanner Solving Ordinary Differential Equations I  Nonstiff Problems Page 190 Hermite Interpolation Hermite Interpolation Hermite Interpolation Hermite Interpolation Linear Interpolation Constant Interpolation"},{"doctype":"documentation","id":"references/GlobalSensitivity","title":"GlobalSensitivity","text":""},{"doctype":"documentation","id":"references/SciMLBase.get_du!","title":"get_du!","text":"Write the current derivative at  t  into  out "},{"doctype":"documentation","id":"references/SciMLBase.LinearProblem","title":"LinearProblem","text":"A x p u0 nothing kwargs f u0 p u0 nothing kwargs Defines a linear system problem Documentation Page http://linearsolve.sciml.ai/dev/basics/LinearProblem Mathematical Specification of a Linear Problem Concrete LinearProblem To define a  LinearProblem  you simply need to give the  AbstractMatrix   A  and an  AbstractVector   b  which defines the linear system Au  b Matrix-Free LinearProblem For matrix-free versions the specification of the problem is given by an operator  A(u,p,t  which computes  A*u  or in-place as  A(du,u,p,t  These are specified via the  AbstractSciMLOperator  interface For more details see the  SciMLBase Documentation  Note that matrix-free versions of LinearProblem definitions are not compatible with all solvers To check a solver for compatibility use the function xxxxx Problem Type Constructors Optionally an initial guess  u₀  can be supplied which is used for iterative methods isinplace  optionally sets whether the function is in-place or not i.e whether the solvers are allowed to mutate By default this is true for  AbstractMatrix  and for  AbstractSciMLOperator s it matches the choice of the operator definition Parameters are optional and if not given then a  NullParameters  singleton will be used which will throw nice errors if you try to index non-existent parameters Any extra keyword arguments are passed on to the solvers Fields A  The representation of the linear operator b  The right-hand side of the linear system p  The parameters for the problem Defaults to  NullParameters  Currently unused u0  The initial condition used by iterative solvers kwargs  The keyword arguments passed on to the solvers"},{"doctype":"documentation","id":"references/SciMLBase.DiffEqArrayOperator","title":"DiffEqArrayOperator","text":"Represents a time-dependent linear operator given by an AbstractMatrix The update function is called by  update_coefficients  and is assumed to have the following signature"},{"doctype":"document","id":"SciMLOperators/premade_operators.md","title":"Premade SciMLOperators","text":"Premade SciMLOperators Direct Operator Definitions Lazy Operator Compositions"},{"doctype":"documentation","id":"references/NeuralPDE.ParamKolmogorovPDEProblem","title":"ParamKolmogorovPDEProblem","text":""},{"doctype":"documentation","id":"references/Integrals.transfor_inf_number","title":"transfor_inf_number","text":""},{"doctype":"documentation","id":"references/SciMLOperators.islinear","title":"islinear","text":""},{"doctype":"documentation","id":"references/Catalyst.get_speciestype","title":"get_speciestype","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.aag_bareiss!","title":"aag_bareiss!","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.store_quad","title":"store_quad","text":""},{"doctype":"documentation","id":"references/SciMLBase.cleansym","title":"cleansym","text":""},{"doctype":"documentation","id":"references/Catalyst.mmr_names","title":"mmr_names","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.AsSubMatrix","title":"AsSubMatrix","text":""},{"doctype":"documentation","id":"references/ModelingToolkit._named","title":"_named","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.has_Wfact","title":"has_Wfact","text":""},{"doctype":"documentation","id":"references/Catalyst.incidencematgraph","title":"incidencematgraph","text":"sir SIR β S I I ν I R β ν complexes sir sir Construct a directed simple graph where nodes correspond to reaction complexes and directed edges to reactions converting between two complexes Notes Requires the  incidencemat  to already be cached in  rn  by a previous call to  reactioncomplexes  For example"},{"doctype":"documentation","id":"references/QuasiMonteCarlo.KroneckerSample","title":"KroneckerSample","text":"KroneckerSample(alpha s0  for a Kronecker sequence where alpha is an length-d vector of irrational numbers often sqrt(d and s0 is a length-d seed vector often 0"},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.but_ordered_incidence","title":"but_ordered_incidence","text":""},{"doctype":"document","id":"NonlinearSolve/tutorials/nonlinear.md","title":"Solving Nonlinear Systems","text":"StaticArrays f u p u u p u0 p probN f u0 p solver probN tol f u p u u u0 probB f u0 sol probB Solving Nonlinear Systems A nonlinear system f(u  0 is specified by defining a function  f(u,p  where  p  are the parameters of the system For example the following solves the vector equation f(u  u^2  p for a vector of equations where  u0  is the initial condition for the rootfind Native NonlinearSolve.jl solvers use the given type of  u0  to determine the type used within the solver and the return Note that the parameters  p  can be any type but most are an AbstractArray for automatic differentiation Using Bracketing Methods For scalar rootfinding problems bracketing methods exist In this case one passes a bracket instead of an initial condition for example"},{"doctype":"documentation","id":"references/ExponentialUtilities.KrylovSubspace","title":"KrylovSubspace","text":"Constructs an uninitialized Krylov subspace which can be filled by  arnoldi  The dimension of the subspace  Ks.m  can be dynamically altered but should be smaller than  maxiter  the maximum allowed arnoldi iterations Access methods for the extended orthonormal basis  V  and the extended Gram-Schmidt coefficients  H  Both methods return a view into the storage arrays and has the correct dimensions as indicated by  Ks.m  Resize  Ks  to a different  maxiter  destroying its contents This is an expensive operation and should be used scarcely"},{"doctype":"document","id":"DiffEqSensitivity/optimal_control/feedback_control.md","title":"Universal Differential Equations for Neural Feedback Control","text":"Lux OptimizationPolyalgorithms OptimizatonOptimJL DifferentialEquations Plots Random rng Random default_rng u0 tspan tsteps model_univ Lux Lux Dense tanh Lux Dense tanh Lux Dense p_model st Lux setup rng model_univ p_model Lux ComponentArray p_model n_weights length p_model p_system Float32 p_all p_model p_system θ Float32 u0 p_all dudt_univ! du u p t model_weights p n_weights α p end β p end model_control system_output u dmodel_control model_univ u model_weights st dsystem_output α system_output β model_control du dmodel_control du dsystem_output prob_univ dudt_univ! u0 tspan p_all sol_univ prob_univ Tsit5 abstol reltol predict_univ θ Array prob_univ Tsit5 u0 θ p θ end sensealg autojacvec saveat tsteps loss_univ θ sum abs2 predict_univ θ l loss_univ θ list_plots iter callback θ l list_plots iter iter list_plots iter println l plt plot predict_univ θ ylim push! list_plots plt display plt adtype optf x p loss_univ x adtype optprob optf θ result_univ optprob PolyOpt cb callback Universal Differential Equations for Neural Feedback Control You can also mix a known differential equation and a neural differential equation so that the parameters and the neural network are estimated simultaneously We will assume that we know the dynamics of the second equation linear dynamics and our goal is to find a neural network that is dependent on the current state of the dynamical system that will control the second equation to stay close to 1"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.INPLACE_WHITE_NOISE_BRIDGE","title":"INPLACE_WHITE_NOISE_BRIDGE","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.RegularConnector","title":"RegularConnector","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_loss","title":"get_loss","text":""},{"doctype":"documentation","id":"references/Surrogates.I_tier_ranking_ND","title":"I_tier_ranking_ND","text":""},{"doctype":"document","id":"NonlinearSolve/basics/FAQ.md","title":"Frequently Asked Questions","text":"Frequently Asked Questions Ask more questions"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.wiener_randn","title":"wiener_randn","text":""},{"doctype":"document","id":"DiffEqSensitivity/hybrid_jump_fitting/bouncing_ball.md","title":"Bouncing Ball Hybrid ODE Optimization","text":"OptimizationPolyalgorithms DifferentialEquations f du u p t du u du p condition u t integrator u affect! integrator integrator u integrator p integrator u cb condition affect! u0 tspan p prob f u0 tspan p sol prob Tsit5 callback cb loss θ sol prob Tsit5 p θ callback cb target abs2 sol end target loss adtype optf x p loss x adtype optprob optf res optprob PolyOpt res u Bouncing Ball Hybrid ODE Optimization The bouncing ball is a classic hybrid ODE which can be represented in the  DifferentialEquations.jl event handling system  This can be applied to ODEs SDEs DAEs DDEs and more Let's now add the DiffEqFlux machinery to this problem in order to optimize the friction that's required to match data Assume we have data for the ball's height after 15 seconds Let's first start by implementing the ODE Here we have a friction coefficient of  0.8  We want to refine this coefficient to find the value so that the predicted height of the ball at the endpoint is 20 We do this by minimizing a loss function against the value 20 This runs in about  0.091215 seconds 533.45 k allocations 80.717 MiB  and finds an optimal drag coefficient Note on Sensitivity Methods The continuous adjoint sensitivities  BacksolveAdjoint   InterpolatingAdjoint  and  QuadratureAdjoint  are compatible with events for ODEs  BacksolveAdjoint  and  InterpolatingAdjoint  can also handle events for SDEs Use  BacksolveAdjoint  if the event terminates the time evolution and several states are saved Currently the continuous adjoint sensitivities do not support multiple events per time point All methods based on discrete sensitivity analysis via automatic differentiation like  ReverseDiffAdjoint   TrackerAdjoint  or  ForwardDiffSensitivity  are the methods to use and  ReverseDiffAdjoint  is demonstrated above are compatible with events This applies to SDEs DAEs and DDEs as well"},{"doctype":"documentation","id":"references/ModelingToolkit.VariableDescriptionType","title":"VariableDescriptionType","text":""},{"doctype":"documentation","id":"references/PoissonRandom.pois_rand","title":"pois_rand","text":"λ rng AbstractRNG λ λ RandomNumbers rng Xorshifts Xoroshiro128Plus rng λ Generates Poisson(λ distributed random numbers using a fast polyalgorithm Examples"},{"doctype":"documentation","id":"references/GlobalSensitivity.run_model","title":"run_model","text":""},{"doctype":"documentation","id":"references/SciMLBase.TYPE_COLOR","title":"TYPE_COLOR","text":""},{"doctype":"documentation","id":"references/DiffEqFlux._norm_batched","title":"_norm_batched","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.CNFLayer","title":"CNFLayer","text":""},{"doctype":"documentation","id":"references/Surrogates.obj2_1D","title":"obj2_1D","text":""},{"doctype":"documentation","id":"references/NeuralPDE.NNStopping","title":"NNStopping","text":"opt sdealg ensemblealg Algorithm for solving Optimal Stopping Problems Arguments chain  A Chain neural network with an N-dimensional output according to N stopping times and the last layer softmax function opt  The optimizer to train the neural network Defaults to  ADAM(0.1  sdealg  The algorithm used to solve the discretized SDE according to the process that X follows Defaults to  EM  ensemblealg  The algorithm used to solve the Ensemble Problem that performs Ensemble simulations for the SDE Defaults to  EnsembleThreads  See the  Ensemble Algorithms  documentation for more details 1]Becker Sebastian et al Solving high-dimensional optimal stopping problems using deep learning arXiv preprint arXiv:1908.01602 2019"},{"doctype":"documentation","id":"references/DiffEqSensitivity.reset!","title":"reset!","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.GeometricBrownianMotionProcess!","title":"GeometricBrownianMotionProcess!","text":"μ σ t0 W0 Z0 nothing kwargs μ σ t0 W0 Z0 nothing kwargs A  GeometricBrownianMotion  process is a Wiener process with constant drift  μ  and constant diffusion  σ  I.e this is the solution of the stochastic differential equation dX_t  mu X_t dt  sigma X_t dW_t The  GeometricBrownianMotionProcess  is distribution exact meaning not a numerical solution of the stochastic differential equation and instead follows the exact distribution properties It can be back interpolated exactly as well The constructor is"},{"doctype":"documentation","id":"references/Surrogates._forward_pass_nd","title":"_forward_pass_nd","text":""},{"doctype":"document","id":"PolyChaos/math.md","title":"[Mathematical Background]( MathematicalBackground)","text":"Mathematical Background  MathematicalBackground This section is heavily based on the book Orthogonal Polynomials Computation and Approximation by Walter Gautschi Oxford University Press Orthogonal Polynomials Basic Theory We always work with absolutely continuous measures for which we write  mathrm{d lambda t  w(t mathrm{d}t  where the so-called  weight function   w is a nonnegative integrable function on the real line  mathbb{R  i.e w mathcal subseteq mathbb rightarrow mathbb_{\\geq 0 has finite limits in case  mathcal{W  mathbb{R  i.e lim_{t to pm infty w(t  infty has finite moments of all orders mu_r(\\mathrm{d}\\lambda  int_{\\mathcal{W t^r mathrm{d lambda t quad r  0 1 2 dots quad text{with mu_0  0 For any pair of integrable functions  u v  their scalar product relative to  mathrm{d lambda  is defined as langle u v rangle_{\\mathrm{d lambda  int_{\\mathcal{W u(t v(t mathrm{d lambda(t Let  mathcal{P  be the set of real polynomials and  mathcal{P}_d subset mathcal{P  be the set of real polynomials of degree at most  d  on  mathcal{W  respectively Monic real polynomials are real polynomials with leading coefficient equal to one i.e  pi_k(t  t^k  dots  for  k  0 1 dots The polynomials  u,v in mathcal{P  with  u neq v  are said to be orthogonal if langle u v rangle_{\\mathrm{d lambda  int_{\\mathcal{W u(t v(t mathrm{d lambda(t  0 The norm of  u  is given by  u  mathrm{d}\\lambda  sqrt{\\langle u u rangle If the polynomials  u in mathcal{P  has unit length   u  mathrm{d}\\lambda  1  it is called  orthonormal  Monic orthogonal polynomials  are polynomials that are monic and orthogonal hence satisfy pi_k(t  pi_k(t mathrm{d lambda  t^k  dots  for  k  0 1 dots  and langle pi_k pi_l rangle_{\\mathrm{d}\\lambda  0  for  k neq l  and  k l  0 1 dots  and  langle pi_k pi_k rangle_{\\mathrm{d}\\lambda   pi_k 2 mathrm{d}\\lambda  0  for  k  0 1 dots  Note The support  mathcal{W  of  mathrm{d lambda  can be an interval finite half-finite infinite or a finite number of disjoint intervals If the support consists of a finite or denumerably infinite number of distinct points  t_k  at which  lambda  has positive jumps  w_k  the measure is called a  discrete measure  For a finite number  N  of points the discrete measure is denoted by  mathrm{d}\\lambda_N  and it is characterized by its nodes and weights   t_k w_k k=1}^N  according to mathrm{d lambda_N t  sum_{k=1}^N w_k delta(t-t_k where  delta  is the  delta function The inner product associated with  mathrm{d lambda_N  is langle u v rangle_{\\mathrm{d}\\lambda_N  int_{\\mathcal{W u(t v(t mathrm{d lambda_N t  sum_{k=1}^{N w_k u(t_k v(t_k There exist only  N  orthogonal polynomials   pi_k(\\mathrm{d lambda_N k=0}^{N-1  that are orthogonal relative to the discrete measure  mathrm{d lambda_N  in the sense langle pi_k(\\mathrm{d lambda_N pi_l(\\mathrm{d lambda_N rangle_{\\mathrm{d}\\lambda_N   pi_k(\\mathrm{d lambda_N mathrm{d lambda_N delta_{kl where  delta_{kl  is the Dirac-delta for  k,l  0 1 dots N-1  Properties Symmetry An absolutely continuous measure  mathrm{d lambda(t  w(t mathrm{d t  is symmetric with respect to the origin if its support is  mathcal{W  a,a  for some  0  a leq infty  and if  w(-t  w(t  for all  t in mathcal{W  Similarly a discrete measure  mathrm{d lambda_N t  sum_{k=1}^N w_k delta(t-t_k  is symmetric if  t_k   t_{N+1-k  and  w_k  w_{N+1-k  for  k=1 2 dots N  Theorem 1.17 states that If  mathrm{d lambda  is symmetric then pi_k(-t mathrm{d lambda  1)^k pi_k(t mathrm{d lambda quad k=0,1 dots hence the parity of  k  decides whether  pi_k  is even/odd Note Symmetry is exploited in  computeSP  where symmetry need not be relative to the origin but some arbitrary point of the support Three-term Recurrence Relation The fact that orthogonal polynomials can be represented in terms of a three-term recurrence formula is at the heart of all numerical methods of the package The importance of the three-term recurrence relation is difficult to overestimate It provides efficient means of evaluating polynomials and derivatives zeros of orthogonal polynomials by means of a eigenvalues of a symmetric tridiagonal matrix acces to quadrature rules normalization constants to create orthonormal polynomials Theorem 1.27 states Let  pi_k(\\cdot  pi_k(\\cdot mathrm{d}\\lambda  for  k  0 1 dots  be the monic orthogonal polynomials with respect to the measure  mathrm{d lambda  Then begin{aligned}\n\\pi_{k+1}(t  t  alpha_k pi_k(t  beta_k pi_{k-1}(t quad k 0 1 dots \n\\pi_o(t  1 \n\\pi_{-1}(t  0,\n\\end{aligned where begin{aligned}\n\\alpha  alpha_k(\\mathrm{d lambda  frac{\\langle t pi_k pi_k rangle_{\\mathrm{d lambda}}{\\langle pi_k pi_k rangle_{\\mathrm{d lambda  k=0,1,2 dots \n\\beta  beta_k(\\mathrm{d lambda  frac{\\langle pi_k pi_k rangle_{\\mathrm{d lambda}}{\\langle pi_{k-1 pi_{k-1 rangle_{\\mathrm{d lambda  k=1,2,\\dots\n\\end{aligned Let  tilde{\\pi}_k(\\cdot  tilde{\\pi}_k(\\cdot mathrm{d lambda t  denote the orthonormal polynomials then begin{aligned}\n\\sqrt{\\beta_{k+1 tilde{\\pi}_k(t  t  alpha_k tilde{\\pi}_{k}(t  sqrt{\\beta_k tilde{\\pi}_{k-1}(t quad k  0 1 dots \n\\tilde{\\pi}_0(t  1 \n\\tilde{\\pi}_{-1}(t  0.\n\\end{aligned Note Within the package the coefficients  α,β  are  the  building block to represent monic orthogonal polynomials Notice that  beta_0  is arbitrary Nevertheless it is convenient to define it as beta_0(\\mathrm{d}\\lambda  langle pi_0 pi_0 rangle_{\\mathrm{d lambda  int_{\\mathcal{W mathrm{d lambda t because it allows to compute the norms of the polynomials based on  beta_k  alone  pi_n mathrm{d lambda  beta_n(\\mathrm{d lambda beta_{n-1}(\\mathrm{d lambda cdots beta_0(\\mathrm{d lambda quad n  0,1 dots Let the support be  mathcal{W  a,b  for  0  a,b  infty  then begin{aligned}\n a  alpha_k(\\mathrm{d lambda  b  k  0,1,2 dots \n 0  beta_k(\\mathrm{d lambda  max(a^2 b^2  k  1 2 dots\n\\end{aligned Quadrature Rules An  n point quadrature rule for the measure  mathrm{d lambda t  is a formula of the form int_{\\mathcal{W f(t mathrm{d lambda(t  sum_{\\nu  1}^{n w_\\nu f(\\tau_\\nu  R_n(f The quadrature rule   tau_\\nu w_\\nu nu=1}^n  composed of mutually distinct nodes  tau_\\nu  and weights  w_\\nu  provides an approximation to the integral The respective error is given by  R_n(f  If for polynomials  p in mathcal{P}_d  the error  R_n(p  vanishes the respective quadrature rule is said to have a degree of exactness  d  Gauss quadrature rule are special quadrature rules that have a degree of exactness  d  2n  1  That means taking a  n 3 point quadrature rule polynomials up to degree 5 can be integrated  exactly  The nodes and weights for the Gauss quadrature rules have some remarkable properties all Gauss nodes are mutually distinct and contained in the interior of the support of  mathrm{d lambda  the  n  Gauss nodes are the zeros of  pi_n  the monic orthogonal polynomial of degree  n  relative to the measure  mathrm{d lambda  all Gauss weights are positive The Gauss nodes and weights can be computed using the  Golub-Welsch algorithm  This means to solve an eigenvalue problem of a symmetric tridiagonal matrix"},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.BipartiteEdgeIter","title":"BipartiteEdgeIter","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.Stegr.liblapack","title":"liblapack","text":""},{"doctype":"documentation","id":"references/MethodOfLines.build_variable_mapping","title":"build_variable_mapping","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.diff_type","title":"diff_type","text":""},{"doctype":"documentation","id":"references/SciMLBase.tighten_container_eltype","title":"tighten_container_eltype","text":""},{"doctype":"documentation","id":"references/PolyChaos.lobatto","title":"lobatto","text":"Gauss-Lobatto quadrature rule Given a weight function encoded by the recurrence coefficients for the associated orthogonal polynomials the function generates the nodes and weights of the  N+2 point Gauss-Lobatto quadrature rule for the weight function having two prescribed nodes  endl   endr  typically the left and right end points of the support interval or points to the left resp to the right thereof Note The function  radau  accepts at most  N  length(α  3  as an input hence providing at most an  length(α  1 point quadrature rule Note Reference OPQ A MATLAB SUITE OF PROGRAMS FOR GENERATING ORTHOGONAL POLYNOMIALS AND RELATED QUADRATURE RULES by Walter Gautschi"},{"doctype":"documentation","id":"references/Surrogates._construct_2nd_order_interp_matrix","title":"_construct_2nd_order_interp_matrix","text":""},{"doctype":"documentation","id":"references/SciMLBase.StandardDDEProblem","title":"StandardDDEProblem","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/DiffEqSensitivity.alg_autodiff","title":"alg_autodiff","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.getmem","title":"getmem","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.AnalyticalJacVecOperator","title":"AnalyticalJacVecOperator","text":""},{"doctype":"documentation","id":"references/PolyChaos.Uniform01Measure","title":"Uniform01Measure","text":""},{"doctype":"documentation","id":"references/Catalyst.node_attrs","title":"node_attrs","text":""},{"doctype":"documentation","id":"references/Catalyst.AttributeValue","title":"AttributeValue","text":""},{"doctype":"documentation","id":"references/MethodOfLines.EdgeAlignedGrid","title":"EdgeAlignedGrid","text":""},{"doctype":"documentation","id":"references/SciMLBase.getindepsym","title":"getindepsym","text":""},{"doctype":"documentation","id":"references/Catalyst.@add_reactions","title":"@add_reactions","text":"Adds the reactions declared to a preexisting  ReactionSystem  All parameters used in the added reactions need to be declared after the reactions See the  Catalyst.jl for Reaction Models  documentation for details on parameters to the macro"},{"doctype":"documentation","id":"references/GlobalSensitivity.DeltaMoment","title":"DeltaMoment","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractODEAlgorithm","title":"AbstractODEAlgorithm","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/PolyChaos.dim","title":"dim","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timeseries_steps_meanvar","title":"timeseries_steps_meanvar","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractRODESolution","title":"AbstractRODESolution","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/NonlinearSolve.UNITLESS_ABS2","title":"UNITLESS_ABS2","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.MatchedCondensationGraph","title":"MatchedCondensationGraph","text":"For some bipartite-graph and an orientation induced on its destination contraction records the condensation DAG of the digraph formed by the orientation I.e this is a DAG of connected components formed by the destination vertices of some underlying bipartite graph N.B This graph does not store explicit neighbor relations of the sccs Therefor the edge multiplicity is derived from the underlying bipartite graph i.e this graph is not strict"},{"doctype":"documentation","id":"references/SciMLBase.resize_non_user_cache!","title":"resize_non_user_cache!","text":"Resizes the non-user facing caches to be compatible with a DE of size  k  This includes resizing Jacobian caches Note In many cases  resize  simply resizes  full_cache  variables and then calls this function This finer control is required for some  AbstractArray  operations"},{"doctype":"documentation","id":"references/ModelingToolkit.get_unit","title":"get_unit","text":"Find the unit of a symbolic item"},{"doctype":"documentation","id":"references/NeuralPDE.generate_random_points","title":"generate_random_points","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.track_callbacks","title":"track_callbacks","text":"Appends a tracking process to determine the time of the callback to be used in the reverse pass The rationale is explain in https://github.com/SciML/DiffEqSensitivity.jl/issues/4"},{"doctype":"documentation","id":"references/DiffEqSensitivity.build_param_jac_config","title":"build_param_jac_config","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractSteadyStateAlgorithm","title":"AbstractSteadyStateAlgorithm","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/SciMLBase.OptimizationProblem","title":"OptimizationProblem","text":"iip f x p lb nothing ub nothing lcons nothing ucons nothing sense nothing kwargs Defines a optimization problem Documentation Page https://galacticoptim.sciml.ai/dev/API/optimization_problem Mathematical Specification of a Optimization Problem To define an Optimization Problem you simply need to give the function  f  which defines the cost function to minimize min_u f(u,p  0 u₀  is an initial guess of the minimum  f  should be specified as  f(u,p  and  u₀  should be an AbstractArray or number whose geometry matches the  desired geometry of  u  Note that we are not limited to numbers or vectors  for  u₀  one is allowed to provide  u₀  as arbitrary matrices   higher-dimension tensors as well Problem Type Constructors isinplace  optionally sets whether the function is in-place or not This is determined automatically but not inferred Note that for OptimizationProblem in-place only refers to the Jacobian and Hessian functions and thus by default if the  OptimizationFunction  is not defined directly then  iip  true  is done by default Parameters are optional and if not given then a  NullParameters  singleton will be used which will throw nice errors if you try to index non-existent parameters Any extra keyword arguments are passed on to the solvers For example if you set a  callback  in the problem then that  callback  will be added in every solve call lb  and  ub  are the upper and lower bounds for box constraints on the optimization They should be an  AbstractArray  matching the geometry of  u  where  lb[I],ub[I  is the box constraint lower and upper bounds for  u[I  lcons  and  ucons  are the upper and lower bounds for equality constraints on the optimization They should be an  AbstractArray  matching the geometry of  u  where  lcons[I],ucons[I  is the constraint lower and upper bounds for  cons[I  If  f  is a standard Julia function it is automatically converted into an  OptimizationFunction  with  NoAD  i.e no automatic generation of the derivative functions Any extra keyword arguments are captured to be sent to the optimizers Fields f  The function in the problem u0  The initial guess for the optima p  The parameters for the problem Defaults to  NullParameters  lb  the lower bounds for the optimization of  u  ub  the upper bounds for the optimization of  u  lcons  ucons  sense  kwargs  The keyword arguments passed on to the solvers"},{"doctype":"documentation","id":"references/DiffEqSensitivity.AdjointSensitivityIntegrand","title":"AdjointSensitivityIntegrand","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.liftint","title":"liftint","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities._rdiv!","title":"_rdiv!","text":""},{"doctype":"documentation","id":"references/SciMLBase.QuadratureProblem","title":"QuadratureProblem","text":""},{"doctype":"documentation","id":"references/SciMLBase.IntegratorTuples","title":"IntegratorTuples","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.update_p_integrand","title":"update_p_integrand","text":""},{"doctype":"documentation","id":"references/SciMLBase.pop_tstop!","title":"pop_tstop!","text":"Pops the last stopping time from the integrator"},{"doctype":"documentation","id":"references/Surrogates._match_container","title":"_match_container","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.ImmutableJacobianWrapper","title":"ImmutableJacobianWrapper","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.namespace_expr","title":"namespace_expr","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.ControlToExpr","title":"ControlToExpr","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.jacobianmat!","title":"jacobianmat!","text":""},{"doctype":"document","id":"NeuralPDE/pinn/system.md","title":"Systems of PDEs","text":"Flux GalacticOptimJL Quadrature Cubature Interval infimum supremum t x u1 u2 u3 Dt Differential t Dtt Differential t Dx Differential x Dxx Differential x eqs Dtt u1 t x Dxx u1 t x u3 t x sin pi x Dtt u2 t x Dxx u2 t x u3 t x cos pi x u1 t x sin pi x u2 t x cos pi x exp t bcs u1 x sin pi x u2 x cos pi x Dt u1 x sin pi x Dt u2 x cos pi x u1 t u2 t exp t u1 t u2 t exp t domains t Interval x Interval input_ length domains n input_ n Flux σ n n Flux σ n _ initθ map c Float64 c _strategy discretization _strategy init_params initθ pde_system eqs bcs domains t x u1 t x u2 t x u3 t x prob pde_system discretization sym_prob pde_system discretization pde_inner_loss_functions prob f f loss_function pde_loss_function pde_loss_functions contents bcs_inner_loss_functions prob f f loss_function bcs_loss_function bc_loss_functions contents callback p l println l println map l_ l_ p pde_inner_loss_functions println map l_ l_ p bcs_inner_loss_functions res prob BFGS callback callback maxiters discretization Flux GalacticOptimJL Quadrature Cubature Interval infimum supremum t x u1 u2 u3 Dt Differential t Dtt Differential t Dx Differential x Dxx Differential x eqs Dtt u1 t x Dxx u1 t x u3 t x sin pi x Dtt u2 t x Dxx u2 t x u3 t x cos pi x u1 t x sin pi x u2 t x cos pi x exp t bcs u1 x sin pi x u2 x cos pi x Dt u1 x sin pi x Dt u2 x cos pi x u1 t u2 t exp t u1 t u2 t exp t domains t Interval x Interval input_ length domains n input_ n Flux σ n n Flux σ n _ initθ map c Float64 c flat_initθ reduce vcat initθ eltypeθ eltype initθ parameterless_type_θ DiffEqBase initθ parameterless_type_θ map phi_ phi_ rand flat_initθ derivative indvars t x depvars u1 u2 u3 length domains quadrature_strategy _pde_loss_functions eq indvars depvars derivative initθ quadrature_strategy eq eqs map loss_f loss_f rand flat_initθ _pde_loss_functions bc_indvars bcs indvars depvars _bc_loss_functions bc indvars depvars derivative initθ quadrature_strategy bc_indvars bc_indvar bc bc_indvar zip bcs bc_indvars map loss_f loss_f rand flat_initθ _bc_loss_functions pde_bounds bcs_bounds domains eqs bcs eltypeθ indvars depvars quadrature_strategy plbs pubs pde_bounds pde_loss_functions _loss lb ub eltypeθ parameterless_type_θ quadrature_strategy _loss lb ub zip _pde_loss_functions plbs pubs map l l flat_initθ pde_loss_functions blbs bubs bcs_bounds bc_loss_functions _loss lb ub eltypeθ parameterless_type_θ quadrature_strategy _loss lb ub zip _bc_loss_functions blbs bubs map l l flat_initθ bc_loss_functions loss_functions pde_loss_functions bc_loss_functions loss_function θ p sum map l l θ loss_functions f_ loss_function prob f_ flat_initθ cb_ p l println l println map l l p loss_functions println map l l p loss_functions end res prob GalacticOptimJL BFGS callback cb_ maxiters Plots ts xs infimum d domain supremum d domain d domains acum accumulate length initθ sep acum i acum i i length acum minimizers_ res minimizer s s sep analytic_sol_func t x exp t sin pi x exp t cos pi x pi exp t u_real analytic_sol_func t x i t ts x xs i u_predict i t x minimizers_ i t ts x xs i diff_u abs u_real i u_predict i i i p1 plot ts xs u_real i linetype contourf title i p2 plot ts xs u_predict i linetype contourf title p3 plot ts xs diff_u i linetype contourf title plot p1 p2 p3 savefig i Flux GalacticOptimJL Quadrature Cubature Interval infimum supremum t x Dt Differential t Dx Differential x u1 u2 u3 Dxu1 Dtu1 Dxu2 Dtu2 eqs_ Dt Dtu1 t x Dx Dxu1 t x u3 t x sin pi x Dt Dtu2 t x Dx Dxu2 t x u3 t x cos pi x exp t u1 t x sin pi x u2 t x cos pi x bcs_ u1 x sin pi x u2 x cos pi x Dt u1 x sin pi x Dt u2 x cos pi x u1 t u2 t exp t u1 t u2 t exp t der_ Dt u1 t x Dtu1 t x Dt u2 t x Dtu2 t x Dx u1 t x Dxu1 t x Dx u2 t x Dxu2 t x bcs__ bcs_ der_ input_ length domains n input_ n Flux σ n n Flux σ n _ initθ map c Float64 c grid_strategy discretization grid_strategy init_params initθ u1 t x u2 t x u3 t x Dxu1 t x Dtu1 t x Dxu2 t x Dtu2 t x pde_system eqs_ bcs__ domains t x prob pde_system discretization sym_prob pde_system discretization pde_inner_loss_functions prob f f loss_function pde_loss_function pde_loss_functions contents inner_loss_functions prob f f loss_function bcs_loss_function bc_loss_functions contents bcs_inner_loss_functions inner_loss_functions aprox_derivative_loss_functions inner_loss_functions end callback p l println l println map l_ l_ p pde_inner_loss_functions println map l_ l_ p bcs_inner_loss_functions println map l_ l_ p aprox_derivative_loss_functions res prob ADAM callback callback maxiters prob prob u0 res minimizer res prob BFGS callback callback maxiters discretization Plots ts xs infimum d domain supremum d domain d domains initθ discretization init_params acum accumulate length initθ sep acum i acum i i length acum minimizers_ res minimizer s s sep u1_real t x exp t sin pi x u2_real t x exp t cos pi x u3_real t x pi exp t Dxu1_real t x pi exp t cos pi x Dtu1_real t x exp t sin pi x Dxu2_real t x pi exp t sin pi x Dtu2_real t x exp t cos pi x analytic_sol_func_all t x u1_real t x u2_real t x u3_real t x Dxu1_real t x Dtu1_real t x Dxu2_real t x Dtu2_real t x u_real analytic_sol_func_all t x i t ts x xs i u_predict i t x minimizers_ i t ts x xs i diff_u abs u_real i u_predict i i titles i p1 plot ts xs u_real i linetype contourf title titles i p2 plot ts xs u_predict i linetype contourf title p3 plot ts xs diff_u i linetype contourf title plot p1 p2 p3 savefig i x y u Dxx x Dyy y eqs Dxx u_ x y Dyy u_ x y u_ u sin pi x sin pi y bcs u x x u x u x u x Flux GalacticOptimJL Plots Quadrature Cubature Interval infimum supremum t x u w Dxx Differential x Dt Differential t a b1 b2 c1 c2 λ1 b1 c2 sqrt b1 c2 b1 c2 b2 c1 λ2 b1 c2 sqrt b1 c2 b1 c2 b2 c1 θ t x exp t cos x a u_analytic t x b1 λ2 b2 λ1 λ2 exp λ1 t θ t x b1 λ1 b2 λ1 λ2 exp λ2 t θ t x w_analytic t x λ1 λ2 exp λ1 t θ t x exp λ2 t θ t x eqs Dt u x t a Dxx u x t b1 u x t c1 w x t Dt w x t a Dxx w x t b2 u x t c2 w x t bcs u x u_analytic x w x w_analytic x u t u_analytic t w t w_analytic t u t u_analytic t w t w_analytic t domains x Interval t Interval input_ length domains n input_ n Flux σ n n Flux σ n _ initθ map c Float64 c _strategy discretization _strategy init_params initθ pde_system eqs bcs domains t x u t x w t x prob pde_system discretization sym_prob pde_system discretization pde_inner_loss_functions prob f f loss_function pde_loss_function pde_loss_functions contents bcs_inner_loss_functions prob f f loss_function bcs_loss_function bc_loss_functions contents callback p l println l println map l_ l_ p pde_inner_loss_functions println map l_ l_ p bcs_inner_loss_functions res prob BFGS callback callback maxiters discretization ts xs infimum d domain supremum d domain d domains acum accumulate length initθ sep acum i acum i i length acum minimizers_ res minimizer s s sep analytic_sol_func t x u_analytic t x w_analytic t x u_real analytic_sol_func t x i t ts x xs i u_predict i t x minimizers_ i t ts x xs i diff_u abs u_real i u_predict i i i p1 plot ts xs u_real i linetype contourf title i p2 plot ts xs u_predict i linetype contourf title p3 plot ts xs diff_u i linetype contourf title plot p1 p2 p3 savefig i Flux GalacticOptimJL DifferentialEquations Roots Plots Quadrature Cubature Interval infimum supremum x y Dx Differential x Dy Differential y Dxu Dyu Dxw Dyw u w f x sin x g x cos x h x x root x f x g x k find_zero root θ x y cosh sqrt f k x sinh sqrt f k x y w_analytic x y θ x y h k f k u_analytic x y k w_analytic x y eqs_ Dx Dxu x y Dy Dyu x y u x y f u x y w x y u x y w x y h u x y w x y Dx Dxw x y Dy Dyw x y w x y g u x y w x y h u x y w x y bcs_ u y u_analytic y u y u_analytic y u x u_analytic x w y w_analytic y w y w_analytic y w x w_analytic x der_ Dy u x y Dyu x y Dy w x y Dyw x y Dx u x y Dxu x y Dx w x y Dxw x y bcs__ bcs_ der_ domains x Interval y Interval input_ length domains n input_ n Flux σ n n Flux σ n _ initθ map c Float64 c _strategy discretization _strategy init_params initθ u x y w x y Dxu x y Dyu x y Dxw x y Dyw x y pde_system eqs_ bcs__ domains x y prob pde_system discretization sym_prob pde_system discretization pde_inner_loss_functions prob f f loss_function pde_loss_function pde_loss_functions contents inner_loss_functions prob f f loss_function bcs_loss_function bc_loss_functions contents bcs_inner_loss_functions inner_loss_functions aprox_derivative_loss_functions inner_loss_functions end callback p l println l println map l_ l_ p pde_inner_loss_functions println map l_ l_ p bcs_inner_loss_functions println map l_ l_ p aprox_derivative_loss_functions res prob BFGS callback callback maxiters discretization xs ys infimum d domain supremum d domain d domains acum accumulate length initθ sep acum i acum i i length acum minimizers_ res minimizer s s sep analytic_sol_func x y u_analytic x y w_analytic x y u_real analytic_sol_func x y i x xs y ys i u_predict i x y minimizers_ i x xs y ys i diff_u abs u_real i u_predict i i i p1 plot xs ys u_real i linetype contourf title i p2 plot xs ys u_predict i linetype contourf title p3 plot xs ys diff_u i linetype contourf title plot p1 p2 p3 savefig i Flux GalacticOptimJL Roots SpecialFunctions Plots Quadrature Cubature Interval infimum supremum t x u w Dx Differential x Dt Differential t Dtt Differential t a b n f x x g x cos π x root x g x f x k find_zero root ξ t x sqrt f k sqrt a sqrt a t x θ t x besselj0 ξ t x bessely0 ξ t x w_analytic t x θ t x u_analytic t x k θ t x eqs Dtt u t x a x n Dx x n Dx u t x u t x f u t x w t x Dtt w t x b x n Dx x n Dx w t x w t x g u t x w t x bcs u x u_analytic x w x w_analytic x u t u_analytic t w t w_analytic t u t u_analytic t w t w_analytic t domains t Interval x Interval input_ length domains n input_ n Flux σ n n Flux σ n _ initθ map c Float64 c _strategy discretization _strategy init_params initθ pde_system eqs bcs domains t x u t x w t x prob pde_system discretization sym_prob pde_system discretization pde_inner_loss_functions prob f f loss_function pde_loss_function pde_loss_functions contents bcs_inner_loss_functions prob f f loss_function bcs_loss_function bc_loss_functions contents callback p l println l println map l_ l_ p pde_inner_loss_functions println map l_ l_ p bcs_inner_loss_functions res prob BFGS callback callback maxiters discretization ts xs infimum d domain supremum d domain d domains acum accumulate length initθ sep acum i acum i i length acum minimizers_ res minimizer s s sep analytic_sol_func t x u_analytic t x w_analytic t x u_real analytic_sol_func t x i t ts x xs i u_predict i t x minimizers_ i t ts x xs i diff_u abs u_real i u_predict i i i p1 plot ts xs u_real i linetype contourf title i p2 plot ts xs u_predict i linetype contourf title p3 plot ts xs diff_u i linetype contourf title plot p1 p2 p3 savefig i Systems of PDEs In this example we will solve the PDE system begin{align*}\n∂_t u_1(t x   ∂_x^2 u_1(t x  u_3(t x  sin(\\pi x  \n∂_t u_2(t x   ∂_x^2 u_2(t x  u_3(t x  cos(\\pi x  \n0   u_1(t x sin(\\pi x  u_2(t x cos(\\pi x  e^{-t  \n\\end{align with the initial conditions begin{align*}\nu_1(0 x   sin(\\pi x  \n∂_t u_1(0 x    sin(\\pi x  \nu_2(0 x   cos(\\pi x  \n∂_t u_2(0 x    cos(\\pi x  \n\\end{align and the boundary conditions begin{align*}\nu_1(t 0   u_1(t 1  0  \nu_2(t 0    u_2(t 1  e^{-t  \n\\end{align with physics-informed neural networks Low-level api And some analysis for both low and high level api sol_uq1 sol_uq2 sol_uq3 Derivative neural network approximation The accuracy and stability of numerical derivative decreases with each successive order The accuracy of the entire solution is determined by the worst accuracy of one of the variables in our case  the highest degree of the derivative Derivative neural network approximation is such an approach that using lower-order numeric derivatives and estimates higher-order derivatives with a neural network so that allows an increase in the marginal precision for all optimization Since  u3  is only in the first and second equations that its accuracy during training is determined by the accuracy of the second numerical derivative  u3(t,x  Dtt(u1(t,x Dxx(u1(t,x  sin(pi*x  We approximate the derivative of the neural network with another neural network  Dt(u1(t,x  Dtu1(t,x  and train it along with other equations and thus we avoid using the second numeric derivative  Dt(Dtu1(t,x  And some analysis aprNN_sol_u1 aprNN_sol_u2 aprNN_sol_u3 Comparison of the second numerical derivative and numerical  neural network derivative DDu1 DDu2 Solving Matrices of PDEs Also in addition to systems we can use the matrix form of PDEs Linear parabolic system of PDEs We can use NeuralPDE to solve the linear parabolic system of PDEs begin{aligned}\n\\frac{\\partial u}{\\partial t  a  frac{\\partial^2 u}{\\partial x^2  b_1 u  c_1 w \n\\frac{\\partial w}{\\partial t  a  frac{\\partial^2 w}{\\partial x^2  b_2 u  c_2 w \n\\end{aligned with initial and boundary conditions begin{aligned}\nu(0 x  frac{b_1  lambda_2}{b_2 lambda_1  lambda_2 cdot cos(\\frac{x}{a   frac{b_1  lambda_1}{b_2 lambda_1  lambda_2 cdot cos(\\frac{x}{a \nw(0 x  0 \nu(t 0  frac{b_1  lambda_2}{b_2 lambda_1  lambda_2 cdot e^{\\lambda_1t   frac{b_1  lambda_1}{b_2 lambda_1  lambda_2 cdot e^{\\lambda_2t  w(t 0  frac{e^{\\lambda_1}-e^{\\lambda_2}}{\\lambda_1  lambda_2 \nu(t 1  frac{b_1  lambda_2}{b_2 lambda_1  lambda_2 cdot e^{\\lambda_1t cdot cos(\\frac{x}{a   frac{b_1  lambda_1}{b_2 lambda_1  lambda_2 cdot e^{\\lambda_2t  cos(\\frac{x}{a \nw(t 1  frac{e^{\\lambda_1 cos(\\frac{x}{a})-e^{\\lambda_2}cos(\\frac{x}{a})}{\\lambda_1  lambda_2}\n\\end{aligned with a physics-informed neural network linear_parabolic_sol_u1   linear_parabolic_sol_u2 Nonlinear elliptic system of PDEs We can also solve nonlinear systems such as the system of nonlinear elliptic PDEs begin{aligned}\n\\frac{\\partial^2u}{\\partial x^2  frac{\\partial^2u}{\\partial y^2  uf(\\frac{u}{w  frac{u}{w}h(\\frac{u}{w \n\\frac{\\partial^2w}{\\partial x^2  frac{\\partial^2w}{\\partial y^2  wg(\\frac{u}{w  h(\\frac{u}{w \n\\end{aligned where f g h are arbitrary functions With initial and boundary conditions begin{aligned}\nu(0,y  y  1 \nw(1 y  cosh(\\sqrt[]{f(k  sinh(\\sqrt[]{f(k)})]\\cdot(y  1 \nw(x,0  cosh(\\sqrt[]{f(k  sinh(\\sqrt[]{f(k \nw(0,y  k(y  1 \nu(1 y  k[cosh(\\sqrt[]{f(k  sinh(\\sqrt[]{f(k)})]\\cdot(y  1 \nu(x,0  k[cosh(\\sqrt[]{f(k  sinh(\\sqrt[]{f(k \n\\end{aligned where k is a root of the algebraic transcendental equation f(k  g(k This is done using a derivative neural network approximation non_linear_elliptic_sol_u1   non_linear_elliptic_sol_u2 Nonlinear hyperbolic system of PDEs Lastly we may also solve hyperbolic systems like the following begin{aligned}\n\\frac{\\partial^2u}{\\partial t^2  frac{a}{x^n frac{\\partial}{\\partial x}(x^n frac{\\partial u}{\\partial x  u f(\\frac{u}{w  \n\\frac{\\partial^2w}{\\partial t^2  frac{b}{x^n frac{\\partial}{\\partial x}(x^n frac{\\partial u}{\\partial x  w g(\\frac{u}{w  \n\\end{aligned where f and g are arbitrary functions With initial and boundary conditions begin{aligned}\nu(0,x  k  j0(ξ(0 x  y0(ξ(0 x \nu(t,0  k  j0(ξ(t 0  y0(ξ(t 0 \nu(t,1  k  j0(ξ(t 1  y0(ξ(t 1 \nw(0,x  j0(ξ(0 x  y0(ξ(0 x \nw(t,0  j0(ξ(t 0  y0(ξ(t 0 \nw(t,1  j0(ξ(t 0  y0(ξ(t 0 \n\\end{aligned where k is a root of the algebraic transcendental equation f(k  g(k j0 and y0 are the Bessel functions and ξ(t x is begin{aligned}\n\\frac{\\sqrt[]{f(k)}}{\\sqrt[]{\\frac{a}{x^n}}}\\sqrt[]{\\frac{a}{x^n}(t+1)^2  x+1)^2}\n\\end{aligned We solve this with Neural nonlinear_hyperbolic_sol_u1   nonlinear_hyperbolic_sol_u2"},{"doctype":"document","id":"MethodOfLines/index.md","title":"[MethodOfLines.jl] ( index)","text":"MethodOfLines.jl   index MethodOfLines.jl is a package for automated finite difference discretization of symbolicaly-defined PDEs in N dimensions It uses symbolic expressions for systems of partial differential equations as defined with  ModelingToolkit.jl  and  Interval  from  DomainSets.jl  to define the space(time over which the simulation runs The package's handling is quite general it is recommended to try out your system of equations and post an issue if you run in to trouble If you want to solve it we want to support it Issues with questions on usage are also welcome as they help us improve the docs See here  brusselator for a full tutorial involving the Brusselator equation Allowable terms in the system include but are not limited to Advection Diffusion Reaction Nonlinear Diffusion Spherical laplacian Any Julia function of the symbolic parameters/dependant variables and other parameters in the environment that's defined on the whole domain Note that more complicated functions may require registration with  register  see the  ModelingToolkit.jl docs  Boundary conditions include but are not limited to Dirichlet Neumann can also include time derivative Robin can also include time derivative Periodic Any function subject to the assumptions below At the moment the centered difference upwind difference nonlinear laplacian and spherical laplacian schemes are implemented If you know of a scheme with better stability or accuracy in any specific case please post an issue with a link to a paper Known Limitations   limitations At the moment the package is able to discretize almost any system with some assumptions listed below That the grid is cartesian That the equation is first order in time Boundary conditions in time are supplied as initial conditions not at the end of the simulation interal If your system requires a final condition please use a change of variables to rectify this This is unlikely to change due to upstream constraints Intergral equations are not supported That dependant variables always have the same argument signature except in BCs That periodic boundary conditions are of the simple form  u(t x_min  u(t x_max  or the same with lhs and rhs reversed Note that this generalises to higher dimensions That boundary conditions do not contain references to derivatives which are not in the direction of the boundary except in time That initial conditions are of the form  u    and don't reference the initial time derivative That simple derivative terms are purely of a dependant variable for example  Dx(u(t,x,y  is allowed but  Dx(u(t,x,y)*v(t,x,y   Dx(u(t,x)+1  or  Dx(f(u(t,x  are not As a workaround please expand such terms with the product rule and use the linearity of the derivative operator or define a new auxiliary dependant variable by adding an equation for it like  eqs  Differential(x)(w(t,x   w(t,x  v(t,x)*u(t,x  along with appropriate BCs/ICs An exception to this is if the differential is a nonlinear or spherical laplacian in which case only the innermost argument should be wrapped That odd order derivatives do not multiply or divide each other A workaround is to wrap all but one derivative per term in an auxiliary variable such as  dxu(x t  Differential(x)(u(x t  The performance hit from auxiliary variables should be negligable due to a structural simplification step If any of these limitations are a problem for you please post an issue and we will prioritize removing them If you discover a limitation that isn't listed here pleae post an issue with example code If you have any usage questions or feature requests please post an issue"},{"doctype":"documentation","id":"references/SciMLBase.AbstractSciMLFunction","title":"AbstractSciMLFunction","text":"DocStringExtensions.TypeDefinition Base for types defining SciML functions"},{"doctype":"documentation","id":"references/PolyChaos.assign2multi","title":"assign2multi","text":""},{"doctype":"documentation","id":"references/Surrogates.DYCORS","title":"DYCORS","text":""},{"doctype":"document","id":"DiffEqFlux/utilities/Collocation.md","title":"Smoothed Collocation","text":"PreallocationTools du PreallocationTools dualcache similar prob u0 preview_est_sol estimated_solution i i size estimated_solution preview_est_deriv estimated_derivative i i size estimated_solution construct_iip_cost_function f du preview_est_sol preview_est_deriv tpoints p _du PreallocationTools get_tmp du p vecdu vec _du cost zero first p i length preview_est_sol est_sol preview_est_sol i f _du est_sol p tpoints i vecdu vec preview_est_deriv i vec _du cost sum abs2 vecdu sqrt cost cost_function construct_iip_cost_function f du preview_est_sol preview_est_deriv tpoints Smoothed Collocation Smoothed collocation also referred to as the two-stage method allows for fitting differential equations to time series data without relying on a numerical differential equation solver by building a smoothed collocating polynomial and using this to estimate the true  u',u  pairs at which point  u'-f(u,p,t  can be directly estimated as a loss to determine the correct parameters  p  This method can be extremely fast and robust to noise though because it does not accumulate through time is not as exact as other methods Kernel Choice Note that the kernel choices of DataInterpolations.jl such as  CubicSpline  are exact i.e go through the data points while the smoothed kernels are regression splines Thus  CubicSpline  is preferred if the data is not too noisy or is relatively sparse If data is sparse and very noisy a  BSpline  can be the best regression spline otherwise one of the other kernels such as as  EpanechnikovKernel  Non-Allocating Forward-Mode L2 Collocation Loss The following is an example of a loss function over the collocation that is non-allocating and compatible with forward-mode automatic differentiation"},{"doctype":"documentation","id":"references/NonlinearSolve.calc_J!","title":"calc_J!","text":""},{"doctype":"documentation","id":"references/SciMLBase.DISCRETE_OUTOFPLACE_DEFAULT","title":"DISCRETE_OUTOFPLACE_DEFAULT","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.SymbolicContinuousCallback","title":"SymbolicContinuousCallback","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.convolve_interior!","title":"convolve_interior!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.aag_bareiss","title":"aag_bareiss","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_connections","title":"get_connections","text":""},{"doctype":"document","id":"MethodOfLines/tutorials/heat.md","title":"[Solving the Heat Equation]( heat)","text":"Solving the Heat Equation  heat In this tutorial we will use the symbolic interface to solve the heat equation Dirichlet boundary conditions Neumann boundary conditions Robin boundary conditions"},{"doctype":"documentation","id":"references/ModelingToolkit.AbstractMultivariateSystem","title":"AbstractMultivariateSystem","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.vars","title":"vars","text":"Return a  Set  containing all variables in  x  that appear in differential equations if  op  Differential difference equations if  op  Differential Example"},{"doctype":"documentation","id":"references/ModelingToolkit.bareiss_virtcolswap","title":"bareiss_virtcolswap","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.update_integrand_and_dgrad","title":"update_integrand_and_dgrad","text":""},{"doctype":"documentation","id":"references/SciMLBase.DynamicalSDEProblem","title":"DynamicalSDEProblem","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/GlobalSensitivity.fuse_designs","title":"fuse_designs","text":""},{"doctype":"documentation","id":"references/SciMLBase.unwrap_cache","title":"unwrap_cache","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timestep_meancor","title":"timestep_meancor","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_connection_type","title":"get_connection_type","text":""},{"doctype":"documentation","id":"references/Catalyst.error_if_constraint_odes","title":"error_if_constraint_odes","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.Equality","title":"Equality","text":""},{"doctype":"documentation","id":"references/SciMLOperators.ScalarOperator","title":"ScalarOperator","text":"Represents a time-dependent scalar/scaling operator The update function is called by  update_coefficients  and is assumed to have the following signature"},{"doctype":"documentation","id":"references/ParameterizedFunctions.@ode_def_all","title":"@ode_def_all","text":"Like  ode_def  but the  opts  options are set so that all possible symbolic functions are generated See the  ode_def  docstring for more details"},{"doctype":"documentation","id":"references/ModelingToolkit.find_first_linear_variable","title":"find_first_linear_variable","text":"DocStringExtensions.MethodSignatures Find the first linear variable such that  𝑠neighbors(adj i)[j  is true given the  constraint "},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.find_independent_subset","title":"find_independent_subset","text":""},{"doctype":"document","id":"NeuralPDE/examples/kolmogorovbackwards.md","title":"Solving Kolmogorov Equations with Neural Networks","text":"μ σ u0 tspan xspan d μ σ tspan xspan d Solving Kolmogorov Equations with Neural Networks A Kolmogorov PDE is of the form  KPDE Consider S to be a solution process to the SDE StochasticP then the solution to the Kolmogorov PDE is given as Solution A Kolmogorov PDE Problem can be defined using a  SDEProblem  Here  u0  is the initial distribution of x Here we define  u(0,x  as the probability density function of  u0  μ  and  σ  are obtained from the SDE for the stochastic process above  d  represents the dimensions of  x   u0  can be defined using  Distributions.jl  Another way of defining a KolmogorovPDE is to use the  KolmogorovPDEProblem  Here  phi  is the initial condition on u(t,x when t  0  μ  and  σ  are obtained from the SDE for the stochastic process above  d  represents the dimensions of  x  To solve this problem use NNKolmogorov(chain opt  sdealg  Uses a neural network to realize a regression function which is the solution for the linear Kolmogorov Equation Here  chain  is a Flux.jl chain with a  d dimensional input and a 1-dimensional output opt  is a Flux.jl optimizer And  sdealg  is a high-order algorithm to calculate the solution for the SDE which is used to define the learning data for the problem Its default value is the classic Euler-Maruyama algorithm"},{"doctype":"documentation","id":"references/ModelingToolkit.reduce_echelon!","title":"reduce_echelon!","text":""},{"doctype":"documentation","id":"references/Integrals.SciMLSolution","title":"SciMLSolution","text":""},{"doctype":"document","id":"DiffEqSensitivity/manual/nonlinear_solve_sensitivities.md","title":"[Sensitivity Algorithms for Nonlinear Problems with Automatic Differentiation (AD)]( sensitivity_nonlinear)","text":"Sensitivity Algorithms for Nonlinear Problems with Automatic Differentiation AD  sensitivity_nonlinear"},{"doctype":"document","id":"LinearSolve/solvers/solvers.md","title":"[Linear System Solvers]( linearsystemsolvers)","text":"MKLPardisoFactorize kwargs PardisoJL fact_phase Pardiso NUM_FACT solve_phase Pardiso SOLVE_ITERATIVE_REFINE kwargs MKLPardisoIterate kwargs PardisoJL solve_phase Pardiso NUM_FACT_SOLVE_REFINE kwargs Base PardisoJL nprocs Union Int Nothing nothing solver_type Union Int Pardiso Solver Nothing nothing matrix_type Union Int Pardiso MatrixType Nothing nothing fact_phase Union Int Pardiso Phase Nothing nothing solve_phase Union Int Pardiso Phase Nothing nothing release_phase Union Int Nothing nothing iparm Union Vector Tuple Int Int Nothing nothing dparm Union Vector Tuple Int Int Nothing nothing args generate_iterator IterativeSolvers gmres_iterable! Pl nothing Pr nothing gmres_restart kwargs args KrylovAlg Krylov gmres! Pl nothing Pr nothing gmres_restart window kwargs Linear System Solvers  linearsystemsolvers solve(prob::LinearProlem,alg;kwargs Solves for  Au=b  in the problem defined by  prob  using the algorithm  alg  If no algorithm is given a default algorithm will be chosen Recommended Methods The default algorithm  nothing  is good for choosing an algorithm that will work but one may need to change this to receive more performance or precision If more precision is necessary  QRFactorization  and  SVDFactorization  are the best choices with SVD being the slowest but most precise For efficiency  RFLUFactorization  is the fastest for dense LU-factorizations For sparse LU-factorizations  KLUFactorization  if there is less structure to the sparsity pattern and  UMFPACKFactorization  if there is more structure Pardiso.jl's methods are also known to be very efficient sparse linear solvers As sparse matrices get larger iterative solvers tend to get more efficient than factorization methods if a lower tolerance of the solution is required IterativeSolvers.jl uses a low-rank Q update in its GMRES so it tends to be faster than Krylov.jl for CPU-based arrays but it's only compatible with CPU-based arrays while Krylov.jl is more general and will support accelerators like CUDA Krylov.jl works with CPUs and GPUs and tends to be more efficient than other Krylov-based methods Finally a user can pass a custom function for handling the linear solve using  LinearSolveFunction  if existing solvers are not optimally suited for their application The interface is detailed  here Full List of Methods RecursiveFactorization.jl RFLUFactorization  a fast pure Julia LU-factorization implementation using RecursiveFactorization.jl This is by far the fastest LU-factorization implementation usually outperforming OpenBLAS and MKL but generally optimized only for Base  Array  with  Float32   Float64   ComplexF32  and  ComplexF64  Base.LinearAlgebra These overloads tend to work for many array types such as  CuArrays  for GPU-accelerated solving using the overloads provided by the respective packages Given that this can be customized per-package details given below describe a subset of important arrays  Matrix   SparseMatrixCSC   CuMatrix  etc LUFactorization(pivot=LinearAlgebra.RowMaximum  Julia's built in  lu  On dense matrices this uses the current BLAS implementation of the user's computer which by default is OpenBLAS but will use MKL if the user does  using MKL  in their system On sparse matrices this will use UMFPACK from SuiteSparse Note that this will not cache the symbolic factorization On CuMatrix it will use a CUDA-accelerated LU from CuSolver On BandedMatrix and BlockBandedMatrix it will use a banded LU QRFactorization(pivot=LinearAlgebra.NoPivot(),blocksize=16  Julia's built in  qr  On dense matrices this uses the current BLAS implementation of the user's computer which by default is OpenBLAS but will use MKL if the user does  using MKL  in their system On sparse matrices this will use SPQR from SuiteSparse On CuMatrix it will use a CUDA-accelerated QR from CuSolver On BandedMatrix and BlockBandedMatrix it will use a banded QR SVDFactorization(full=false,alg=LinearAlgebra.DivideAndConquer  Julia's built in  svd  On dense matrices this uses the current BLAS implementation of the user's computer which by default is OpenBLAS but will use MKL if the user does  using MKL  in their system GenericFactorization(fact_alg  Constructs a linear solver from a generic factorization algorithm  fact_alg  which complies with the Base.LinearAlgebra factorization API Quoting from Base If  A  is upper or lower triangular or diagonal no factorization of  A  is required and the system is solved with either forward or backward substitution For non-triangular square matrices an LU factorization is used For rectangular  A  the result is the minimum-norm least squares solution computed by a pivoted QR factorization of  A  and a rank estimate of  A  based on the R factor When  A  is sparse a similar polyalgorithm is used For indefinite matrices the  LDLt  factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices LinearSolve.jl LinearSolve.jl contains some linear solvers built in SimpleLUFactorization  a simple LU-factorization implementation without BLAS Fast for small matrices SuiteSparse.jl By default the SuiteSparse.jl are implemented for efficiency by caching the symbolic factorization I.e if  set_A  is used it is expected that the new  A  has the same sparsity pattern as the previous  A  If this algorithm is to be used in a context where that assumption does not hold set  reuse_symbolic=false  KLUFactorization(;reuse_symbolic=true  A fast sparse LU-factorization which specializes on sparsity patterns with less structure UMFPACKFactorization(;reuse_symbolic=true  A fast sparse multithreaded LU-factorization which specializes on sparsity patterns that are more structured Pardiso.jl Note Using this solver requires adding the package LinearSolvePardiso.jl The following algorithms are pre-specified MKLPardisoFactorize(;kwargs  A sparse factorization method MKLPardisoIterate(;kwargs  A mixed factorization+iterative method Those algorithms are defined via The full set of keyword arguments for  PardisoJL  are CUDA.jl Note that  CuArrays  are supported by  GenericFactorization  in the normal way The following are non-standard GPU factorization routines Note Using this solver requires adding the package LinearSolveCUDA.jl CudaOffloadFactorization  An offloading technique used to GPU-accelerate CPU-based computations Requires a sufficiently large  A  to overcome the data transfer costs IterativeSolvers.jl IterativeSolversJL_CG(args...;kwargs  A generic CG implementation IterativeSolversJL_GMRES(args...;kwargs  A generic GMRES implementation IterativeSolversJL_BICGSTAB(args...;kwargs  A generic BICGSTAB implementation IterativeSolversJL_MINRES(args...;kwargs  A generic MINRES implementation The general algorithm is Krylov.jl KrylovJL_CG(args...;kwargs  A generic CG implementation KrylovJL_GMRES(args...;kwargs  A generic GMRES implementation KrylovJL_BICGSTAB(args...;kwargs  A generic BICGSTAB implementation KrylovJL_MINRES(args...;kwargs  A generic MINRES implementation The general algorithm is KrylovKit.jl KrylovKitJL_CG(args...;kwargs  A generic CG implementation KrylovKitJL_GMRES(args...;kwargs  A generic GMRES implementation The general algorithm is"},{"doctype":"documentation","id":"references/ModelingToolkit.has_jac","title":"has_jac","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.getboundarytype","title":"getboundarytype","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.observed2graph","title":"observed2graph","text":""},{"doctype":"documentation","id":"references/PolyChaos.w_beta","title":"w_beta","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_or_construct_tearing_state","title":"get_or_construct_tearing_state","text":""},{"doctype":"documentation","id":"references/SciMLOperators.update_coefficients","title":"update_coefficients","text":""},{"doctype":"documentation","id":"references/Catalyst.complexoutgoingmat","title":"complexoutgoingmat","text":"Given a  ReactionSystem  and complex incidence matrix  B  return a matrix of size num of complexes by num of reactions that identifies substrate complexes Notes The complex outgoing matrix  Delta  is defined by Delta_{i j  begin{cases}\n     0    text{if  B_{i j  1 \n     B_{i j text{otherwise.}\n\\end{cases Set sparse=true for a sparse matrix representation"},{"doctype":"documentation","id":"references/SciMLBase.solve_batch","title":"solve_batch","text":""},{"doctype":"documentation","id":"references/LinearSolve","title":"LinearSolve","text":""},{"doctype":"documentation","id":"references/NeuralOperators.inverse","title":"inverse","text":""},{"doctype":"documentation","id":"references/MethodOfLines.findcorners","title":"findcorners","text":"Create a vector containing indices of the corners of the domain"},{"doctype":"documentation","id":"references/SciMLBase.IncrementingODEFunction","title":"IncrementingODEFunction","text":""},{"doctype":"document","id":"NeuralPDE/solvers/optimal_stopping.md","title":"Neural Network Solvers for Optimal Stopping Time Problems","text":"Neural Network Solvers for Optimal Stopping Time Problems TODO"},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.factor_poly","title":"factor_poly","text":""},{"doctype":"documentation","id":"references/LinearSolve.KrylovJL_MINRES","title":"KrylovJL_MINRES","text":""},{"doctype":"documentation","id":"references/LinearSolve.LUFactorization","title":"LUFactorization","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.getbounds","title":"getbounds","text":"Get the bounds associated with symbolc variable  x  Create parameters with bounds like this Returns a dict with pairs  p  lb ub  mapping parameters of  sys  to lower and upper bounds Create parameters with bounds like this Return vectors of lower and upper bounds of parameter vector  p  Create parameters with bounds like this See also  tunable_parameters   hasbounds"},{"doctype":"documentation","id":"references/SciMLBase.__has_jvp","title":"__has_jvp","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.TimeDilation","title":"TimeDilation","text":"alpha t0skip zero alpha t1skip zero alpha TimeDilation  AbstractLSSregularizer A regularization method for  LSS  See  LSS  for additional information and other methods Constructor"},{"doctype":"documentation","id":"references/DiffEqOperators.decompose","title":"decompose","text":"Ax Ay  decompose(A::ComposedBoundaryPaddedArray Decomposes a ComposedBoundaryPaddedArray in to components that extend along each dimension individually Qx Qy  decompose(Q::ComposedMultiDimBC Decomposes a ComposedMultiDimBC in to components that extend along each dimension individually"},{"doctype":"documentation","id":"references/ModelingToolkit.has_controls","title":"has_controls","text":""},{"doctype":"documentation","id":"references/NeuralPDE.find_thing_in_expr","title":"find_thing_in_expr","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.CheckpointSolution","title":"CheckpointSolution","text":""},{"doctype":"documentation","id":"references/GlobalSensitivity._permute_outputs","title":"_permute_outputs","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.getdefault","title":"getdefault","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.accept_solution","title":"accept_solution","text":""},{"doctype":"document","id":"Catalyst/tutorials/reaction_systems.md","title":"Programmatic Construction of Symbolic Reaction Systems","text":"α K n δ γ β μ t m₁ t m₂ t m₃ t P₁ t P₂ t P₃ t rxs P₃ α K n nothing m₁ P₁ α K n nothing m₂ P₂ α K n nothing m₃ δ m₁ nothing γ nothing m₁ δ m₂ nothing γ nothing m₂ δ m₃ nothing γ nothing m₃ β m₁ m₁ P₁ β m₂ m₂ P₂ β m₃ m₃ P₃ μ P₁ nothing μ P₂ nothing μ P₃ nothing repressilator rxs t α β t A t B t rx α β t A A B t P₁ t P₂ t P₃ t rxs P₃ α K n ∅ m₁ P₁ α K n ∅ m₂ P₂ α K n ∅ m₃ δ m₁ ∅ γ ∅ m₁ δ m₂ ∅ γ ∅ m₂ δ m₃ ∅ γ ∅ m₃ β m₁ m₁ P₁ β m₂ m₂ P₂ β m₃ m₃ P₃ μ P₁ ∅ μ P₂ ∅ μ P₃ ∅ repressilator rxs t rx P α K n A B P α K n t A t B t rx P α K n A B Programmatic Construction of Symbolic Reaction Systems While the DSL provides a simple interface for creating  ReactionSystem s it can often be convenient to build or augment a  ReactionSystem  programmatically In this tutorial we show how to build the repressilator model of the  Using Catalyst  tutorial directly using symbolic variables and then summarize the basic API functionality for accessing information stored within  ReactionSystem s Directly Building the Repressilator with  ReactionSystem s We first load Catalyst and then define symbolic variables for each parameter and species in the system the latter corresponding to a  variable  or  state  in ModelingToolkit terminology Note each species is declared as a variable that is a function of time Next we specify the chemical reactions that comprise the system using Catalyst  Reaction s Here we use  nothing  where the DSL used  varnothing  Finally we are ready to construct our  ReactionSystem  as Notice the model is named  repressilator  A name must always be specified when directly constructing a  ReactionSystem  the DSL will auto-generate one if left out Using  named  when constructing a  ReactionSystem  causes the name of the system to be the same as the name of the variable storing the system Alternatively one can use the  name=:repressilator  keyword argument to the  ReactionSystem  constructor We can check that this is the same model as the one we defined via the DSL as follows this requires that we use the same names for rates species and the system For more options in building  ReactionSystem s see the  ReactionSystem  API docs More General  Reaction s In the example above all the specified  Reaction s were first or zero order The three-argument form of  Reaction  implicitly assumes all species have a stoichiometric coefficient of one i.e for substrates  S₁,...,Sₘ  and products  P₁,...,Pₙ  it has the possible forms To allow for other stoichiometric coefficients we also provide a five argument form Finally we note that the rate constant  rate  above does not need to be a constant or fixed function but can be a general symbolic expression See the FAQs  user_functions for info on using general user-specified functions for the rate constant reaction  macro for constructing  Reaction s In some cases one wants to build reactions incrementally as in the repressilator example but it would be nice to still have a short hand as in the  reaction_network  DSL In this case one can construct individual reactions using the  reaction  macro For example the repressilator reactions could also have been constructed like Note there are a few differences when using the  reaction  macro to specify one reaction versus using the full  reaction_network  macro to create a  ReactionSystem  First only one reaction i.e a single forward arrow type can be used i.e reversible arrows like    will not work since they define more than one reaction Second the  reaction  macro must try to infer which symbols are species versus parameters and uses the heuristic that anything appearing in the rate expression is a parameter Coefficients in the reaction part are also inferred as parameters while rightmost symbols i.e substrates and products are inferred as species As such the following are equivalent is equivalent to Here  P,α,K,n  are parameters and  A,B  are species This behavior is the reason that in the repressilator example above we pre-declared  P₁(t),P₂(t),P₃(t  as variables and then used them via interpolating their values into the rate law expressions using    in the macro This ensured they were properly treated as species and not parameters See the  reaction  macro docstring for more information Basic Querying of  ReactionSystems The  Catalyst.jl API  provides a large variety of functionality for querying properties of a reaction network Here we go over a few of the most useful basic functions Given the  repressillator  defined above we have that We can test if a  Reaction  is mass action i.e the rate does not depend on  t  or other species as while Similarly we can determine which species a reaction's rate law will depend on like Basic stoichiometry matrices can be obtained from a  ReactionSystem  as Here the  i,j  entry gives the corresponding stoichiometric coefficient of species  i  for reaction  j  Finally we can directly access fields of individual reactions like See the  Catalyst.jl API  for much more detail on the various querying and analysis functions provided by Catalyst"},{"doctype":"documentation","id":"references/DiffEqOperators.GhostDerivativeOperator","title":"GhostDerivativeOperator","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.Neumann0BC","title":"Neumann0BC","text":"l  and  r  are the BC coefficients i.e  αl βl γl  and  αl βl γl  tuples and vectors work and correspond to BCs of the form αl  u  βl  u  γl   αr  u  βr  u  γr imposed on the lower  l  and higher  r  index boundaries respectively RobinBC  implements a Robin boundary condition operator  Q  that acts on a vector to give an extended vector as a result see https://github.com/JuliaDiffEq/DiffEqOperators.jl/files/3267835/ghost_node.pdf Write vector b̄₁ as a vertical concatenation with b0 and the rest of the elements of b̄₁ denoted b̄ ₁ the same with ū into u0 and ū  b̄ ₁  b̄ 2  fill(β/Δx length(stencil)-1 Pull out the product of u0 and b0 from the dot product The stencil used to approximate u is denoted s b0  α+(β/Δx)*s[1 Rearrange terms to find a general formula for u0 b̄ ₁̇⋅ū b0  γ/b0 which is dependent on ū  the robin coefficients and Δx The non-identity part of Qa is qa b`₁/b0  β s[2:end]/(α+β s[1]/Δx The constant part is Qb  γ/(α+β*s[1]/Δx do the same at the other boundary amounts to a flip of s[2:end with the other set of boundary coefficients"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.generate_boxes1","title":"generate_boxes1","text":""},{"doctype":"documentation","id":"references/SciMLBase.IntegratorIntervals","title":"IntegratorIntervals","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/SciMLBase.SecondOrderDDEProblem","title":"SecondOrderDDEProblem","text":"DocStringExtensions.TypeDefinition Define a second order DDE problem with the specified function Arguments f  The function for the second derivative du0  The initial derivative u0  The initial condition h  The initial history function tspan  The timespan for the problem p  Parameter values for  f  callback  A callback to be applied to every solver which uses the problem Defaults to nothing isinplace  optionally sets whether the function is inplace or not This is determined automatically but not inferred"},{"doctype":"document","id":"NeuralPDE/pinn/heterogeneous.md","title":"Differential Equations with Heterogeneous Inputs","text":"Differential Equations with Heterogeneous Inputs A differential equation is said to have heterogeneous inputs when its dependent variables depend on different independent variables u(x  w(x v  frac{\\partial w(x v)}{\\partial w Here we write an arbitrary heterogeneous system"},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.find_dense","title":"find_dense","text":""},{"doctype":"documentation","id":"references/PolyChaos.quadgp","title":"quadgp","text":"general purpose quadrature based on Gautschi Orthogonal Polynomials Computation and Approximation Section 2.2.2 pp 93-95 Compute the  N point quadrature rule for  weight  with support  lb   ub  The quadrature rule can be specified by the keyword  quadrature  The keyword  bnd  sets the numerical value for infinity"},{"doctype":"document","id":"Optimization/API/optimization_problem.md","title":"Defining OptimizationProblems","text":"Defining OptimizationProblems"},{"doctype":"documentation","id":"references/DiffEqSensitivity.compute_f!","title":"compute_f!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.@namespace","title":"@namespace","text":"DocStringExtensions.MethodSignatures Rewrite  namespace a.b.c  to  getvar(getvar(a b namespace  true c namespace  true "},{"doctype":"documentation","id":"references/Surrogates._phi_int","title":"_phi_int","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.NoiseWrapper","title":"NoiseWrapper","text":"source T N Vector T2 inplace reset reverse indx nothing T N T2 inplace StochasticDiffEq f1 u p t u g1 u p t u dt prob1 f1 g1 sol1 prob1 EM dt dt save_noise W2 sol1 W prob1 f1 g1 noise W2 sol2 prob1 EM dt dt sol1 u sol2 u W3 sol1 W prob2 f1 g1 noise W3 dt sol3 prob2 EM dt dt Plots plot sol1 plot! sol2 plot! sol3 plot sol1 W plot! sol2 W plot! sol3 W prob f1 g1 ones sol4 prob SRI abstol save_noise W2 sol4 W prob2 f1 g1 ones noise W2 sol5 prob2 SRIW1 abstol Plots plot sol4 plot! sol5 This produces a new noise process from an old one which will use its interpolation to generate the noise This allows you to re-use a previous noise process not just with the same timesteps but also with new adaptive timesteps as well Thus this is very good for doing Multi-level Monte Carlo schemes and strong convergence testing Constructor NoiseWrapper Example In this example we will solve an SDE three times First to generate a noise process Second with the same timesteps to show the values are the same Third with half-sized timsteps First we will generate a noise process by solving an SDE Now we wrap the noise into a NoiseWrapper and solve the same problem We can test to see that the values are essentially equal Now we can use the same process to solve the same trajectory with a smaller  dt  We can plot the results to see what this looks like noise_process In this plot  sol2  covers up  sol1  because they hit essentially the same values You can see that  sol3  its similar to the others because it's using the same underlying noise process just sampled much finer To double check we see that coupled_wiener the coupled Wiener processes coincide at every other time point and the intermediate timepoints were calculated according to a Brownian bridge Adaptive NoiseWrapper Example Here we will show that the same noise can be used with the adaptive methods using the  NoiseWrapper   SRI  and  SRIW1  use slightly different error estimators and thus give slightly different stepping behavior We can see how they solve the same 2D SDE differently by using the noise wrapper SRI_SRIW1_diff"},{"doctype":"document","id":"DiffEqSensitivity/ode_fitting/exogenous_input.md","title":"Handling Exogenous Input Signals","text":"I t t f du u p t du I t du u f du u p t I du I t du u _f du u p t f du u p t x x DifferentialEquations Lux OptimizationPolyalgorithms OptimizationFlux Plots Random rng Random default_rng tspan Float32 tsteps range tspan tspan length t_vec collect tsteps ex vec ones Float32 length tsteps f x atan x atan atan hammerstein_system u y zeros size u k length u y k f u k y k y y Float32 hammerstein_system ex plot collect tsteps y ticks native nn_model Lux Lux Dense tanh Lux Dense p_model st Lux setup nn_model u0 Float32 dudt u p t out st nn_model vcat u ex Int round p st out prob dudt u0 tspan nothing predict_neuralode p _prob prob p p Array _prob Tsit5 saveat tsteps abstol reltol loss p sol predict_neuralode p N length sol sum abs2 y N sol N adtype optf x p loss x adtype optprob optf Lux setup p_model res0 optprob PolyOpt maxiters sol predict_neuralode res0 u plot tsteps sol N length sol scatter! tsteps y N savefig Handling Exogenous Input Signals The key to using exogeneous input signals is the same as in the rest of the SciML universe just use the function in the definition of the differential equation For example if it's a standard differential equation you can use the form so that  I(t  is an exogenous input signal into  f  Another form that could be useful is a closure For example which encloses an extra argument into  f  so that  f  is now the interface-compliant differential equation definition Note that you can also learn what the exogenous equation is from data For an example on how to do this you can use the Optimal Control Example  optcontrol which shows how to parameterize a  u(t  by a universal function and learn that from data Example of a Neural ODE with Exogenous Input In the following example a discrete exogenous input signal  ex  is defined and used as an input into the neural network of a neural ODE system"},{"doctype":"documentation","id":"references/ExponentialUtilities.checkdims","title":"checkdims","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.has_noiseeqs","title":"has_noiseeqs","text":""},{"doctype":"documentation","id":"references/RecursiveArrayTools.VectorOfArrayStyle","title":"VectorOfArrayStyle","text":""},{"doctype":"documentation","id":"references/LinearSolve.RFLUFactorization","title":"RFLUFactorization","text":""},{"doctype":"documentation","id":"references/LinearSolve.default_tol","title":"default_tol","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.calculate_massmatrix","title":"calculate_massmatrix","text":""},{"doctype":"documentation","id":"references/Surrogates._construct_y_matrix","title":"_construct_y_matrix","text":""},{"doctype":"document","id":"Optimization/API/solve.md","title":"Common Solver Options (Solve Keyword Arguments)","text":"Common Solver Options Solve Keyword Arguments"},{"doctype":"documentation","id":"references/ModelingToolkit.StreamConnector","title":"StreamConnector","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timestep_quantile","title":"timestep_quantile","text":""},{"doctype":"document","id":"PolyChaos/chi_squared_k_greater1.md","title":"Chi-squared Distribution (k>1)","text":"Chi-squared Distribution  k>1  Theory Given  k  standard random variables  X_i sim mathcal{N}(0,1  for  i=1,\\dots,k  we would like to find the random variable  Y  sum_{i=1}^k X_i^2  The analytic solution is known  Y  follows a chi-squared distribution with  k  degrees of freedom Using polynomial chaos expansion PCE the problem can be solved using Galerkin projection Let  phi_k k=0}^{n  be the monic orthogonal basis relative to the probability density of  X  X_1 dots X_k  namely f_X(x   prod_{i=1}^k frac{1}{\\sqrt{2 pi  exp left  frac{x_i^2}{2 right Then the PCE of  X_i  is given by X_i  sum_{k=0}^n x_{i,k phi_k with x_{i,0  0 quad x_{i,i+1  1 quad x_{i,l  0 quad forall l in 1,\\dots,n setminus i+1 To find the PCE coefficients  y_k  for  Y  sum_{k=}^n y_k phi_k  we apply Galerkin projection which leads to y_m langle phi_m phi_m rangle  sum_{i=1}^k sum_{j_1=0}^n sum_{j_2=0}^n x_{i,j_1 x_{i,j_2 langle phi_{j_1 phi_{j_2 phi_m rangle quad forall m  0 dots n Hence knowing the scalars  langle phi_m phi_m rangle  and  langle phi_{j_1 phi_{j_2 phi_m rangle  the PCE coefficients  y_k  can be obtained immediately From the PCE coefficients we can get the moments and compare them to the closed-form expressions Notice  A maximum degree of 2 suffices to get the  exact  solution with PCE In other words increasing the maximum degree to values greater than 2 introduces nothing but computational overhead and numerical errors possibly Practice First we create a orthogonal basis relative to  f_X(x  of degree at most  d=2   degree  below Notice that we consider a total of  Nrec  recursion coefficients and that we also add a quadrature rule by setting  addQuadrature  true  Now let's define a multivariate basis Next we define the PCE for all  X_i  with  i  1 dots k  With the orthogonal basis and the quadrature at hand we can compute the tensors  t2  and  t3  that store the entries  langle phi_m phi_m rangle  and  langle phi_{j_1 phi_{j_2 phi_m rangle  respectively With the tensors at hand we can compute the Galerkin projection Notice there are more efficient ways to do this but let's keep it simple Let's compare the moments via PCE to the closed-form expressions Let's plot the probability density function to compare results We first draw samples from the measure with the help of  sampleMeasure  and then evaluate the basis at these samples and multiply times the PCE coefficients The latter stop is done using  evaluatePCE  Both steps are combined in the function  samplePCE  Finally we compare the result agains the analytical PDF  rho(t  frac{t^{t/2-1}\\mathrm{e}^{-t/2}}{2^{k/2  Gamma(k/2  of the chi-squared distribution with one degree of freedom"},{"doctype":"documentation","id":"references/PolyChaos.Uniform_11OrthoPoly","title":"Uniform_11OrthoPoly","text":""},{"doctype":"documentation","id":"references/PolyChaos.AbstractMeasure","title":"AbstractMeasure","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.AbstractSplineLayer","title":"AbstractSplineLayer","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.is_poly","title":"is_poly","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.TrackedAffect","title":"TrackedAffect","text":""},{"doctype":"documentation","id":"references/Surrogates.SecondOrderPolynomialSurrogate","title":"SecondOrderPolynomialSurrogate","text":"mutable struct InverseDistanceSurrogate  AbstractSurrogate The square polynomial model can be expressed by 𝐲  𝐗β  ϵ with β  𝐗ᵗ𝐗⁻¹𝐗ᵗ𝐲"},{"doctype":"documentation","id":"references/NeuralPDE._dot_","title":"_dot_","text":""},{"doctype":"document","id":"GlobalSensitivity/methods/regression.md","title":"Regression Method","text":"rank Bool linear_batch X A B A X B X linear X A B A X B X p_range reg linear_batch p_range batch reg linear p_range batch reg linear p_range batch Regression Method RegressionGSA  has the following keyword arguments rank  flag which determines whether to calculate the rank coefficients Defaults to  false  It returns a  RegressionGSAResult  which contains the  pearson   standard_regression  and  partial_correlation  coefficients described below If  rank  is true then it also contains the ranked versions of these coefficients Note that the ranked version of the  pearson  coefficient is also known as the Spearman coefficient which is returned here as the  pearson_rank  coefficient For multi-variable models the coefficient for the  X_i  input variable relating to the  Y_j  output variable is given as the  i j  entry in the corresponding returned matrix Regression Details It is possible to fit a linear model explaining the behavior of Y given the values of X provided that the sample size n is sufficiently large at least n  d The measures provided for this analysis by us in GlobalSensitivity.jl are a Pearson Correlation Coefficient r  frac{\\sum_{i=1}^{n x_i  overline{x})(y_i  overline{y})}{\\sqrt{\\sum_{i=1}^{n x_i  overline{x})^2(y_i  overline{y})^2 b Standard Regression Coefficient SRC SRC_j  beta_{j sqrt{\\frac{Var(X_j)}{Var(Y where  beta_j  is the linear regression coefficient associated to  X_j  This is also known as a sigma-normalized derivative c Partial Correlation Coefficient PCC PCC_j  rho(X_j  hat{X_{-j}},Y_j  hat{Y_{-j where  hat{X_{-j  is the prediction of the linear model expressing  X_{j  with respect to the other inputs and  hat{Y_{-j  is the prediction of the linear model where  X_j  is absent PCC measures the sensitivity of  Y  to  X_j  when the effects of the other inputs have been canceled If  rank  is set to  true  then the rank coefficients are also calculated API function gsa(f method::RegressionGSA p_range::AbstractVector samples::Int  1000 batch::Bool  false kwargs Example"},{"doctype":"documentation","id":"references/RecursiveArrayTools.vecvecapply","title":"vecvecapply","text":"f Base Callable v Calls  f  on each element of a vecvec  v "},{"doctype":"documentation","id":"references/DiffEqFlux","title":"DiffEqFlux","text":""},{"doctype":"document","id":"PoissonRandom/index.md","title":"PoissonRandom.jl: Fast Poisson Random Numbers","text":"Pkg Pkg add λ RandomNumbers rng Xorshifts Xoroshiro128Plus rng λ RandomNumbers Distributions BenchmarkTools StaticArrays Plots labels rng Xorshifts Xoroshiro128Plus n_count rng λ n tmp i n tmp rng λ n_pois rng λ n tmp i n tmp rng λ n_ad rng λ n tmp i n tmp rng λ n_dist λ n tmp i n tmp rand Poisson λ time_λ rng λ n t1 n_count rng λ n t2 n_ad rng λ n t3 n_pois rng λ n t4 n_dist λ n t1 t2 t3 t4 time_λ rng times time_λ rng n n plot times labels labels lw PoissonRandom.jl Fast Poisson Random Numbers PoissonRandom.jl is a component of the SciML ecosystem which allows for fast generation of Poisson random numbers Installation To install ParameterizedFunctions.jl use the Julia package manager Usage Implementation It mixes two methods A simple count method and a method from a normal approximation See  this blog post for details  Benchmark benchmark result So this package ends up about 30 or so faster than Distributions.jl the method at the far edge is λ-independent so that goes on forever Contributing Please refer to the  SciML ColPrac Contributor's Guide on Collaborative Practices for Community Packages  for guidance on PRs issues and other matters relating to contributing to SciML There are a few community forums the diffeq-bridged channel in the  Julia Slack JuliaDiffEq  on Gitter on the  Julia Discourse forums see also  SciML Community page"},{"doctype":"documentation","id":"references/SciMLBase.NoAD","title":"NoAD","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractQuadratureSolution","title":"AbstractQuadratureSolution","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.AbstractBC","title":"AbstractBC","text":"T  is the range type of the discretized function Boundary condition operators extrapolate the discretized function as illustrated in the README adding a ghost node at each end such that an interpolated polynomial satisfies the boundary condition Some of these are affine operators with a constant term not linear operators so using    and    is a minor abuse of notation For example when there is a non-zero Dirichlet boundary value the value at the ghost node is constant and does not scale proportional to the interior function values Because boundary condition operators are not linear they can not be concretized as matrices GhostDerivativeOperator  somehow concretizes as a pair of matrices The implementation of  AffineBC Vector  returns a  BoundaryPaddedVector  This is a structure that stores the ghost values separately from the original vector Presumably this is to avoid allocations although the  mul  method for  GhostDerivativeOperator  and  Matrix  method allocates a new  BoundaryPaddedArray  for each column In higher-dimensional cases it might return a  BoundaryPaddedArray  The    operator for a derivative multiplying a boundary condition packages them up as a  GhostDerivativeOperator  This cannot return a sparse matrix because the operator is affine not linear When a vector is multiplied by one of those it uses  AbstractBC AbstractVector  to pad with ghost nodes then  mul!(::DerivativeOperator AbstractVector  evaluates the derivative See also  DerivativeOperator   PeriodicBC   NeumannBC   DirichletBC   RobinBC "},{"doctype":"documentation","id":"references/SciMLBase.EnsembleThreads","title":"EnsembleThreads","text":"DocStringExtensions.TypeDefinition"},{"doctype":"document","id":"DiffEqOperators/operators/vector_calculus_operators.md","title":"Vector Calculus Operators","text":"approximation_order Int dx Union NTuple N AbstractVector NTuple N T len NTuple N Int coeff_func nothing approximation_order Int dx Union NTuple AbstractVector NTuple T len NTuple Int coeff_func nothing approximation_order Int dx Union NTuple N AbstractVector NTuple N T len NTuple N Int coeff_func nothing A AbstractArray T1 N B AbstractArray T2 N u AbstractArray T1 N A AbstractArray T2 N2 B AbstractArray T3 N2 A AbstractArray T1 B AbstractArray T2 u AbstractArray T1 A AbstractArray T2 B AbstractArray T3 A AbstractArray T N u AbstractArray T1 N1 A AbstractArray T2 N2 Vector Calculus Operators A good way to represent physical vectors is by storing them as a  N+1  dimensional matrix for a  N dimensional physical vector with each last index  i  storing the iᵗʰ component of it at grid point specified by the indices prior to it For e.g  u[p,q,r,2  stores the 2ⁿᵈ component at  x[p y[q z[r  Various operators and functions have been introduced here to carry out common calculus operations like  Gradient   Curl    square_norm  etc for them Operators All operators store  CenteredDifference  operators along various axes for computing the underlying derivatives lazily They differ in the way convolutions are performed Following are the constructors  These can then be used as  A*u   A  holding our constructor and  u  being the input  Array  either representing a multi-variable function in a  N dim Tensor which would be compatible with  Gradient  or the  N+1 dim Tensor representation desribed earlier holding our physical vector compatible with  Divergence  and  Curl  The arguments are  approximation_order   the order of the discretization in terms of O(dx^order dx   tuple containing the spacing of the discretization in order of dimensions When  dx  has  eltype  Number  that would imply uniform discretization while it also supports  eltype  Array{Number  for non-uniform grids len  tuple storing length of the discretization in the respective directions coeff_func  An operational argument which sets the coefficients of the operator If  coeff_func  is a  Number  then the coefficients are set to be constant with that number If  coeff_func  is an  AbstractArray  with length matching  len  then the coefficients are constant but spatially dependent Functions Some common functions used in Vector calculus that have been made available are  A  and   B  are  N+1 dim Tensors of same sizes The output would be a  N dim Tensor storing the corresponding value of operation at each grid point All of these support inplace operations with    notation as described above dot_product  translates to  A ⋅ B   cross_product  to  A × B  and  square_norm  to  L2-norm  in real sense"},{"doctype":"documentation","id":"references/Surrogates.cubicRadial","title":"cubicRadial","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.apply_div_rule","title":"apply_div_rule","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_ps","title":"get_ps","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractDiffEqLinearOperator","title":"AbstractDiffEqLinearOperator","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/DiffEqSensitivity.NoiseChoice","title":"NoiseChoice","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.AbstractDiffEqAffineOperator","title":"AbstractDiffEqAffineOperator","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_observed","title":"get_observed","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.has_𝑠vertex","title":"has_𝑠vertex","text":""},{"doctype":"documentation","id":"references/LabelledArrays.LVector","title":"LVector","text":"NamedTuple kwargs a b a b The standard constructor for  LVector  For example Creates a 1D copy of v1 with corresponding items in kwargs replaced For example"},{"doctype":"documentation","id":"references/PolyChaos.showbasis","title":"showbasis","text":"Show all basis polynomials given the recurrence coefficients  α   β  They keyword  sym  sets the name of the variable and  digits  controls the number of shown digits Tailored to types from  PolyChaos.jl Show all basis polynomials of an  AbstractOrthoPoly "},{"doctype":"documentation","id":"references/SciMLBase.__has_tgrad","title":"__has_tgrad","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.getprob","title":"getprob","text":""},{"doctype":"documentation","id":"references/PoissonRandom.ad_rand","title":"ad_rand","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.ODEForwardSensitivityProblem","title":"ODEForwardSensitivityProblem","text":"f u0 tspan p nothing sensealg kwargs sol asmatrix Val Val sol i Integer asmatrix Val Val sol t Union Number AbstractVector asmatrix Val Val f du u p t du dx p u p u u du dy p u u u p prob f p sol prob DP8 x dp sol x dp sol i x dp sol t x dp sol da dp plot sol t da lw ForwardDiff Calculus test_f p prob f eltype p eltype p p prob Vern9 abstol reltol save_everystep end p fd_res ForwardDiff test_f p calc_res Calculus finite_difference_jacobian test_f p x sol sol prob indvars da sol sol prob indvars sol prob indvars db sol sol prob indvars sol prob indvars dc sol sol prob indvars sol prob indvars function ODEForwardSensitivityProblem(f::Union                                       u0,tspan,p=nothing                                       alg::AbstractForwardSensitivityAlgorithm  ForwardSensitivity                                       kwargs Local forward sensitivity analysis gives a solution along with a timeseries of the sensitivities Thus if one wishes to have a derivative at every possible time point directly using the  ODEForwardSensitivityProblem  can be the most efficient method Warning ODEForwardSensitivityProblem requires being able to solve   a differential equation defined by the parameter struct  p  Thus while   DifferentialEquations.jl can support any parameter struct type usage   with ODEForwardSensitivityProblem requires that  p  could be a valid   type for being the initial condition  u0  of an array This means that   many simple types such as  Tuple s and  NamedTuple s will work as   parameters in normal contexts but will fail during ODEForwardSensitivityProblem   construction To work around this issue for complicated cases like nested structs    look into defining  p  using  AbstractArray  libraries such as RecursiveArrayTools.jl    or ComponentArrays.jl ODEForwardSensitivityProblem Syntax ODEForwardSensitivityProblem  is similar to an  ODEProblem  but takes an  AbstractForwardSensitivityAlgorithm  that describes how to append the forward sensitivity equation calculation to the time evolution to simultaneously compute the derivative of the solution with respect to parameters Once constructed this problem can be used in  solve  just like any other ODEProblem  The solution can be deconstructed into the ODE solution and sensitivities parts using the  extract_local_sensitivities  function with the following dispatches For information on the mathematics behind these calculations consult the sensitivity math page  sensitivity_math Example using an ODEForwardSensitivityProblem To define a sensitivity problem simply use the  ODEForwardSensitivityProblem  type instead of an ODE type For example we generate an ODE with the sensitivity equations attached for the Lotka-Volterra equations by This generates a problem which the ODE solvers can solve Note that the solution is the standard ODE system and the sensitivity system combined We can use the following helper functions to extract the sensitivity information In each case  x  is the ODE values and  dp  is the matrix of sensitivities The first gives the full timeseries of values and  dp[i  contains the time series of the sensitivities of all components of the ODE with respect to  i th parameter The second returns the  i th time step while the third interpolates to calculate the sensitivities at time  t  For example if we do then  da  is the timeseries for  frac{\\partial u(t)}{\\partial p  We can plot this transposing so that the rows the timeseries is plotted Local Sensitivity Solution Here we see that there is a periodicity to the sensitivity which matches the periodicity of the Lotka-Volterra solutions However as time goes on the sensitivity increases This matches the analysis of Wilkins in Sensitivity Analysis for Oscillating Dynamical Systems We can also quickly see that these values are equivalent to those given by automatic differentiation and numerical differentiation through the ODE solver Here we just checked the derivative at the end point Internal representation of the Solution For completeness we detail the internal representation When using ForwardDiffSensitivity the representation is with  Dual  numbers under the standard interpretation The values for the ODE's solution at time  i  are the  ForwardDiff.value.(sol[i  portions and the derivative with respect to parameter  j  is given by  ForwardDiff.partials.(sol[i])[j  When using ForwardSensitivity the solution to the ODE are the first  n  components of the solution This means we can grab the matrix of solution values like Since each sensitivity is a vector of derivatives for each function the sensitivities are each of size  sol.prob.indvars  We can pull out the parameter sensitivities from the solution as follows This means that  da[1,i  is the derivative of the  x(t  by the parameter  a  at time  sol.t[i  Note that all of the functionality available to ODE solutions is available in this case including interpolations and plot recipes the recipes will plot the expanded system"},{"doctype":"documentation","id":"references/PolyChaos.removeZeroWeights","title":"removeZeroWeights","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.Tail5","title":"Tail5","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.check_equations","title":"check_equations","text":"Assert that equations are well-formed when building ODE i.e only containing a single independent variable"},{"doctype":"documentation","id":"references/QuasiMonteCarlo.LatinHypercubeSample","title":"LatinHypercubeSample","text":"threading Samples using a Latin Hypercube Keyword arguments threading  whether to use threading Default is false i.e serial"},{"doctype":"documentation","id":"references/DiffEqSensitivity._jacNoise!","title":"_jacNoise!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.@showarr","title":"@showarr","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity._track_callback","title":"_track_callback","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.printbranch","title":"printbranch","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.ReverseDiffAdjoint","title":"ReverseDiffAdjoint","text":"ReverseDiffAdjoint  AbstractAdjointSensitivityAlgorithm An implementation of discrete adjoint sensitivity analysis using the ReverseDiff.jl tracing-based AD Supports in-place functions through an Array of Structs formulation and supports out of place through struct of arrays Constructor SciMLProblem Support This  sensealg  supports any  DEProblem  if the algorithm is  SciMLBase.isautodifferentiable  Requires that the state variables are CPU-based  Array  types"},{"doctype":"documentation","id":"references/LabelledArrays.@SLArray","title":"@SLArray","text":"ABCD a b c d x ABCD x a x b x c x x d x EFG e f g y EFG EFG e f g The macro creates a labelled static vector with element type  ElType  names from  Names  and size from  Size  If no eltype is given then the eltype is determined from the arguments in the constructor For example Users can also specify the indices directly"},{"doctype":"documentation","id":"references/NonlinearSolve.perform_step","title":"perform_step","text":""},{"doctype":"documentation","id":"references/Surrogates.EI","title":"EI","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_iv","title":"get_iv","text":""},{"doctype":"documentation","id":"references/Catalyst.hill_names","title":"hill_names","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.nonlinear_diffusion","title":"nonlinear_diffusion","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.collect_ivs_from_nested_operator!","title":"collect_ivs_from_nested_operator!","text":"Get all the independent variables with respect to which differentials/differences are taken"},{"doctype":"document","id":"ModelingToolkit/basics/DependencyGraphs.md","title":"Dependency Graphs","text":"Dependency Graphs Types Utility functions for  BiPartiteGraph s Functions for calculating dependency graphs"},{"doctype":"documentation","id":"references/NonlinearSolve.NewtonRaphsonCache","title":"NewtonRaphsonCache","text":""},{"doctype":"documentation","id":"references/LinearSolve.__init__","title":"__init__","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.SensitivityFunction","title":"SensitivityFunction","text":""},{"doctype":"documentation","id":"references/NeuralPDE.get_bounds_","title":"get_bounds_","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.slice_rmul","title":"slice_rmul","text":"slice_rmul lets you multiply each vector like strip of an array  u  with a linear operator  A  sliced along dimension  dim"},{"doctype":"documentation","id":"references/ModelingToolkit.NonZeros","title":"NonZeros","text":""},{"doctype":"documentation","id":"references/MethodOfLines.map_symbolic_to_discrete","title":"map_symbolic_to_discrete","text":""},{"doctype":"documentation","id":"references/LinearSolve.KrylovJL_CG","title":"KrylovJL_CG","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.QuadratureCache","title":"QuadratureCache","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.rms","title":"rms","text":""},{"doctype":"documentation","id":"references/SciMLBase.check_error","title":"check_error","text":"Check state of  integrator  and return one of the  Return Codes"},{"doctype":"documentation","id":"references/SciMLBase.build_solution","title":"build_solution","text":""},{"doctype":"documentation","id":"references/Catalyst.isconstant","title":"isconstant","text":"Tests if the given symbolic variable corresponds to a constant species"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.deleteat_stack!","title":"deleteat_stack!","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.NeuralHamiltonianDE","title":"NeuralHamiltonianDE","text":"model tspan args kwargs Contructs a Neural Hamiltonian DE Layer for solving Hamiltonian Problems parameterized by a Neural Network  HamiltonianNN  Arguments model  A Chain FastChain or Hamiltonian Neural Network that predicts the Hamiltonian of the system tspan  The timespan to be solved on kwargs  Additional arguments splatted to the ODE solver See the  Common Solver Arguments  documentation for more details"},{"doctype":"documentation","id":"references/ModelingToolkit.independent_variables","title":"independent_variables","text":"DocStringExtensions.TypedMethodSignatures Get the set of independent variables for the given system"},{"doctype":"documentation","id":"references/ModelingToolkit.collect_vars!","title":"collect_vars!","text":""},{"doctype":"documentation","id":"references/Catalyst.makemajump","title":"makemajump","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.process_variables!","title":"process_variables!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.var_derivative!","title":"var_derivative!","text":""},{"doctype":"documentation","id":"references/NeuralPDE.get_numeric_integral","title":"get_numeric_integral","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_delay_val","title":"get_delay_val","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.NonlinearFunctionExpr","title":"NonlinearFunctionExpr","text":"Create a Julia expression for an  ODEFunction  from the  ODESystem  The arguments  dvs  and  ps  are used to set the order of the dependent variable and parameter vectors respectively"},{"doctype":"documentation","id":"references/DiffEqFlux.FFJORD","title":"FFJORD","text":"model basedist nothing monte_carlo tspan args kwargs Constructs a continuous-time recurrent neural network also known as a neural ordinary differential equation neural ODE with fast gradient calculation via adjoints 1 and specialized for density estimation based on continuous normalizing flows CNF 2 with a stochastic approach 2 for the computation of the trace of the dynamics jacobian At a high level this corresponds to the following steps Parameterize the variable of interest x(t as a function f(z θ t of a base variable z(t with known density p_z Use the transformation of variables formula to predict the density p_x as a function of the density p_z and the trace of the Jacobian of f Choose the parameter θ to minimize a loss function of p_x usually the negative likelihood of the data After these steps one may use the NN model and the learned θ to predict the density p_x for new values of x Arguments model  A Chain neural network that defines the dynamics of the model basedist  Distribution of the base variable Set to the unit normal by default tspan  The timespan to be solved on kwargs  Additional arguments splatted to the ODE solver See the  Common Solver Arguments  documentation for more details References 1 Pontryagin Lev Semenovich Mathematical theory of optimal processes CRC press 1987 2 Chen Ricky TQ Yulia Rubanova Jesse Bettencourt and David Duvenaud Neural ordinary differential equations In Proceedings of the 32nd International Conference on Neural Information Processing Systems pp 6572-6583 2018 3 Grathwohl Will Ricky TQ Chen Jesse Bettencourt Ilya Sutskever and David Duvenaud Ffjord Free-form continuous dynamics for scalable reversible generative models arXiv preprint arXiv:1810.01367 2018"},{"doctype":"documentation","id":"references/ExponentialUtilities.ExpMethodDiagonalization","title":"ExpMethodDiagonalization","text":"Matrix exponential method corresponding to the diagonalization with  eigen  possibly by removing imaginary part introduced by the numerical approximation"},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.DST","title":"DST","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.process_NonlinearProblem","title":"process_NonlinearProblem","text":""},{"doctype":"document","id":"DiffEqSensitivity/manual/direct_forward_sensitivity.md","title":"[Direct Forward Sensitivity Analysis of ODEs]( forward_sense)","text":"Direct Forward Sensitivity Analysis of ODEs  forward_sense"},{"doctype":"documentation","id":"references/ModelingToolkit.detime_dvs","title":"detime_dvs","text":""},{"doctype":"documentation","id":"references/SciMLBase.get_proposed_dt","title":"get_proposed_dt","text":"Gets the proposed  dt  for the next timestep"},{"doctype":"documentation","id":"references/Catalyst.make_mmr_exp","title":"make_mmr_exp","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.updateparams!","title":"updateparams!","text":""},{"doctype":"documentation","id":"references/Catalyst.getsubsystypes!","title":"getsubsystypes!","text":""},{"doctype":"document","id":"DiffEqSensitivity/dde_fitting/delay_diffeq.md","title":"Delay Differential Equations","text":"DifferentialEquations OptimizationPolyalgorithms delay_lotka_volterra! du u h p t x y u α β δ γ p du dx α β y h p t du dy δ x γ y p h p t ones eltype p u0 prob_dde delay_lotka_volterra! u0 h constant_lags predict_dde p Array prob_dde MethodOfSteps Tsit5 u0 u0 p p saveat sensealg loss_dde p sum abs2 x x predict_dde p cb p l display loss_dde p cb p loss_dde p adtype optf x p loss_dde x adtype optprob optf p result_dde optprob PolyOpt p cb cb cb p l display loss_dde p cb p loss_dde p adtype optf x p loss_dde x adtype optprob optf p result_dde optprob PolyOpt p cb cb Delay Differential Equations Other differential equation problem types from DifferentialEquations.jl are supported For example we can build a layer with a delay differential equation like Notice that we chose  sensealg  ReverseDiffAdjoint  to utilize the ReverseDiff.jl reverse-mode to handle the delay differential equation We define a callback to display the solution at the current parameters for each step of the training We use  Optimization.solve  to optimize the parameters for our loss function"},{"doctype":"documentation","id":"references/SciMLBase.AbstractADType","title":"AbstractADType","text":"DocStringExtensions.TypeDefinition Base type for AD choices"},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.highest_order_variable_mask","title":"highest_order_variable_mask","text":""},{"doctype":"documentation","id":"references/Surrogates.std_error_at_point","title":"std_error_at_point","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timestep_median","title":"timestep_median","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.isoutput","title":"isoutput","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.vecjacobian!","title":"vecjacobian!","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.EXACT_SOLUTION_RIGHT","title":"EXACT_SOLUTION_RIGHT","text":""},{"doctype":"documentation","id":"references/SciMLBase.getsyms","title":"getsyms","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.generate_basis","title":"generate_basis","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.ReverseDiffVJP","title":"ReverseDiffVJP","text":"compile ReverseDiffVJP  VJPChoice Uses ReverseDiff.jl to compute the vector-Jacobian products If  f  is in-place then it uses a array of structs formulation to do scalarized reverse mode while if  f  is out-of-place then it uses an array-based reverse mode Usually the fastest when scalarized operations exist in the f function like in scientific machine learning applications like Universal Differential Equations and the boolean compilation is enabled i.e ReverseDiffVJP(true if EnzymeVJP fails on a given choice of  f  Does not support GPUs CuArrays Constructor Keyword Arguments compile  Whether to cache the compilation of the reverse tape This heavily increases the performance of the method but requires that the  f  function of the ODE/DAE/SDE/DDE has no branching"},{"doctype":"documentation","id":"references/NeuralPDE.v_semiinf","title":"v_semiinf","text":""},{"doctype":"documentation","id":"references/PolyChaos.AbstractTensor","title":"AbstractTensor","text":""},{"doctype":"documentation","id":"references/LabelledArrays.@LVector","title":"@LVector","text":"Type Names A Float64 a b c d A rand b a b c The  LVector  macro creates an  LArray  of dimension 1 with eltype and undefined values The vector's length is equal to the number of names given As with an  LArray  the user can initialize the vector and set its values later On the other hand users can also initialize the vector and set its values at the same time"},{"doctype":"documentation","id":"references/NeuralOperators.DeepONet","title":"DeepONet","text":"DeepONet(architecture_branch::Tuple architecture_trunk::Tuple act_branch  identity act_trunk  identity init_branch  Flux.glorot_uniform init_trunk  Flux.glorot_uniform bias_branch=true bias_trunk=true   DeepONet(branch_net::Flux.Chain trunk_net::Flux.Chain Create an unstacked DeepONet architecture as proposed by Lu et al arXiv:1910.03193 The model works as follows x  branch   ⊠--u  y  trunk  Where  x  represents the input function discretely evaluated at its respective sensors So the ipnut is of shape m for one instance or m x b for a training set  y  are the probing locations for the operator to be trained It has shape N x n for N different variables in the PDE i.e spatial and temporal coordinates with each n distinct evaluation points  u  is the solution of the queried instance of the PDE given by the specific choice of parameters Both inputs  x  and  y  are multiplied together via dot product Σᵢ bᵢⱼ tᵢₖ You can set up this architecture in two ways By Specifying the architecture and all its parameters as given above This always creates  Dense  layers for the branch and trunk net and corresponds to the DeepONet proposed by Lu et al By passing two architectures in the form of two Chain structs directly Do this if you want more flexibility and e.g use an RNN or CNN instead of simple  Dense  layers Strictly speaking DeepONet does not imply either of the branch or trunk net to be a simple DNN Usually though this is the case which is why it's treated as the default case here Example Consider a transient 1D advection problem ∂ₜu  u ⋅ ∇u  0 with an IC u(x,0  g(x We are given several b  200 instances of the IC discretized at 50 points each and want to query the solution for 100 different locations and times 0;1 That makes the branch input of shape 50 x 200 and the trunk input of shape 2 x 100 So the input for the branch net is 50 and 100 for the trunk net Usage"},{"doctype":"documentation","id":"references/SciMLBase.getops","title":"getops","text":""},{"doctype":"documentation","id":"references/Catalyst.pure_rate_arrows","title":"pure_rate_arrows","text":""},{"doctype":"document","id":"DiffEqFlux/layers/HamiltonianNN.md","title":"Hamiltonian Neural Network","text":"Hamiltonian Neural Network The following layer helps construct a neural network which allows learning dynamics and conservation laws by approximating the hamiltonian of a system"},{"doctype":"documentation","id":"references/Catalyst.hillar","title":"hillar","text":"An activation/repressing Hill rate function"},{"doctype":"documentation","id":"references/DiffEqSensitivity.NILSASProblem","title":"NILSASProblem","text":""},{"doctype":"document","id":"DiffEqSensitivity/ad_examples/differentiating_ode.md","title":"[Differentiating an ODE Solution with Automatic Differentiation]( auto_diff)","text":"DifferentialEquations lotka_volterra! du u p t du dx p u p u u du dy p u p u u p u0 prob lotka_volterra! u0 p sol prob Tsit5 reltol abstol ForwardDiff f x _prob prob u0 x p x end _prob Tsit5 reltol abstol saveat x u0 p dx ForwardDiff f x sum_of_solution u0 p _prob prob u0 u0 p p sum _prob Tsit5 reltol abstol saveat du01 dp1 Zygote gradient sum_of_solution u0 p sum_of_solution u0 p _prob prob u0 u0 p p sum _prob Tsit5 reltol abstol saveat sensealg du01 dp1 Zygote gradient sum_of_solution u0 p Differentiating an ODE Solution with Automatic Differentiation  auto_diff Note This tutorial assumes familiarity with DifferentialEquations.jl If you are not familiar with DifferentialEquations.jl please consult  the DifferentialEquations.jl documentation In this tutorial we will introduce how to use local sensitivity analysis via automatic differentiation The automatic differentiation interfaces are the most common ways that local sensitivity analysis is done It's fairly fast and flexible but most notably it's a very small natural extension to the normal differential equation solving code and is thus the easiest way to do most things Setup Let's first define a differential equation we wish to solve We will choose the Lotka-Volterra equation This is done via DifferentialEquations.jl using Now let's differentiate the solution to this ODE using a few different automatic differentiation methods Forward-Mode Automatic Differentiation with ForwardDiff.jl Let's say we need the derivative of the solution with respect to the initial condition  u0  and its parameters  p  One of the simplest ways to do this is via ForwardDiff.jl To do this all that one needs to do is use  the ForwardDiff.jl library  to differentiate some function  f  which uses a differential equation  solve  inside of it For example let's say we want the derivative of the first component of ODE solution with respect to these quantities at evenly spaced time points of  dt  1  We can compute this via Let's dig into what this is saying a bit  x  is a vector which concatenates the initial condition and parameters meaning that the first 2 values are the initial conditions and the last 4 are the parameters We use the  remake  function to build a function  f(x  which uses these new initial conditions and parameters to solve the differential equation and return the time series of the first component Then  ForwardDiff.jacobian(f,x  computes the Jacobian of  f  with respect to  x  The output  dx[i,j  corresponds to the derivative of the solution of the first component at time  t=j-1  with respect to  x[i  For example  dx[3,2  is the derivative of the first component of the solution at time  t=1  with respect to  p[1  Note Since  the global error is 1-2 orders of magnitude higher than the local error  we use accuracies of 1e-6 instead of the default 1e-3 to get reasonable sensitivities Reverse-Mode Automatic Differentiation The  solve  function is automatically compatible with AD systems like Zygote.jl  and thus there is no machinery that is necessary to use other than to put  solve  inside of a function that is differentiated by Zygote For example the following computes the solution to an ODE and computes the gradient of a loss function the sum of the ODE's output at each timepoint with dt=0.1 via the adjoint method Zygote.jl's automatic differentiation system is overloaded to allow SciMLSensitivity.jl to redefine the way the derivatives are computed allowing trade-offs between numerical stability memory and compute performance similar to how ODE solver algorithms are chosen The algorithms for differentiation calculation are called  AbstractSensitivityAlgorithms  or  sensealg s for short These are choosen by passing the  sensealg  keyword argument into solve Let's demonstrate this by choosing the  QuadratureAdjoint   sensealg  for the differentiation of this system Here this computes the derivative of the output with respect to the initial condition and the the derivative with respect to the parameters respectively using the  QuadratureAdjoint  For more information on the choices of sensitivity algorithms see the reference documentation in choosing sensitivity algorithms  sensitivity_diffeq When Should You Use Forward or Reverse Mode Good question The simple answer is if you are differentiating a system of 100 equations or less use forward-mode otherwise reverse-mode But it can be a lot more complicated than that For more information see the reference documentation in choosing sensitivity algorithms  sensitivity_diffeq"},{"doctype":"documentation","id":"references/ModelingToolkit.generate_control_jacobian","title":"generate_control_jacobian","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.transform","title":"transform","text":""},{"doctype":"documentation","id":"references/SciMLBase.StandardBVProblem","title":"StandardBVProblem","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.𝑑vertices","title":"𝑑vertices","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.lowest_order_variable_mask","title":"lowest_order_variable_mask","text":""},{"doctype":"documentation","id":"references/SciMLBase.get_dt","title":"get_dt","text":""},{"doctype":"documentation","id":"references/LinearSolve.KrylovKitJL","title":"KrylovKitJL","text":""},{"doctype":"documentation","id":"references/SciMLBase.wrapfun_iip","title":"wrapfun_iip","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.componentwise_weighted_meancov","title":"componentwise_weighted_meancov","text":""},{"doctype":"documentation","id":"references/Surrogates._dominates","title":"_dominates","text":""},{"doctype":"document","id":"NeuralPDE/examples/100_HJB.md","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation","text":"Flux GalacticFlux DifferentialEquations LinearAlgebra d X0 fill d tspan λ g X log sum X f X u σᵀ∇u p t λ sum σᵀ∇u μ_f X p t zero X σ_f X p t Diagonal sqrt ones Float32 d prob g f μ_f σ_f X0 tspan hls d opt Flux ADAM u0 Flux Dense d hls relu Dense hls hls relu Dense hls σᵀ∇u Flux Dense d hls relu Dense hls hls relu Dense hls hls relu Dense hls d pdealg u0 σᵀ∇u opt opt ans prob pdealg verbose maxiters trajectories alg EM dt pabstol d X0 fill d tspan λ g X log sum X f X u σᵀ∇u p t λ sum σᵀ∇u μ_f X p t zero X σ_f X p t Diagonal sqrt ones Float32 d prob g f μ_f σ_f X0 tspan hls d opt Flux ADAM u0 Flux Dense d hls relu Dense hls hls relu Dense hls σᵀ∇u Flux Dense d hls relu Dense hls hls relu Dense hls hls relu Dense hls d pdealg u0 σᵀ∇u opt opt ans prob pdealg verbose maxiters trajectories alg EM dt pabstol Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation First here's a fully working code for the solution of a 100-dimensional Hamilton-Jacobi-Bellman equation that takes a few minutes on a laptop Now let's explain the details H-J-B equation The Hamilton-Jacobi-Bellman equation is the solution to a stochastic optimal control problem Symbolic Solution Here we choose to solve the classical Linear Quadratic Gaussian LQG control problem of 100 dimensions which is governed by the SDE  dX_t  2sqrt(λ)c_t dt  sqrt(2)dW_t  where  c_t  is a control process The solution to the optimal control is given by a PDE of the form HJB with terminating condition  g(X  log(0.5f0  0.5f0*sum(X.^2  Solving LQG Problem with Neural Net Define the Problem To get the solution above using the  TerminalPDEProblem  we write Define the Solver Algorithm As described in the API docs we now need to define our  NNPDENS  algorithm by giving it the Flux.jl chains we want it to use for the neural networks  u0  needs to be a  d  dimensional  1 dimensional chain while  σᵀ∇u  needs to be  d+1  dimensional to  d  dimensions Thus we define the following Solving with Neural Net Here we want to solve the underlying neural SDE using the Euler-Maruyama SDE solver with our chosen  dt=0.2  do at most 100 iterations of the optimizer 100 SDE solves per loss evaluation for averaging and stop if the loss ever goes below  1f-2 "},{"doctype":"documentation","id":"references/ModelingToolkit.check_operator_variables","title":"check_operator_variables","text":"Check if all the LHS are unique"},{"doctype":"document","id":"NeuralPDE/examples/nnrode_example.md","title":"Solving Random Ordinary Differential Equations","text":"f u p t W u sin W tspan u0 dt W nothing Flux Dense elu Dense tanh opt ADAM alg W opt init_params sol prob W opt dt dt verbose abstol maxiters Solving Random Ordinary Differential Equations In this tutorial we will solve a RODE with  NNRODE  Consider the equation du  f(u,p,t,W)dt where  f(u,p,t,W)=2u\\sin(W  and  W(t  is a Noise process We start off by defining the  NoiseProcess   W(t  In this case we define a simple Gaussian Process See  Noise Processes  for defining other types of processes Then we need to define our model In order to define a model we can use  Flux.chain  or  DiffEqFlux.FastChain  And let's define our optimizer function Now let's pass all the parameters to the algorithm and then call the solver If we already have some initial parameters we can pass them into the  NNRODE  as well"},{"doctype":"documentation","id":"references/DiffEqSensitivity.get_jacvec","title":"get_jacvec","text":""},{"doctype":"documentation","id":"references/PolyChaos.genLaguerreMeasure","title":"genLaguerreMeasure","text":""},{"doctype":"documentation","id":"references/GlobalSensitivity.EASIResult","title":"EASIResult","text":""},{"doctype":"document","id":"DiffEqSensitivity/training_tips/local_minima.md","title":"Strategies to Avoid Local Minima","text":"pmin optprob NewtonTrustRegion cb cb maxiters allow_f_increases Lux DifferentialEquations Optimizaton OptimizationOptimJL Plots Random rng Random default_rng u0 Float32 datasize tspan tsteps range tspan tspan length datasize trueODEfunc du u p t true_A du u true_A prob_trueode trueODEfunc u0 tspan ode_data Array prob_trueode Tsit5 saveat tsteps dudt2 Lux ActivationFunction x x Lux Dense tanh Lux Dense pinit st Lux setup rng dudt2 prob_neuralode dudt2 tspan Vern7 saveat tsteps abstol reltol predict_neuralode p Array prob_neuralode u0 p st loss_neuralode p pred predict_neuralode p loss sum abs2 ode_data size pred pred loss pred iter callback p l pred doplot iter iter display l doplot plt scatter tsteps size pred ode_data size pred label scatter! plt tsteps size pred pred label display plot plt adtype optf x p loss_neuralode x adtype optprob optf Lux ComponentArray pinit result_neuralode optprob ADAM cb callback maxiters callback result_neuralode u loss_neuralode result_neuralode u doplot savefig prob_neuralode dudt2 Tsit5 saveat tsteps tsteps adtype optf x p loss_neuralode x adtype optprob optf ComponentArray pinit result_neuralode2 optprob ADAM cb callback maxiters callback result_neuralode2 u loss_neuralode result_neuralode2 u doplot savefig prob_neuralode dudt2 Tsit5 saveat tsteps tsteps optprob optf result_neuralode u result_neuralode3 optprob ADAM maxiters cb callback callback result_neuralode3 u loss_neuralode result_neuralode3 u doplot savefig prob_neuralode dudt2 Tsit5 saveat tsteps optprob optf result_neuralode3 u result_neuralode4 optprob ADAM maxiters cb callback callback result_neuralode4 u loss_neuralode result_neuralode4 u doplot savefig Flux Plots DifferentialEquations u0 Float32 datasize tspan tsteps range tspan tspan length datasize trueODEfunc du u p t true_A du u true_A prob_trueode trueODEfunc u0 tspan ode_data Array prob_trueode Tsit5 saveat tsteps dudt2 Dense tanh Dense p re Flux destructure dudt2 dudt u p t re p u prob dudt u0 tspan predict_n_ode Array prob u0 u0 p p saveat tsteps loss_n_ode pred predict_n_ode sqnorm x sum abs2 x loss sum abs2 ode_data pred loss cb doplot pred predict_n_ode display sum abs2 ode_data pred doplot pl plot tsteps ode_data label plot! pl tsteps pred label display plot pl predict_n_ode loss_n_ode cb doplot data Iterators repeated Flux train! loss_n_ode Flux u0 p data Flux Optimise ADAM cb cb u0 Float32 Flux train! loss_n_ode Flux p data Flux Optimise ADAM cb cb cb doplot datasize tspan tsteps range tspan tspan length datasize prob_trueode trueODEfunc u0 tspan ode_data Array prob_trueode Tsit5 saveat tsteps dudt2 Dense tanh Dense p re Flux destructure dudt2 dudt u p t re p u prob dudt u0 tspan data Iterators repeated Flux train! loss_n_ode Flux u0 p data Flux Optimise ADAM cb cb u0 Float32 Flux train! loss_n_ode Flux p data Flux Optimise ADAM cb cb cb doplot Strategies to Avoid Local Minima Local minima can be an issue with fitting neural differential equations However there are many strategies to avoid local minima Insert stochasticity into the loss function through minibatching Weigh the loss function to allow for fitting earlier portions first Changing the optimizers to  allow_f_increases Iteratively grow the fit Training the initial conditions and the parameters to start allow_f_increases=true With Optim.jl optimizers you can set  allow_f_increases=true  in order to let increases in the loss function not cause an automatic halt of the optimization process Using a method like BFGS or NewtonTrustRegion is not guaranteed to have monotonic convergence and so this can stop early exits which can result in local minima This looks like Iterative Growing Of Fits to Reduce Probability of Bad Local Minima In this example we will show how to use strategy 4 in order to increase the robustness of the fit Let's start with the same neural ODE example we've used before except with one small twist we wish to find the neural ODE that fits on  0,5.0  Naively we use the same training strategy as before However we've now fallen into a trap of a local minima If the optimizer changes the parameters so it dips early it will increase the loss because there will be more error in the later parts of the time series Thus it tends to just stay flat and never fit perfectly This thus suggests strategies 2 and 3 do not allow the later parts of the time series to influence the fit until the later stages Strategy 3 seems to be more robust so this is what will be demonstrated Let's start by reducing the timespan to  0,1.5  This fits beautifully Now let's grow the timespan and utilize the parameters from our  0,1.5  fit as the initial condition to our next fit Once again a great fit Now we utilize these parameters as the initial condition to the full fit Training both the initial conditions and the parameters to start In this example we will show how to use strategy 5 in order to accomplish the same goal except rather than growing the trajectory iteratively we can train on the whole trajectory We do this by allowing the neural ODE to learn both the initial conditions and parameters to start and then reset the initial conditions back and train only the parameters Note this strategy is demonstrated for the 0 5 time span and 0 10 any longer and more iterations will be required Alternatively one could use a mix of 4 and 5 or breaking up the trajectory into chunks and just 5 And there we go a set of robust strategies for fitting an equation that would otherwise get stuck in a local optima"},{"doctype":"document","id":"NeuralPDE/examples/optimal_stopping_american.md","title":"Optimal Stopping Times of American Options","text":"d r beta K T u0 fill d f du u p t du r u sigma du u p t du Diagonal beta u tspan T N dt tspan N g t x exp r t max K maximum x prob f sigma u0 tspan g g m Dense d tanh Dense tanh Dense N softmax opt Flux ADAM sdealg EM ensemblealg sol prob m opt sdealg ensemblealg verbose dt dt abstol maxiters trajectories Optimal Stopping Times of American Options Here we will aim to solve an optimal stopping problem using the  NNStopping  algorithm Let us consider standard American options Unlike European options American options can be exercized before their maturity and thus the problem reduces to finding an optimal stopping time As stated above since we can execute the option at any optimal time before the maturity of the option the standard Black-Scholes model gets modified to   frac{∂V}{∂t  rS\\frac{∂V}{∂S  frac{1}{2}{\\σ^2}{S^2}\\frac{∂^2 V}{\\∂S^2 rV ≤ 0 The stock price will follow a standard geometric brownian motion given by   dS_t  rS_tdt  σS_tdW_t And thus our final aim will be to calculate american_option We will be using a  SDEProblem  to denote a problem of this type We can define this as a  SDEProblem  and add a terminal condition  g  in order to price the American Options We will take the case of an American max put option with strike price  K  constant volatility  β  a risk-free rate  r  the initial stock price  u0  80.00  the maturity  T  and number of steps  N  The forcing function  f  and noise function  sigma  are defined for the type of model  See StochasticDiffEq documentation The final part is the payoff function payoff_func The discounted payoff function is Now in order to define an optimal stopping problem we will use the  SDEProblem  and pass the discounted payoff function  g  as an  kwarg  Finally let's build our neural network model using Flux GalacticFlux.jl Note that the final layer should be the softmax Flux.softmax function as we need the sum of probabilities at all stopping times to be 1 And then add an optimizer function We add algorithms to solve the SDE and the Ensemble These are the algorithms required to solve the  SDEProblem  we use the Euler-Maruyama algorithm in this case and the  EnsembleProblem  to run multiple simulations  See Ensemble Algorithms Finally we call the solve function"},{"doctype":"documentation","id":"references/Integrals.QuadSensitivityAlg","title":"QuadSensitivityAlg","text":""},{"doctype":"documentation","id":"references/GlobalSensitivity.GSAMethod","title":"GSAMethod","text":""},{"doctype":"documentation","id":"references/Integrals.__solvebp_call","title":"__solvebp_call","text":""},{"doctype":"documentation","id":"references/Integrals.IntegralVJP","title":"IntegralVJP","text":""},{"doctype":"documentation","id":"references/Catalyst.NodeID","title":"NodeID","text":""},{"doctype":"documentation","id":"references/Surrogates._calc_gek_coeffs","title":"_calc_gek_coeffs","text":""},{"doctype":"document","id":"ModelingToolkit/tutorials/ode_modeling.md","title":"Composing Ordinary Differential Equations","text":"t x t RHS t τ D Differential t fol D x x τ DifferentialEquations Plots plot prob fol x τ sol prob plot sol t x t τ D Differential t fol_model D x x τ DifferentialEquations Plots prob fol_model x τ plot prob RHS t fol_separate RHS x τ D x RHS fol_simplified fol_separate fol_simplified fol_simplified fol_model prob fol_simplified x τ sol prob plot sol x RHS f t fol_variable_f f sin t D x f x τ value_vector randn f_fun t t value_vector end value_vector Int floor t f_fun t fol_external_f f f_fun t D x f x τ prob fol_external_f x τ sol prob plot sol x f fol_factory separate name τ t x t f t RHS t eqs separate RHS f x τ D x RHS D x f x τ eqs name fol_1 fol_factory fol_2 fol_factory connections fol_1 f fol_2 f fol_1 x connected connections name connected fol_1 fol_2 connected_simp connected connected_simp u0 fol_1 x fol_2 x p fol_1 τ fol_2 τ prob connected_simp u0 p plot prob unitstep_fol_factory name τ t x t D x x τ name Dict x τ unitstep_fol_factory name fol BenchmarkTools prob Rodas4 prob_an connected_simp u0 p jac sparse prob_an Rodas4 Composing Ordinary Differential Equations This is an introductory example for the usage of ModelingToolkit MTK It illustrates the basic user-facing functionality by means of some examples of Ordinary Differential Equations ODE Some references to more specific documentation are given at appropriate places Copy-Pastable Simplified Example A much deeper tutorial with forcing functions and sparse Jacobians is all below But if you want to just see some code and run here's an example Simulation result of first-order lag element with right-hand side  Now let's start digging into MTK Your very first ODE Let us start with a minimal example The system to be modelled is a first-order lag element dot{x  frac{f(t  x(t)}{\\tau Here  t  is the independent variable time  x(t  is the scalar state variable  f(t  is an external forcing function and  tau  is a constant parameter In MTK this system can be modelled as follows For simplicity we first set the forcing function to a constant value Note that equations in MTK use the tilde character    as equality sign Also note that the  named  macro simply ensures that the symbolic name matches the name in the REPL If omitted you can directly set the  name  keyword After construction of the ODE you can solve it using  DifferentialEquations.jl  Simulation result of first-order lag element The initial state and the parameter values are specified using a mapping from the actual symbolic elements to their values represented as an array of  Pair s which are constructed using the    operator Algebraic relations and structural simplification You could separate the calculation of the right-hand side by introducing an intermediate variable  RHS  To directly solve this system you would have to create a Differential-Algebraic Equation DAE problem since besides the differential equation there is an additional algebraic equation now However this DAE system can obviously be transformed into the single ODE we used in the first example above MTK achieves this by means of structural simplification You can extract the equations from a system using  equations  and in the same way  states  and  parameters  The simplified equation is exactly the same as the original one so the simulation performance will also be the same However there is one difference MTK does keep track of the eliminated algebraic variables as observables see  Observables and Variable Elimination  That means MTK still knows how to calculate them out of the information available in a simulation result The intermediate variable  RHS  therefore can be plotted along with the state variable Note that this has to be requested explicitly though Simulation result of first-order lag element with right-hand side Note that similarly the indexing of the solution works via the names and so  sol[x  gives the timeseries for  x   sol[x,2:10  gives the 2nd through 10th values of  x  matching  sol.t  etc Note that this works even for variables which have been eliminated and thus  sol[RHS  retrieves the values of  RHS  Specifying a time-variable forcing function What if the forcing function the external input  f(t  is not constant Obviously one could use an explicit symbolic function of time But often there is time-series data such as measurement data from an experiment we want to embed as data in the simulation of a PDE or as a forcing function on the right-hand side of an ODE  is it is the case here For this MTK allows to register arbitrary Julia functions which are excluded from symbolic transformations but are just used as-is So you could for example interpolate a given time series using  DataInterpolations.jl  Here we illustrate this option by a simple lookup zero-order hold of a vector of random values Simulation result of first-order lag element step-wise forcing function Building component-based hierarchical models Working with simple one-equation systems is already fun but composing more complex systems from simple ones is even more fun Best practice for such a modeling framework could be to use factory functions for model components Such a factory can then used to instantiate the same component multiple times but allows for customization The  named  macro rewrites  fol_2  fol_factory(true  into  fol_2  fol_factory(true,:fol_2  Now these two components can be used as subsystems of a parent system i.e one level higher in the model hierarchy The connections between the components again are just algebraic relations All equations variables and parameters are collected but the structure of the hierarchical model is still preserved That is you can still get information about  fol_1  by addressing it by  connected.fol_1  or its parameter by  connected.fol_1.τ  Before simulation we again eliminate the algebraic variables and connection equations from the system using structural simplification As expected only the two state-derivative equations remain as if you had manually eliminated as many variables as possible from the equations Some observed variables are not expanded unless  full_equations  is used As mentioned above the hierarchical structure is preserved though So the initial state and the parameter values can be specified accordingly when building the  ODEProblem  Simulation of connected system two first-order lag elements in series More on this topic may be found in Composing Models and Building Reusable Components  acausal Defaults Often it is a good idea to specify reasonable values for the initial state and the parameters of a model component Then these do not have to be explicitly specified when constructing the  ODEProblem  Note that the defaults can be functions of the other variables which is then resolved at the time of the problem construction Of course the factory function could accept additional arguments to optionally specify the initial state or parameter values etc Symbolic and sparse derivatives One advantage of a symbolic toolkit is that derivatives can be calculated explicitly and that the incidence matrix of partial derivatives the sparsity pattern can also be explicitly derived These two facts lead to a substantial speedup of all model calculations e.g when simulating a model over time using an ODE solver By default analytical derivatives and sparse matrices e.g for the Jacobian the matrix of first partial derivatives are not used Let's benchmark this  prob  still is the problem using the  connected_simp  system above Now have MTK provide sparse analytical derivatives to the solver This has to be specified during the construction of the  ODEProblem  The speedup is significant For this small dense model 3 of 4 entries are populated using sparse matrices is counterproductive in terms of required memory allocations For large hierarchically built models which tend to be sparse speedup and the reduction of memory allocation can be expected to be substantial In addition these problem builders allow for automatic parallelism using the structural information For more information see the ODESystem  ODESystem page Notes and pointers how to go on Here are some notes that may be helpful during your initial steps with MTK Sometimes the symbolic engine within MTK is not able to correctly identify the independent variable e.g time out of all variables In such a case you usually get an error that some variable(s is missing from variable map In most cases it is then sufficient to specify the independent variable as second argument to  ODESystem  e.g  ODESystem(eqs t  A completely macro-free usage of MTK is possible and is discussed in a separate tutorial This is for package developers since the macros are only essential for automatic symbolic naming for modelers Vector-valued parameters and variables are possible A cleaner more consistent treatment of these is work in progress though Once finished this introductory tutorial will also cover this feature Where to go next Not sure how MTK relates to similar tools and packages Read  Comparison of ModelingToolkit vs Equation-Based and Block Modeling Languages  Depending on what you want to do with MTK have a look at some of the other  Symbolic Modeling Tutorials  If you want to automatically convert an existing function to a symbolic representation you might go through the  ModelingToolkitize Tutorials  To learn more about the inner workings of MTK consider the sections under  Basics  and  System Types "},{"doctype":"documentation","id":"references/LinearSolve.IterativeSolversJL_MINRES","title":"IterativeSolversJL_MINRES","text":""},{"doctype":"documentation","id":"references/SciMLBase.has_mul","title":"has_mul","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractSDEProblem","title":"AbstractSDEProblem","text":"DocStringExtensions.TypeDefinition Base for types which define SDE problems"},{"doctype":"documentation","id":"references/ModelingToolkit.generate_difference_cb","title":"generate_difference_cb","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractDEOptions","title":"AbstractDEOptions","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/DiffEqSensitivity.RODEUGradientWrapper","title":"RODEUGradientWrapper","text":""},{"doctype":"documentation","id":"references/NeuralPDE.TrainingStrategies","title":"TrainingStrategies","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.generate_connection_equations_and_stream_connections","title":"generate_connection_equations_and_stream_connections","text":""},{"doctype":"documentation","id":"references/RecursiveArrayTools.ArrayPartitionStyle","title":"ArrayPartitionStyle","text":""},{"doctype":"documentation","id":"references/LabelledArrays.labels2axes","title":"labels2axes","text":""},{"doctype":"documentation","id":"references/Catalyst.make_stoich_str","title":"make_stoich_str","text":""},{"doctype":"documentation","id":"references/Catalyst.@unpacksys","title":"@unpacksys","text":"sir SIR β S I I ν I R β ν sir Loads all species variables parameters and observables defined in  sys  as variables within the calling module For example will load the symbolic variables  S   I   R   ν  and  β  Notes Can not be used to load species variables or parameters of subsystems or constraints Either call  unpacksys  on those systems directly or  flatten  to collate them into one system before calling Note that this places symbolic variables within the calling module's scope so calling from a function defined in a script or the REPL will still result in the symbolic variables being defined in the  Main  module"},{"doctype":"documentation","id":"references/DiffEqOperators.AbstractComposedBoundaryPaddedArray","title":"AbstractComposedBoundaryPaddedArray","text":""},{"doctype":"document","id":"MethodOfLines/MOLFiniteDifference.md","title":"[Discretization] ( molfd)","text":"G DiffEqBase dxs time approx_order Int upwind_order Int grid_align G dxs time nothing approx_order upwind_order grid_align approx_order approx_order approx_order upwind_order typeof grid_align dxs time approx_order upwind_order grid_align Discretization   molfd Where  dxs  is a vector of pairs of parameters to the grid step in this dimension i.e  x=>0.2 y=>0.1  For a non uniform rectilinear grid replace any or all of the step sizes with the grid you'd like to use with that variable must be an  AbstractVector  but not a  StepRangeLen  Note that the second argument to  MOLFiniteDifference  is optional all parameters can be discretized if all required boundary conditions are specified Currently supported grid types  center_align  and  edge_align  Edge align will give better accuracy with Neumann boundary conditions center_align  naive grid starting from lower boundary ending on upper boundary with step of  dx edge_align  offset grid set halfway between the points that would be generated with center_align with extra points at either end that are above and below the supremum and infimum by  dx/2  This improves accuracy for Neumann BCs"},{"doctype":"documentation","id":"references/Catalyst.Edge","title":"Edge","text":""},{"doctype":"documentation","id":"references/PolyChaos.w_legendre","title":"w_legendre","text":""},{"doctype":"documentation","id":"references/LinearSolve.IterativeSolversJL_BICGSTAB","title":"IterativeSolversJL_BICGSTAB","text":""},{"doctype":"documentation","id":"references/MethodOfLines.get_discrete","title":"get_discrete","text":""},{"doctype":"documentation","id":"references/Surrogates.multiquadricRadial","title":"multiquadricRadial","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.AbstractDirectionalBoundaryPaddedArray","title":"AbstractDirectionalBoundaryPaddedArray","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.RODEAdjointProblem","title":"RODEAdjointProblem","text":""},{"doctype":"documentation","id":"references/MethodOfLines.MOLFiniteDifference","title":"MOLFiniteDifference","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.SymbolicContinuousCallbacks","title":"SymbolicContinuousCallbacks","text":""},{"doctype":"documentation","id":"references/GlobalSensitivity.FractionalFactorial","title":"FractionalFactorial","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.expv_timestep!","title":"expv_timestep!","text":"Non-allocating version of  expv_timestep "},{"doctype":"documentation","id":"references/SciMLBase.SteadyStateProblem","title":"SteadyStateProblem","text":"f u0 p kwargs f u0 p kwargs prob Defines an Defines a steady state ODE problem Documentation Page https://diffeq.sciml.ai/stable/types/steady state types Mathematical Specification of a Steady State Problem To define an Steady State Problem you simply need to give the function  f  which defines the ODE frac{du}{dt  f(u,p,t and an initial guess  u_0  of where  f(u,p,t)=0   f  should be specified as  f(u,p,t  or in-place as  f(du,u,p,t  and  u₀  should be an AbstractArray or number whose geometry matches the desired geometry of  u  Note that we are not limited to numbers or vectors for  u₀  one is allowed to provide  u₀  as arbitrary matrices  higher dimension tensors as well Note that for the steady-state to be defined we must have that  f  is autonomous that is  f  is independent of  t  But the form which matches the standard ODE solver should still be used The steady state solvers interpret the  f  by fixing  t=0  Problem Type Constructors isinplace  optionally sets whether the function is inplace or not This is determined automatically but not inferred Additionally the constructor from  ODEProblem s is provided Parameters are optional and if not given then a  NullParameters  singleton will be used which will throw nice errors if you try to index non-existent parameters Any extra keyword arguments are passed on to the solvers For example if you set a  callback  in the problem then that  callback  will be added in every solve call For specifying Jacobians and mass matrices see the DiffEqFunctions  performance_overloads page Fields f  The function in the ODE u0  The initial guess for the steady state p  The parameters for the problem Defaults to  NullParameters kwargs  The keyword arguments passed onto the solves Special Solution Fields The  SteadyStateSolution  type is different from the other DiffEq solutions because it does not have temporal information DocStringExtensions.MethodSignatures Define a steady state problem using an instance of  AbstractODEFunction   AbstractODEFunction DocStringExtensions.MethodSignatures Define a steady state problem from a standard ODE problem"},{"doctype":"documentation","id":"references/SciMLBase.DEFAULT_UPDATE_FUNC","title":"DEFAULT_UPDATE_FUNC","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.n_extra_equations","title":"n_extra_equations","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.get_chunksize","title":"get_chunksize","text":""},{"doctype":"documentation","id":"references/Catalyst.edgifycomplex","title":"edgifycomplex","text":""},{"doctype":"documentation","id":"references/PolyChaos.build_w_genlaguerre","title":"build_w_genlaguerre","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.ForwardDiffOverAdjoint","title":"ForwardDiffOverAdjoint","text":"sensealg ForwardDiffOverAdjoint  AbstractSecondOrderSensitivityAlgorithm ForwardDiff.jl over a choice of  sensealg  method for the adjoint Constructor SciMLProblem Support This supports any SciMLProblem that the  sensealg  choice supports provided the solver algorithm is  SciMLBase.isautodifferentiable  References Hindmarsh A C and Brown P N and Grant K E and Lee S L and Serban R and Shumaker D E and Woodward C S SUNDIALS Suite of nonlinear and differential/algebraic equation solvers ACM Transactions on Mathematical Software TOMS 31 pp:363–396 2005"},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timeseries_point_mean","title":"timeseries_point_mean","text":""},{"doctype":"document","id":"Catalyst/tutorials/basic_examples.md","title":"Basic Chemical Reaction Network Examples","text":"rs c1 X X c2 X c3 X c1 c2 c3 p c1 c2 c3 tspan u0 X oprob rs u0 tspan p osol oprob Tsit5 ssprob rs u0 p sssol ssprob SSRootfind sprob rs u0 tspan p ssol sprob EM dt u0 X dprob rs u0 tspan p jprob JumpProblem rs dprob Direct jsol jprob SSAStepper rs c1 S E SE c2 SE S E c3 SE P E c1 c2 c3 p c1 c2 c3 tspan u0 S E SE P oprob rs u0 tspan p osol oprob Tsit5 u0 S E SE P dprob rs u0 tspan p jprob JumpProblem rs dprob Direct jsol jprob SSAStepper Basic Chemical Reaction Network Examples Example Birth-Death Process Example Michaelis-Menten Enzyme Kinetics"},{"doctype":"documentation","id":"references/NeuralPDE.MiniMaxAdaptiveLoss","title":"MiniMaxAdaptiveLoss","text":"A way of adaptively reweighting the components of the loss function in the total sum such that the loss weights are maximized by an internal optimiser which leads to a behavior where loss functions that have not been satisfied get a greater weight reweight_every  how often to reweight the PDE and BC loss functions measured in iterations  reweighting is cheap since it re-uses the value of loss functions generated during the main optimisation loop pde_max_optimiser  a Flux.Optimise.AbstractOptimiser that is used internally to maximize the weights of the PDE loss functions bc_max_optimiser  a Flux.Optimise.AbstractOptimiser that is used internally to maximize the weights of the BC loss functions pde_loss_weights  either a scalar which will be broadcast or vector the size of the number of PDE equations which describes the initial weight the respective PDE loss has in the full loss sum bc_loss_weights  either a scalar which will be broadcast or vector the size of the number of BC equations which describes the initial weight the respective BC loss has in the full loss sum additional_loss_weights  a scalar which describes the weight the additional loss function has in the full loss sum this is currently not adaptive and will be constant with this adaptive loss from paper Self-Adaptive Physics-Informed Neural Networks using a Soft Attention Mechanism Levi McClenny Ulisses Braga-Neto https://arxiv.org/abs/2009.04544"},{"doctype":"documentation","id":"references/DiffEqFlux.SinBasis","title":"SinBasis","text":"n Constructs a sine basis of the form sin(x sin(2 x  sin(n x Arguments n  number of terms in the sine expansion"},{"doctype":"documentation","id":"references/ModelingToolkit.push_eqs!","title":"push_eqs!","text":""},{"doctype":"documentation","id":"references/SciMLBase.step!","title":"step!","text":"Perform one successful step on the integrator Alternative if a  dt  is given then  step  the integrator until there is a temporal difference  ≥ dt  in  integ.t   When  true  is passed to the optional third argument the integrator advances exactly  dt "},{"doctype":"documentation","id":"references/SciMLBase.calculate_solution_errors!","title":"calculate_solution_errors!","text":""},{"doctype":"documentation","id":"references/SciMLBase.discretize","title":"discretize","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.stochastic_integral_transform","title":"stochastic_integral_transform","text":"DocStringExtensions.TypedMethodSignatures Choose correction_factor=-1//2 1//2 to converte Ito  Stratonovich Stratonovich->Ito"},{"doctype":"documentation","id":"references/Catalyst.combinatoric_ratelaws","title":"combinatoric_ratelaws","text":"Returns the effective default  combinatoric_ratelaw  value for a compositional system calculated by taking the logical or of each component  ReactionSystem  Can be overriden during calls to  convert  of problem constructors"},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.poly_deg","title":"poly_deg","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.unit_indices","title":"unit_indices","text":"A function that creates a tuple of CartesianIndices of unit length and  N  dimensions one pointing along each dimension"},{"doctype":"documentation","id":"references/ModelingToolkit.zero!","title":"zero!","text":""},{"doctype":"document","id":"NeuralOperators/index.md","title":"NeuralOperators","text":"model Dense gelu gelu gelu Dense gelu Dense model ch modes σ gelu loss 𝐱 𝐲 model 𝐱 𝐲 opt Flux Optimiser WeightDecay Flux ADAM Flux Flux train! loss model data opt model σ tanh branch Dense σ Dense σ trunk Dense tanh Dense tanh model branch trunk loss xtrain ytrain sensor Flux Losses mse model xtrain sensor ytrain evalcb loss xval yval grid learning_rate opt ADAM learning_rate model Flux Flux train! loss xtrain ytrain grid opt cb evalcb NeuralOperators      Ground Truth   Inferenced  The demonstration shown above is Navier-Stokes equation learned by the  MarkovNeuralOperator  with only one time step information Example can be found in  example/FlowOverCircle  Quick start The package can be installed with the Julia package manager From the Julia REPL type    to enter the Pkg REPL mode and run Usage Fourier Neural Operator Or one can just call And then train as a Flux model DeepONet Or specify branch and trunk as separate  Chain  from Flux and pass to  DeepONet You can again specify loss optimization and training parameters just as you would for a simple neural network with Flux A more complete example using DeepONet architecture to solve Burgers equation can be found in the  examples "},{"doctype":"documentation","id":"references/DiffEqSensitivity.SteadyStateAdjointSensitivityFunction","title":"SteadyStateAdjointSensitivityFunction","text":""},{"doctype":"documentation","id":"references/PolyChaos.EmptyQuad","title":"EmptyQuad","text":""},{"doctype":"document","id":"DiffEqSensitivity/neural_ode/neural_ode_galacticoptim.md","title":"Neural Ordinary Differential Equations with GalacticOptim.jl","text":"DifferentialEquations Plots GalacticOptim GalacticFlux GalacticOptimJL u0 Float32 datasize tspan tsteps range tspan tspan length datasize trueODEfunc du u p t true_A du u true_A prob_trueode trueODEfunc u0 tspan ode_data Array prob_trueode Tsit5 saveat tsteps dudt2 x p x tanh prob_neuralode dudt2 tspan Tsit5 saveat tsteps predict_neuralode p Array prob_neuralode u0 p loss_neuralode p pred predict_neuralode p loss sum abs2 ode_data pred loss pred callback p l pred doplot display l plt scatter tsteps ode_data label scatter! plt tsteps pred label doplot display plot plt adtype GalacticOptim optf GalacticOptim loss_neuralode adtype optprob GalacticOptim optfunc prob_neuralode p result_neuralode GalacticOptim optprob ADAM cb callback maxiters optprob2 optprob u0 result_neuralode u result_neuralode2 GalacticOptim optprob2 LBFGS cb callback allow_f_increases DifferentialEquations Plots u0 Float32 datasize tspan tsteps range tspan tspan length datasize trueODEfunc du u p t true_A du u true_A prob_trueode trueODEfunc u0 tspan ode_data Array prob_trueode Tsit5 saveat tsteps dudt2 x p x tanh prob_neuralode dudt2 tspan Tsit5 saveat tsteps dudt2 x x Dense tanh Dense predict_neuralode p Array prob_neuralode u0 p loss_neuralode p pred predict_neuralode p loss sum abs2 ode_data pred loss pred callback p l pred doplot display l plt scatter tsteps ode_data label scatter! plt tsteps pred label doplot display plot plt Neural Ordinary Differential Equations with GalacticOptim.jl DiffEqFlux.jl defines  sciml_train  which is a high level utility that automates a lot of the choices using heuristics to determine a potentially efficient method However in some cases you may want more control over the optimization process The underlying optimization package behind  sciml_train  is  GalacticOptim.jl  In this tutorial we will show how to more deeply interact with the optimization library to tweak its processes We can use a neural ODE as our example A neural ODE is an ODE where a neural network defines its derivative function Thus for example with the multilayer perceptron neural network  FastChain(FastDense(2 50 tanh FastDense(50 2  we obtain  the following results Copy-Pasteable Code Before getting to the explanation here's some code to start with We will follow a full explanation of the definition and training process Neural ODE Explanation Let's get a time series array from the Lotka-Volterra equation as data Now let's define a neural network with a  NeuralODE  layer First we define the layer Here we're going to use  FastChain  which is a faster neural network structure for NeuralODEs Note that we can directly use  Chain s from Flux.jl as well for example In our model we used the  x  x.^3  assumption in the model By incorporating structure into our equations we can reduce the required size and training time for the neural network but a good guess needs to be known From here we build a loss function around it The  NeuralODE  has an optional second argument for new parameters which we will use to iteratively change the neural network in our training loop We will use the L2 loss of the network's output against the time series data We define a callback function We then train the neural network to learn the ODE Here we showcase starting the optimization with  ADAM  to more quickly find a minimum and then honing in on the minimum by using  LBFGS  By using the two together we are able to fit the neural ODE in 9 seconds Note the timing commented out the plotting You can easily incorporate the procedure below to set up custom optimization problems For more information on the usage of  GalacticOptim.jl  please consult  this  documentation The  x  and  p  variables in the optimization function are different than  x  and  p  above The optimization function runs over the space of parameters of the original problem so  x_optimization    p_original  We then complete the training using a different optimizer starting from where  ADAM  stopped We do  allow_f_increases=false  to make the optimization automatically halt when near the minimum"},{"doctype":"documentation","id":"references/ModelingToolkit.has_dvs","title":"has_dvs","text":""},{"doctype":"document","id":"ModelingToolkit/systems/SDESystem.md","title":"SDESystem","text":"jacobian_sparsity SDESystem System Constructors To convert an  ODESystem  to an  SDESystem  directly Composition and Accessor Functions get_eqs(sys  or  equations(sys  The equations that define the SDE get_states(sys  or  states(sys  The set of states in the SDE get_ps(sys  or  parameters(sys  The parameters of the SDE get_iv(sys  The independent variable of the SDE Transformations Analyses Applicable Calculation and Generation Functions Problem Constructors"},{"doctype":"documentation","id":"references/ExponentialUtilities.inplace_add!","title":"inplace_add!","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.CompleteHalfCenteredDifference","title":"CompleteHalfCenteredDifference","text":"A helper function to compute the coefficients of a derivative operator including the boundary coefficients in the half offset centered scheme See table 2 in https://web.njit.edu/~jiang/math712/fornberg.pdf"},{"doctype":"documentation","id":"references/NeuralPDE.get_numeric_derivative","title":"get_numeric_derivative","text":""},{"doctype":"document","id":"DiffEqSensitivity/ad_examples/chaotic_ode.md","title":"[Sensitivity analysis for chaotic systems (shadowing methods)]( shadowing_methods)","text":"OrdinaryDiffEq lorenz! du u p t du u u du u p u u du u u u p tspan u0 prob lorenz! u0 tspan p sol prob Vern9 abstol reltol sol2 prob Vern9 abstol eps Float64 reltol lorenz! du u p t du p u u du u p u u du u u p u p tspan_init tspan_attractor u0 rand prob_init lorenz! u0 tspan_init p sol_init prob_init Tsit5 prob_attractor lorenz! sol_init end tspan_attractor p g u p t u end G p _prob prob_attractor p p _sol _prob Vern9 abstol reltol saveat sensealg alpha g g sum _sol u dp1 Zygote gradient p G p p lss_problem sol_attractor alpha g resfw lss_problem res dp1 atol Sensitivity analysis for chaotic systems shadowing methods  shadowing_methods Let us define the instantaneous objective  g(u,p  which depends on the state  u  and the parameter  p  of the differential equation Then if the objective is a long-time average quantity langle g rangle_∞  lim_{T rightarrow ∞ langle g rangle_T where langle g rangle_T  frac{1}{T int_0^T g(u,p text{d}t under the assumption of ergodicity  langle g rangle_∞  only depends on  p  In the case of chaotic systems the trajectories diverge with  O(1  error This can be seen for instance when solving the  Lorenz system  at  1e-14  tolerances with 9th order integrators and a small machine-epsilon perturbation Chaotic behavior of the Lorenz system More formally such chaotic behavior can be analyzed using tools from uncertainty quantification  uncertainty_quantification This effect of diverging trajectories is known as the butterfly effect and can be formulated as most small perturbations on initial conditions or parameters lead to new trajectories diverging exponentially fast from the original trajectory The latter statement can be roughly translated to the level of sensitivity calculation as follows For most initial conditions the homogeneous tangent solutions grow exponentially fast To compute derivatives of an objective  langle g rangle_∞  with respect to the parameters  p  of a chaotic systems one thus encounters that traditional forward and adjoint sensitivity methods diverge because the tangent space diverges with a rate given by the Lyapunov exponent Taking the average of these derivative can then also fail i.e one finds that the average derivative is not the derivative of the average Although numerically computed chaotic trajectories diverge from the true/original trajectory the  shadowing theorem  guarantees that there exists an errorless trajectory with a slightly different initial condition that stays near shadows the numerically computed one see e.g the  blog post  or the  non-intrusive least squares shadowing paper  for more details Essentially the idea is to replace the ill-conditioned ODE by a well-conditioned optimization problem Shadowing methods use the shadowing theorem within a renormalization procedure to distill the long-time effect from the joint observation of the long-time and the butterfly effect This allows us to accurately compute derivatives w.r.t the long-time average quantities The following  sensealg  choices exist ForwardLSS(;alpha=CosWindowing(),ADKwargs  An implementation of the forward  least square shadowing  method For  alpha  one can choose between two different windowing options  CosWindowing  default and  Cos2Windowing  and  alpha::Number  which corresponds to the weight of the time dilation term in  ForwardLSS  AdjointLSS(;alpha=10.0,ADKwargs  An implementation of the adjoint-mode  least square shadowing  method  alpha  controls the weight of the time dilation term in  AdjointLSS  NILSS(nseg nstep rng  Xorshifts.Xoroshiro128Plus(rand(UInt64 ADKwargs  An implementation of the  non-intrusive least squares shadowing NILSS  method  nseg  is the number of segments  nstep  is the number of steps per segment NILSAS(nseg nstep M=nothing rng  Xorshifts.Xoroshiro128Plus(rand(UInt64 ADKwargs  An implementation of the  non-intrusive least squares adjoint shadowing NILSAS  method  nseg  is the number of segments  nstep  is the number of steps per segment  M  nus  1  has to be provided where  nus  is the number of unstable covariant Lyapunov vectors Recommendation Since the computational and memory costs of  NILSS  scale with the number of positive unstable Lyapunov it is typically less expensive than  ForwardLSS   AdjointLSS  and  NILSAS  are favorable for a large number of system parameters As an example for the Lorenz system with  g(u,p,t  u[3  i.e the  z  coordinate as the instantaneous objective we can use the direct interface by passing  ForwardLSS  as the  sensealg  Alternatively we can define the  ForwardLSSProblem  and solve it via  shadow_forward  as follows"},{"doctype":"documentation","id":"references/SciMLBase.TwoPointBVPFunction","title":"TwoPointBVPFunction","text":"DocStringExtensions.TypeDefinition"},{"doctype":"document","id":"DiffEqSensitivity/ode_fitting/stiff_ode_fit.md","title":"Parameter Estimation on Highly Stiff Systems","text":"DifferentialEquations OptimizationOptimJL LinearAlgebra ForwardDiff DiffEqBase Plots rober du u p t y₁ y₂ y₃ u k₁ k₂ k₃ p du k₁ y₁ k₃ y₂ y₃ du k₁ y₁ k₂ y₂ k₃ y₂ y₃ du k₂ y₂ nothing p u0 prob rober u0 p sol prob Rosenbrock23 ts sol t Js map u I ForwardDiff rober p u sol u predict_adjoint p p exp p _prob prob p p Array _prob Rosenbrock23 autodiff saveat ts sensealg autojacvec loss_adjoint p prediction predict_adjoint p prediction prediction i i axes prediction diff map J u data J abs2 u data Js prediction sol u loss sum abs sum diff sqrt loss prediction cb p l pred println l println exp p plot prob p exp p Rosenbrock23 display initp ones cb initp loss_adjoint initp adtype optf x p loss_adjoint x p adtype optprob optf initp res optprob ADAM cb cb maxiters optprob2 optf res u res2 optprob2 BFGS cb cb maxiters allow_f_increases println p round exp res2 u sigdigits round norm exp res2 u p norm p sigdigits DifferentialEquations OptimizationOptimJL LinearAlgebra ForwardDiff DiffEqBase Plots rober du u p t y₁ y₂ y₃ u k₁ k₂ k₃ p du k₁ y₁ k₃ y₂ y₃ du k₁ y₁ k₂ y₂ k₃ y₂ y₃ du k₂ y₂ nothing p u0 prob rober u0 p sol prob Rosenbrock23 ts sol t Js map u I ForwardDiff rober p u sol u predict_adjoint p p exp p _prob prob p p Array _prob Rosenbrock23 autodiff saveat ts sensealg autojacvec loss_adjoint p prediction predict_adjoint p prediction prediction i i axes prediction diff map J u data J abs2 u data Js prediction sol u loss sum abs sum diff sqrt loss prediction cb p l pred println l println exp p plot prob p exp p Rosenbrock23 display initp ones cb initp loss_adjoint initp adtype optf x p loss_adjoint x adtype optprob optf initp res optprob ADAM cb cb maxiters optprob2 optf res u res2 optprob2 BFGS cb cb maxiters allow_f_increases println p round exp res2 u sigdigits round norm exp res2 u p norm p sigdigits Parameter Estimation on Highly Stiff Systems This tutorial goes into training a model on stiff chemical reaction system data Copy-Pasteable Code Before getting to the explanation here's some code to start with We will follow a full explanation of the definition and training process Output Explanation First let's get a time series array from the Robertson's equation as data Note that we also computed a shifted and scaled Jacobian along with the solution We will use this matrix to scale the loss later We fit the parameters in log space so we need to compute  exp.(p  to get back the original parameters The difference between the data and the prediction is weighted by the transformed Jacobian to do a relative scaling of the loss We define a callback function We then use a combination of  ADAM  and  BFGS  to minimize the loss function to accelerate the optimization The initial guess of the parameters are chosen to be  1 1 1.0  Finally we can analyze the difference between the fitted parameters and the ground truth It gives the output"},{"doctype":"documentation","id":"references/DiffEqOperators.cross_product!","title":"cross_product!","text":""},{"doctype":"documentation","id":"references/SciMLBase.num_types_in_tuple","title":"num_types_in_tuple","text":"DocStringExtensions.MethodSignatures Get the number of parameters of a Tuple type i.e the number of fields"},{"doctype":"documentation","id":"references/Surrogates._hinge","title":"_hinge","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.namespaced_var","title":"namespaced_var","text":""},{"doctype":"documentation","id":"references/Surrogates._backward_pass_1d","title":"_backward_pass_1d","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity","title":"DiffEqSensitivity","text":""},{"doctype":"documentation","id":"references/RecursiveArrayTools.recursive_bottom_eltype","title":"recursive_bottom_eltype","text":""},{"doctype":"documentation","id":"references/MethodOfLines.calculate_weights","title":"calculate_weights","text":""},{"doctype":"document","id":"Surrogates/optimizations.md","title":"Optimization techniques","text":"Optimization techniques SRBF LCBS EI DYCORS SOP Adding another optimization method To add another optimization method you just need to define a new SurrogateOptimizationAlgorithm and write its corresponding algorithm overloading the following"},{"doctype":"documentation","id":"references/Optimization.AutoZygote","title":"AutoZygote","text":"f kwargs AutoZygote  AbstractADType An AbstractADType choice for use in OptimizationFunction for automatically generating the unspecified derivative functions Usage This uses the  Zygote.jl  package This is the staple reverse-mode AD that handles a large portion of Julia with good efficiency Hessian construction is fast via forward-over-reverse mixing ForwardDiff.jl with Zygote.jl Compatible with GPUs Compatible with Hessian-based optimization via ForwardDiff Compatible with Hv-based optimization via ForwardDiff Not compatible with constraint functions Note that only the unspecified derivative functions are defined For example if a  hess  function is supplied to the  OptimizationFunction  then the Hessian is not defined via Zygote"},{"doctype":"documentation","id":"references/Catalyst.addconstraints!","title":"addconstraints!","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractNonlinearProblem","title":"AbstractNonlinearProblem","text":"DocStringExtensions.TypeDefinition Base for types which define nonlinear solve problems f(u)=0"},{"doctype":"documentation","id":"references/PolyChaos.Uniform01OrthoPoly","title":"Uniform01OrthoPoly","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.pCN!","title":"pCN!","text":"Create a new but correlated noise process from  noise  and additional entropy with correlation ρ This update defines an autoregressive process in the space of Wiener or noise process trajectories which can be used as proposal distribution in Metropolis-Hastings algorithms often called preconditioned Crank–Nicolson scheme External links Preconditioned Crank–Nicolson algorithm on Wikipedia"},{"doctype":"documentation","id":"references/ModelingToolkit._positivemax","title":"_positivemax","text":""},{"doctype":"documentation","id":"references/SciMLBase.RightRootFind","title":"RightRootFind","text":""},{"doctype":"documentation","id":"references/Catalyst.mmr","title":"mmr","text":"A repressive Michaelis-Menten rate function"},{"doctype":"documentation","id":"references/SciMLBase.EnsembleTestSolution","title":"EnsembleTestSolution","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/ModelingToolkit.AbstractTimeIndependentSystem","title":"AbstractTimeIndependentSystem","text":""},{"doctype":"documentation","id":"references/MethodOfLines.nvars","title":"nvars","text":""},{"doctype":"document","id":"Surrogates/randomforest.md","title":"randomforest","text":"Random forests surrogate tutorial Note This surrogate requires the SurrogatesRandomForest module which can be added by inputting add SurrogatesRandomForest from the Julia command line Random forests is a supervised learning algorithm that randomly creates and merges multiple decision trees into one forest We are going to use a Random forests surrogate to optimize  f(x)=sin(x)+sin(10/3  x  First of all import  Surrogates  and  Plots  Sampling We choose to sample f in 4 points between 0 and 1 using the  sample  function The sampling points are chosen using a Sobol sequence this can be done by passing  SobolSample  to the  sample  function Building a surrogate With our sampled points we can build the Random forests surrogate using the  RandomForestSurrogate  function randomforest_surrogate  behaves like an ordinary function which we can simply plot Addtionally you can specify the number of trees created using the parameter num_round Optimizing Having built a surrogate we can now use it to search for minimas in our original function  f  To optimize using our surrogate we call  surrogate_optimize  method We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique Random Forest ND First of all we will define the  Bukin Function N 6  function we are going to build surrogate for Sampling Let's define our bounds this time we are working in two dimensions In particular we want our first dimension  x  to have bounds  5 10  and  0 15  for the second dimension We are taking 50 samples of the space using Sobol Sequences We then evaluate our function on all of the sampling points Building a surrogate Using the sampled points we build the surrogate the steps are analogous to the 1-dimensional case Optimizing With our surrogate we can now search for the minimas of the function Notice how the new sampled points which were created during the optimization process are appended to the  xys  array This is why its size changes"},{"doctype":"documentation","id":"references/SciMLBase.user_cache","title":"user_cache","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.liftnothing","title":"liftnothing","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractDDEIntegrator","title":"AbstractDDEIntegrator","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/ModelingToolkit.SystemStructures.TearingState","title":"TearingState","text":""},{"doctype":"document","id":"DiffEqNoiseProcess/index.md","title":"DiffEqNoiseProcess.jl: Noise Processes for Stochastic Modeling","text":"Pkg Pkg add μ σ W μ σ prob f g u0 tspan noise W noise tspan μ σ W μ σ prob W sol prob dt monte_prob MonteCarloProblem prob sol monte_prob dt num_monte W dt W dt dt u nothing p nothing W dt u p i W dt u p DiffEqNoiseProcess.jl Noise Processes for Stochastic Modeling Noise processes are essential in continuous stochastic modeling The  NoiseProcess  types are distributionally-exact meaning they are not solutions of stochastic differential equations and instead are directly generated according to their analytical distributions These processes are used as the noise term in the SDE and RODE solvers Additionally the noise processes themselves can be simulated and solved using the DiffEq common interface including the Monte Carlo interface Installation To install DiffEqNoiseProcess.jl use the Julia package manager Using Noise Processes Passing a Noise Process to a Problem Type AbstractNoiseProcess es can be passed directly to the problem types to replace the standard Wiener process Brownian motion with your choice of noise To do this simply construct the noise and pass it to the  noise  keyword argument Basic Interface The  NoiseProcess  acts like a DiffEq solution For some noise process  W  you can get its  i th timepoint like  W[i  and the associated time  W.t[i  If the  NoiseProcess  has a bridging distribution defined it can be interpolated to arbitrary time points using  W(t  Note that every interpolated value is saved to the  NoiseProcess  so that way it can stay distributionally correct A plot recipe is provided which plots the timeseries Direct Simulation of the Noise Process Since the  NoiseProcess  types are distribution-exact and do not require the stochastic differential equation solvers many times one would like to directly simulate trajectories from these proecesses The  NoiseProcess  has a  NoiseProcessProblem  type for which  solve  works For example we can simulate a distributionally-exact Geometric Brownian Motion solution by solve  requires the  dt  is given the solution it returns is a  NoiseProcess  which has stepped through the timespan Because this follows the common interface all of the normal functionality works For example we can use the Monte Carlo functionality as follows simulates 100 Geometric Brownian Motions Direct Interface Most of the time a  NoiseProcess  is received from the solution of a stochastic or random differential equation in which case  sol.W  gives the  NoiseProcess  and it is already defined along some timeseries In other cases  NoiseProcess  types are directly simulated see below However  NoiseProcess  types can also be directly acted on The basic functionality is given by  calculate_step  to calculate a future time point and  accept_step  to accept the step If steps are rejected the Rejection Sampling with Memory algorithm is applied to keep the solution distributionally exact This kind of stepping is done via Contributing Please refer to the  SciML ColPrac Contributor's Guide on Collaborative Practices for Community Packages  for guidance on PRs issues and other matters relating to contributing to ModelingToolkit There are a few community forums the diffeq-bridged channel in the  Julia Slack JuliaDiffEq  on Gitter on the  Julia Discourse forums see also  SciML Community page"},{"doctype":"documentation","id":"references/ModelingToolkit.SDESystem","title":"SDESystem","text":"σ ρ β t x t y t z t D Differential t eqs D x σ y x D y x ρ z y D z x y β z noiseeqs x y z de eqs noiseeqs t x y z σ ρ β DocStringExtensions.TypeDefinition A system of stochastic differential equations Fields DocStringExtensions.TypeFields(false Example"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.sample_distribution","title":"sample_distribution","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.stencil","title":"stencil","text":""},{"doctype":"documentation","id":"references/LinearSolve.LUSolver","title":"LUSolver","text":""},{"doctype":"documentation","id":"references/Surrogates._construct_rbf_y_matrix","title":"_construct_rbf_y_matrix","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.reordered_matrix","title":"reordered_matrix","text":""},{"doctype":"documentation","id":"references/SciMLBase.NoInit","title":"NoInit","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/DiffEqSensitivity.second_order_sensitivities","title":"second_order_sensitivities","text":"OrdinaryDiffEq ForwardDiff Test lotka! du u p t du dx p u p u u du dy p u p u u p u0 prob lotka! u0 p loss sol sum sol v ones H loss prob Vern9 saveat abstol reltol H  second order sensitivities(loss,prob,alg,args                                sensealg=ForwardDiffOverAdjoint(InterpolatingAdjoint(autojacvec=ReverseDiffVJP                                kwargs Second order sensitivity analysis is used for the fast calculation of Hessian matrices Warning Adjoint sensitivity analysis functionality requires being able to solve   a differential equation defined by the parameter struct  p  Thus while   DifferentialEquations.jl can support any parameter struct type usage   with adjoint sensitivity analysis requires that  p  could be a valid   type for being the initial condition  u0  of an array This means that   many simple types such as  Tuple s and  NamedTuple s will work as   parameters in normal contexts but will fail during adjoint differentiation   To work around this issue for complicated cases like nested structs look   into defining  p  using  AbstractArray  libraries such as RecursiveArrayTools.jl    or ComponentArrays.jl so that  p  is an  AbstractArray  with a concrete element type Example second order sensitivity analysis calculation Arguments The arguments for this function match  adjoint_sensitivities  The only notable difference is  sensealg  which requires a second order sensitivity algorithm of which currently the only choice is  ForwardDiffOverAdjoint  which uses forward-over-reverse to mix a forward-mode sensitivity analysis with an adjoint sensitivity analysis for a faster computation than either double forward or double reverse  ForwardDiffOverAdjoint s positional argument just accepts a first order sensitivity algorithm"},{"doctype":"documentation","id":"references/ModelingToolkit.SystemStructures.algeqs","title":"algeqs","text":""},{"doctype":"documentation","id":"references/GlobalSensitivity.SobolResult","title":"SobolResult","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.NoiseApproximation","title":"NoiseApproximation","text":"source1 source2 Union Nothing nothing reset μ σ f u p t μ u g u p t σ u prob f g Inf integrator prob SRIW1 W integrator prob f g Inf noise W In many cases one would like to define a noise process directly by a stochastic differential equation which does not have an analytical solution Of course this will not be distributionally-exact and how well the properties match depends on how well the differential equation is integrated but in many cases this can be used as a good approximation when other methods are much more difficult A  NoiseApproximation  is defined by a  DEIntegrator  The constructor for a  NoiseApproximation  is The  DEIntegrator  should have a final time point of integration far enough such that it will not halt during the integration For ease of use you can use a final time point as  Inf  Note that the time points do not have to match the time points of the future integration since the interpolant of the SDE solution will be used Thus the limiting factor is error tolerance and not hitting specific points NoiseApproximation Example In this example we will show how to use the  NoiseApproximation  in order to build our own Geometric Brownian Motion from its stochastic differential equation definition In normal usage you should use the  GeometricBrownianMotionProcess  instead since that is more efficient and distributionally-exact First let's define the  SDEProblem  Here will use a timespan  0.0,Inf  so that way the noise can be used over an indefinite integral Now we build the noise process by building the integrator and sending that integrator to the  NoiseApproximation  constructor We can use this noise process like any other noise process For example we can now build a geometric Brownian motion whose noise process is colored noise that itself is a geometric Brownian motion The possibilities are endless"},{"doctype":"documentation","id":"references/ModelingToolkit.find_pivot_col","title":"find_pivot_col","text":""},{"doctype":"documentation","id":"references/SciMLBase.ODEProblem","title":"ODEProblem","text":"DiffEqProblemLibrary ODEProblemLibrary ODEProblemLibrary importodeproblems prob ODEProblemLibrary prob_ode_linear sol prob Defines an ordinary differential equation ODE problem Documentation Page https://diffeq.sciml.ai/stable/types/ode_types Mathematical Specification of an ODE Problem To define an ODE Problem you simply need to give the function  f  and the initial condition  u_0  which define an ODE M frac{du}{dt  f(u,p,t There are two different ways of specifying  f  f(du,u,p,t  in-place Memory-efficient when avoiding allocations Best option for most cases unless mutation is not allowed f(u,p,t  returning  du  Less memory-efficient way particularly suitable when mutation is not allowed e.g with certain automatic differentiation packages such as Zygote u₀  should be an AbstractArray or number whose geometry matches the desired geometry of  u  Note that we are not limited to numbers or vectors for  u₀  one is allowed to provide  u₀  as arbitrary matrices  higher dimension tensors as well For the mass matrix  M  see the documentation of  ODEFunction  Problem Type Constructors ODEProblem  can be constructed by first building an  ODEFunction  or by simply passing the ODE right-hand side to the constructor The constructors are ODEProblem(f::ODEFunction,u0,tspan,p=NullParameters();kwargs ODEProblem{isinplace}(f,u0,tspan,p=NullParameters();kwargs   Defines the ODE with the specified functions  isinplace  optionally sets whether the function is inplace or not This is determined automatically but not inferred Parameters are optional and if not given then a  NullParameters  singleton will be used which will throw nice errors if you try to index non-existent parameters Any extra keyword arguments are passed on to the solvers For example if you set a  callback  in the problem then that  callback  will be added in every solve call For specifying Jacobians and mass matrices see the  ODEFunction  documentation Fields f  The function in the ODE u0  The initial condition tspan  The timespan for the problem p  The parameters kwargs  The keyword arguments passed onto the solves Example Problems Example problems can be found in  DiffEqProblemLibrary.jl  To use a sample problem such as  prob_ode_linear  you can do something like Define an ODE problem from an  ODEFunction "},{"doctype":"documentation","id":"references/SciMLOperators.has_expmv","title":"has_expmv","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.SimpleWienerProcess!","title":"SimpleWienerProcess!","text":"t0 W0 Z0 nothing kwargs t0 W0 Z0 nothing kwargs The  SimpleWienerProcess  also known as Brownian motion or the noise in the Langevin equation is the stationary process with white noise increments and a distribution  N(0,dt  The constructor is Unlike WienerProcess this uses the SimpleNoiseProcess and thus does not support adaptivity but is slightly more lightweight"},{"doctype":"document","id":"ModelingToolkit/tutorials/optimization.md","title":"Modeling Optimization Problems","text":"GalacticOptim Optim x y a b loss a x b y x sys loss x y a b u0 x y p a b prob sys u0 p grad hess prob Newton Modeling Optimization Problems Needs more text but it's super cool and auto-parallelizes and sparsifies too Plus you can hierarchically nest systems to have it generate huge optimization problems"},{"doctype":"documentation","id":"references/ModelingToolkit.throw_missingvars","title":"throw_missingvars","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_op","title":"get_op","text":""},{"doctype":"documentation","id":"references/NeuralPDE.KolmogorovPDEProblem","title":"KolmogorovPDEProblem","text":"A standard Kolmogorov PDE Problem Arguments f   The drift function from Feynman-Kac representation of the PDE g   The noise function from Feynman-Kac representation of the PDE phi   The terminal condition for the PDE tspan  The timespan for the problem xspan  The xspan for the problem d  The dimensions of the input x noise_rate_prototype  A prototype type instance for the noise rates that is the output g"},{"doctype":"documentation","id":"references/Catalyst.Statement","title":"Statement","text":""},{"doctype":"documentation","id":"references/DiffEqFlux._trace_batched","title":"_trace_batched","text":""},{"doctype":"documentation","id":"references/SciMLBase.build_linear_solution","title":"build_linear_solution","text":""},{"doctype":"document","id":"SciMLBase/interfaces/SciMLFunctions.md","title":"[SciMLFunctions (Jacobians, Sparsity, Etc.)]( scimlfunctions)","text":"f iip f iip f LinearAlgebra f du u p t du t u jac J u p t J t J t J jp Diagonal zeros fun f jac jac jac_prototype jp SciMLFunctions Jacobians Sparsity Etc  scimlfunctions The SciML ecosystem provides an extensive interface for declaring extra functions associated with the differential equation's data In traditional libraries there is usually only one option the Jacobian However we allow for a large array of pre-computed functions to speed up the calculations This is offered via the  SciMLFunction  types which can be passed to the problems Definition of the AbstractSciMLFunction Interface The following standard principles should be adhered to across all  AbstractSciMLFunction  instantiations Common Function Choice Definitions The full interface available to the solvers is as follows jac  The Jacobian of the differential equation with respect to the state variable  u  at a time  t  with parameters  p  paramjac  The Jacobian of the differential equation with respect to  p  at state  u  at time  t  analytic  Defines an analytical solution using  u0  at time  t  with  p  which will cause the solvers to return errors Used for testing syms  Allows you to name your variables for automatic names in plots and other output jac_prototype  Defines the type to be used for any internal Jacobians within the solvers sparsity  Defines the sparsity pattern to be used for the sparse differentiation schemes By default this is equal to  jac_prototype  See the sparsity handling portion of this page for more information colorvec  The coloring pattern used by the sparse differentiator See the sparsity handling portion of this page for more information observed  A function which allows for generating other observables from a solution Each function type additionally has some specific arguments refer to their documentation for details In-place Specification and No-Recompile Mode Each SciMLFunction type can be called with an is inplace iip choice which is a boolean for whether the function is in the inplace form mutating to change the first value This is automatically determined using the methods table but note that for full type-inferrability of the  SciMLProblem  this iip-ness should be specified Additionally the functions are fully specialized to reduce the runtimes If one would instead like to not specialize on the functions to reduce compile time then one can set  recompile  to false This makes the ODE solver compilation independent of the function and so changing the function will not cause recompilation One can change the default value by changing the  const RECOMPILE_BY_DEFAULT  true  to false in the SciMLBase.jl source code Specifying Jacobian Types The  jac  field of an inplace style  SciMLFunction  has the signature  jac(J,u,p,t  which updates the jacobian  J  in-place The intended type for  J  can sometimes be inferred e.g when it is just a dense  Matrix  but not in general To supply the type information you can provide a  jac_prototype  in the function's constructor The following example creates an inplace  ODEFunction  whose jacobian is a  Diagonal  Note that the integrators will always make a deep copy of  fun.jac_prototype  so there's no worry of aliasing In general the jacobian prototype can be anything that has  mul  defined in particular sparse matrices or custom lazy types that support  mul  A special case is when the  jac_prototype  is a  AbstractDiffEqLinearOperator  in which case you do not need to supply  jac  as it is automatically set to  update_coefficients  Refer to the  DiffEqOperators  section for more information on setting up time/parameter dependent operators Sparsity Handling The solver libraries internally use packages such as  FiniteDiff.jl  and  SparseDiffTools.jl  for high performance calculation of sparse Jacobians and Hessians along with matrix-free calculations of Jacobian-Vector products J*v vector-Jacobian products v J   and Hessian-vector products H v The SciML interface gives users the ability to control these connections in order to allow for top notch performance The key arguments in the SciMLFunction is the  prototype  which is an object that will be used as the underlying Jacobian/Hessian Thus if one wants to use a sparse Jacobian one should specify  jac_prototype  to be a sparse matrix The sparsity pattern used in the differentiation scheme is defined by  sparsity  By default  sparsity=jac_prototype  meaning that the sparse automatic differentiation scheme should specialize on the sparsity pattern given by the actual sparsity pattern This can be overridden to say perform partial matrix coloring approximations Additionally the color vector for the sparse differentiation directions can be specified directly via  colorvec  For more information on how these arguments control the differentiation process see the aforementioned differentiation library documentations Traits AbstractSciMLFunction API Abstract SciML Functions Concrete SciML Functions"},{"doctype":"documentation","id":"references/ModelingToolkit.InvalidSystemException","title":"InvalidSystemException","text":""},{"doctype":"documentation","id":"references/SciMLBase.IntegralProblem","title":"IntegralProblem","text":"Defines an integral problem Documentation Page https://github.com/SciML/Integrals.jl Mathematical Specification of a Integral Problem Integral problems are multi-dimensional integrals defined as int_{lb}^{ub f(u,p du where  p  are parameters  u  is a  Number  or  AbstractArray  whose geometry matches the space being integrated Problem Type Constructors IntegralProblem(f,lb,ub,p=NullParameters                   nout=1 batch  0 kwargs f the integrand  dx=f(x,p  for out-of-place or  f(dx,x,p  for in-place lb Either a number or vector of lower bounds ub Either a number or vector of upper bounds p The parameters associated with the problem nout The output size of the function f Defaults to 1 i.e a scalar integral output batch The preferred number of points to batch This allows user-side parallelization  of the integrand If batch  0 then each x[:,i is a different point of the integral  to calculate and the output should be nout x batchsize Note that batch is a suggestion  for the number of points and it is not necessarily true that batch is the same as  batchsize in all algorithms kwargs Keyword arguments copied to the solvers Additionally we can supply iip like IntegralProblem as true or false to declare at  compile time whether the integrator function is in-place Fields The fields match the names of the constructor arguments"},{"doctype":"documentation","id":"references/Surrogates._approx_rbf","title":"_approx_rbf","text":""},{"doctype":"documentation","id":"references/Catalyst.conservationlaws","title":"conservationlaws","text":"Given the net stoichiometry matrix of a reaction system computes a matrix of conservation laws each represented as a row in the output Return the conservation law matrix of the given  ReactionSystem  calculating it if it is not already stored within the system or returning an alias to it Notes The first time being called it is calculated and cached in  rn  subsequent calls should be fast"},{"doctype":"documentation","id":"references/DiffEqSensitivity._second_order_sensitivity_product","title":"_second_order_sensitivity_product","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.invalidate_cache!","title":"invalidate_cache!","text":""},{"doctype":"document","id":"LinearSolve/basics/Preconditioners.md","title":"[Preconditioners]( prec)","text":"LinearAlgebra s rand n Pl Diagonal s A rand n n b rand n prob A b sol prob IterativeSolvers_GMRES Pl Pl Preconditioners  prec Many linear solvers can be accelerated by using what is known as a  preconditioner  an approximation to the matrix inverse action which is cheap to evaluate These can improve the numerical conditioning of the solver process and in turn improve the performance LinearSolve.jl provides an interface for the definition of preconditioners which works with the wrapped packages Using Preconditioners Mathematical Definition Preconditioners are specified in the keyword arguments of  init  or  solve  The right preconditioner  Pr  transforms the linear system  Au  b  into the form AP_r^{-1}(Pu  AP_r^{-1}y  b to add the solving step  P_r u  y  The left preconditioner  Pl  transforms the linear system into the form P_l^{-1}(Au  b  0 A two-sided preconditioned system is of the form P_l A P_r^{-1 P_r u  P_l b By default if no preconditioner is given the preconditioner is assumed to be the identity  I  Using Preconditioners In the following we will use the  DiagonalPreconditioner  to define a two-sided preconditioned system which first divides by some random numbers and then multiplies by the same values This is commonly used in the case where if instead of random  s  is an approximation to the eigenvalues of a system Preconditioner Interface To define a new preconditioner you define a Julia type which satisfies the following interface Base.eltype(::Preconditioner  Required only for Krylov.jl LinearAlgebra.ldiv!(::AbstractVector,::Preconditioner,::AbstractVector  and  LinearAlgebra.ldiv!(::Preconditioner,::AbstractVector Curated List of Pre-Defined Preconditioners The following preconditioners match the interface of LinearSolve.jl LinearSolve.ComposePreconditioner(prec1,prec2  composes the preconditioners to apply  prec1  before  prec2  LinearSolve.InvPreconditioner(prec  inverts  mul  and  ldiv  in a preconditioner definition as a lazy inverse LinearAlgera.Diagonal(s::Union{Number,AbstractVector  the lazy Diagonal matrix type of Base.LinearAlgebra Used for efficient construction of a diagonal preconditioner Other  Base.LinearAlgera  types all define the full Preconditioner interface IncompleteLU.ilu  an implementation of the incomplete LU-factorization preconditioner This requires  A  as a  SparseMatrixCSC  Preconditioners.CholeskyPreconditioner(A i  An incomplete Cholesky preconditioner with cut-off level  i  Requires  A  as a  AbstractMatrix  and positive semi-definite AlgebraicMultiGrid  Implementations of the algebraic multigrid method Must be converted to a preconditioner via  AlgebraicMultiGrid.aspreconditioner(AlgebraicMultiGrid.precmethod(A  Requires  A  as a  AbstractMatrix  Provides the following methods AlgebraicMultiGrid.ruge_stuben(A AlgebraicMultiGrid.smoothed_aggregation(A PyAMG  Implementations of the algebraic multigrid method Must be converted to a preconditioner via  PyAMG.aspreconditioner(PyAMG.precmethod(A  Requires  A  as a  AbstractMatrix  Provides the following methods PyAMG.RugeStubenSolver(A PyAMG.SmoothedAggregationSolver(A ILUZero.ILU0Precon(A::SparseMatrixCSC  b_type  T  An incomplete LU implementation Requires  A  as a  SparseMatrixCSC  LimitedLDLFactorizations.lldl  A limited-memory LDLᵀ factorization for symmetric matrices Requires  A  as a  SparseMatrixCSC  Applying  F  lldl(A F.D  abs.(F.D  before usage as a preconditioner makes the preconditioner symmetric postive definite and thus is required for Krylov methods which are specialized for symmetric linear systems RandomizedPreconditioners.NystromPreconditioner  A randomized sketching method for positive semidefinite matrices  A  Builds a preconditioner  P ≈ A  μ*I  for the system  A  μ*I)x  b"},{"doctype":"document","id":"Catalyst/api/catalyst_api.md","title":"Catalyst.jl API","text":"rn k X Y W k OrdinaryDiffEq StochasticDiffEq DiffEqJump β γ t S t I t R t rxs β S I I γ I R rs rxs t u₀map S I R parammap β γ tspan odesys convert rs oprob odesys u₀map tspan parammap sol oprob Tsit5 sdesys convert rs sprob sdesys u₀map tspan parammap sol sprob EM dt jumpsys convert rs u₀map S I R dprob jumpsys u₀map tspan parammap jprob JumpProblem jumpsys dprob Direct sol jprob SSAStepper Latexify latexify rn Catalyst.jl API Reaction Network Generation and Representation Catalyst provides the  reaction_network  macro for generating a complete network stored as a  ReactionSystem  which in turn is composed of  Reaction s  ReactionSystem s can be converted to other  ModelingToolkit.AbstractSystem s including a  ModelingToolkit.ODESystem   ModelingToolkit.SDESystem  or  ModelingToolkit.JumpSystem  An empty network can be generated using  reaction_network  with no arguments or one argument to name the system or the  make_empty_network  function These can then be extended programmatically using  addspecies   addparam  and  addreaction  It is important to note for  reaction_network  that any variable not declared to be a parameter after  end  will be treated as a chemical species of the system i.e in X   Y  and  W  will all be classified as chemical species The  ReactionSystem  generated by the  reaction_network  macro is a  ModelingToolkit.AbstractSystem  that symbolically represents a system of chemical reactions In some cases it can be convenient to bypass the macro and directly generate a collection of  Reaction s and a corresponding  ReactionSystem  encapsulating them Below we illustrate with a simple SIR example how a system can be directly constructed and demonstrate how to then generate from the  ReactionSystem  and solve corresponding chemical reaction ODE models chemical Langevin equation SDE models and stochastic chemical kinetics jump process models ModelingToolkit and Catalyst Accessor Functions A  ReactionSystem  is an instance of a  ModelingToolkit.AbstractTimeDependentSystem  and has a number of fields that can be accessed using the Catalyst API and the  ModelingToolkit.jl Abstract   System Interface  Below we overview these components There are three basic sets of convenience accessors that will return information either from a top-level system the top-level system and all sub-systems that are also  ReactionSystem s i.e the full reaction-network or the top-level system all subs-systems and all constraint systems i.e the full model To retrieve info from just a base  ReactionSystem   rn  ignoring sub-systems of  rn  one can use the ModelingToolkit accessors these provide direct access to the corresponding internal fields of the  ReactionSystem  get_states(rn  is a vector that collects all the species defined within  rn  get_ps(rn  is a vector that collects all the parameters defined  within  reactions in  rn  get_eqs(rn  is a vector that collects all the  Reaction s defined within  rn  get_iv(rn  is the independent variable used in the system usually  t  to represent time get_systems(rn  is a vector of all sub-systems of  rn  get_defaults(rn  is a dictionary of all the default values for parameters and species in  rn  These are complemented by the Catalyst accessor Catalyst.get_constraints(sys  is the constraint system of  rn  If none is defined will return  nothing  The preceding accessors do not allocate directly accessing internal fields of the  ReactionSystem  To retrieve information from the full reaction network represented by a system  rn  which corresponds to information within both  rn  and all sub-systems of type  ReactionSystem  one can call species(rn  is a vector collecting all the chemical species within the system and any sub-systems that are also  ReactionSystems  reactionparams(rn  is a vector of all the parameters within the system and any sub-systems that are also  ReactionSystem s These include all parameters that appear within some  Reaction  reactions(rn  is a vector of all the  Reaction s within the system and any sub-systems that are also  ReactionSystem s These accessors will allocate unless there are no subsystems In the latter case they are equivalent to the corresponding  get  functions Finally as some sub-systems may be other system types for example specifying algebraic constraints with a  NonlinearSystem  it can also be convenient to collect all state variables e.g species and algebraic variables and such The following ModelingToolkit functions provide this information ModelingToolkit.states(rn  returns all species  and variables  across the system  all sub-systems  and all constraint systems ModelingToolkit.parameters(rn  returns all parameters across the system  all sub-systems  and all constraint systems ModelingToolkit.equations(rn  returns all  Reaction s and all  Equations  defined across the system  all sub-systems  and all constraint systems states  and  parameters  should be assumed to always allocate while  equations  will allocate unless there are no subsystems or constraint systems In the latter case  equations  is equivalent to  get_eqs  Below we list the remainder of the Catalyst API accessor functions mentioned above Basic System Properties See  Programmatic Construction of Symbolic Reaction Systems  for examples and  ModelingToolkit and   Catalyst Accessor Functions  for more details on the basic accessor functions Basic Reaction Properties Functions to Extend or Modify a Network ReactionSystem s can be programmatically extended using  addspecies   addparam   addreaction   add_reactions  or composed using  ModelingToolkit.extend  and  ModelingToolkit.compose  Network Analysis and Representations Network Comparison Network Visualization Latexify  can be used to convert networks to LaTeX mhchem equations by If  Graphviz  is installed and commandline accessible it can be used to create and save network diagrams using  Graph  and  savegraph  Rate Laws As the underlying  ReactionSystem  is comprised of  ModelingToolkit  expressions one can directly access the generated rate laws and using  ModelingToolkit  tooling generate functions or Julia  Expr s from them Transformations Unit Validation Utility Functions"},{"doctype":"documentation","id":"references/SciMLBase.RootfindOpt","title":"RootfindOpt","text":""},{"doctype":"documentation","id":"references/SciMLBase.isconstant","title":"isconstant","text":""},{"doctype":"documentation","id":"references/Surrogates.adjust_step_size","title":"adjust_step_size","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.ReverseDiffGPUStateCompatibilityError","title":"ReverseDiffGPUStateCompatibilityError","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.sample_wedge","title":"sample_wedge","text":""},{"doctype":"documentation","id":"references/SciMLBase.RODEProblem","title":"RODEProblem","text":"Defines a random ordinary differential equation RODE problem Documentation Page https://diffeq.sciml.ai/stable/types/rode_types Mathematical Specification of a RODE Problem To define a RODE Problem you simply need to give the function  f  and the initial condition  u_0  which define an ODE frac{du}{dt  f(u,p,t,W(t where  W(t  is a random process  f  should be specified as  f(u,p,t,W  or in-place as  f(du,u,p,t,W  and  u₀  should be an AbstractArray or number whose geometry matches the desired geometry of  u  Note that we are not limited to numbers or vectors for  u₀  one is allowed to provide  u₀  as arbitrary matrices  higher dimension tensors as well Constructors RODEProblem(f::RODEFunction,u0,tspan,p=NullParameters();noise=WHITE_NOISE,rand_prototype=nothing,callback=nothing RODEProblem{isinplace}(f,u0,tspan,p=NullParameters();noise=WHITE_NOISE,rand_prototype=nothing,callback=nothing,mass_matrix=I   Defines the RODE with the specified functions The default noise is  WHITE_NOISE   isinplace  optionally sets whether the function is inplace or not This is determined automatically but not inferred Parameters are optional and if not given then a  NullParameters  singleton will be used which will throw nice errors if you try to index non-existent parameters Any extra keyword arguments are passed on to the solvers For example if you set a  callback  in the problem then that  callback  will be added in every solve call For specifying Jacobians and mass matrices see the DiffEqFunctions  performance_overloads page Fields f  The drift function in the SDE u0  The initial condition tspan  The timespan for the problem p  The optional parameters for the problem Defaults to  NullParameters  noise  The noise process applied to the noise upon generation Defaults to Gaussian white noise For information on defining different noise processes see the noise process documentation page  noise_process rand_prototype  A prototype type instance for the noise vector It defaults to  nothing  which means the problem should be interpreted as having a noise vector whose size matches  u0  kwargs  The keyword arguments passed onto the solves"},{"doctype":"documentation","id":"references/ModelingToolkit.states","title":"states","text":"DocStringExtensions.TypedMethodSignatures Get the set of states for the given system"},{"doctype":"document","id":"Surrogates/neural.md","title":"Neural network tutorial","text":"Neural network tutorial Note This surrogate requires the SurrogatesFlux module which can be added by inputting add SurrogatesFlux from the Julia command line It's possible to define a neural network as a surrogate using Flux This is useful because we can call optimization methods on it First of all we will define the  Schaffer  function we are going to build surrogate for Sampling Let's define our bounds this time we are working in two dimensions In particular we want our first dimension  x  to have bounds  0 8  and  0 8  for the second dimension We are taking 60 samples of the space using Sobol Sequences We then evaluate our function on all of the sampling points Building a surrogate You can specify your own model optimization function loss functions and epochs As always getting the model right is hardest thing Optimization We can now call an optimization function on the neural network"},{"doctype":"documentation","id":"references/LinearSolve.simplelu_solve!","title":"simplelu_solve!","text":""},{"doctype":"documentation","id":"references/LinearSolve.get_KrylovJL_solver","title":"get_KrylovJL_solver","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.has_ivs","title":"has_ivs","text":""},{"doctype":"documentation","id":"references/MethodOfLines.BoundaryHandler","title":"BoundaryHandler","text":"Creates a map of boundaries for each variable to be used later when discretizing the boundary condition equations and"},{"doctype":"documentation","id":"references/DiffEqOperators.dot_product","title":"dot_product","text":""},{"doctype":"document","id":"SciMLBase/interfaces/Solutions.md","title":"SciMLSolutions","text":"sol j sol t j sol i j sol i k j sol i SciMLSolutions Definition of the SciMLSolution Interface All  SciMLSolution  types are a subset of some  AbstractArray  Types with time series like  ODESolution  are subtypes of  RecursiveArrayTools.AbstractVectorOfArray  and  RecursiveArrayTools.AbstractDiffEqArray  where appropriate Types without a time series like  OptimizationSolution  are directly subsets of  AbstractArray  Array Interface Instead of working on the  Vector{uType  directly we can use the provided array interface to access the value at timestep  j  if the timeseries was saved and to access the value of  t  at timestep  j  For multi-dimensional systems this will address first by component and lastly by time and thus will be the  i th component at timestep  j  Hence  sol[j][i  sol[i j  This is done because Julia is column-major so the leading dimension should be contiguous in memory If the independent variables had shape for example was a matrix then  i  is the linear index We can also access solutions with shape gives the  i,k  component of the system at timestep  j  The colon operator is supported meaning that gives the timeseries for the  i th component Common Field Names u  the solution values t  the independent variable values matching the length of the solution if applicable resid  the residual of the solution if applicable original  the solution object from the original solver if it's a wrapper algorithm retcode  see the documentation section on return codes prob  the problem that was solved alg  the algorithm used to solve the problem Return Codes RetCodes  retcodes The solution types have a  retcode  field which returns a symbol signifying the error state of the solution The retcodes are as follows Default  The solver did not set retcodes Success  The integration completed without erroring or the steady state solver from  SteadyStateDiffEq  found the steady state Terminated  The integration is terminated with  terminate!(integrator  Note that this may occur by using  TerminateSteadyState  from the callback library  DiffEqCallbacks  MaxIters  The integration exited early because it reached its maximum number of iterations DtLessThanMin  The timestep method chose a stepsize which is smaller than the allowed minimum timestep and exited early Unstable  The solver detected that the solution was unstable and exited early InitialFailure  The DAE solver could not find consistent initial conditions ConvergenceFailure  The internal implicit solvers failed to converge Failure  General uncategorized failures or errors Traits SciMLSolution API Abstract SciML Solutions Concrete SciML Solutions"},{"doctype":"documentation","id":"references/PolyChaos.MultiOrthoPoly","title":"MultiOrthoPoly","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.isdifferenceeq","title":"isdifferenceeq","text":""},{"doctype":"document","id":"DiffEqOperators/operators/jacobian_vector_product.md","title":"Jacobian-Vector Product Operators","text":"T f u AbstractArray p nothing t Union Nothing Number nothing autodiff ishermitian opnorm Jacobian-Vector Product Operators The  JacVecOperator  is a linear operator  J*v  where  J  acts like  df/du  for some function  f(u,p,t  For in-place operations  mul!(w,J,v   f  is an in-place function  f(du,u,p,t "},{"doctype":"documentation","id":"references/ModelingToolkit.has_Wfact_t","title":"has_Wfact_t","text":""},{"doctype":"documentation","id":"references/PolyChaos.build_w_meixner_pollaczek","title":"build_w_meixner_pollaczek","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.has_𝑑vertex","title":"has_𝑑vertex","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.ODEAdjointProblem","title":"ODEAdjointProblem","text":""},{"doctype":"documentation","id":"references/SciMLBase.SecondOrderODEProblem","title":"SecondOrderODEProblem","text":"f du0 u0 tspan callback Defines a second order ordinary differential equation ODE problem Documentation Page https://diffeq.sciml.ai/stable/types/dynamical_types Mathematical Specification of a 2nd Order ODE Problem To define a 2nd Order ODE Problem you simply need to give the function  f  and the initial condition  u_0  which define an ODE u  f(u',u,p,t f  should be specified as  f(du,u,p,t  or in-place as  f(ddu,du,u,p,t  and  u₀  should be an AbstractArray or number whose geometry matches the desired geometry of  u  Note that we are not limited to numbers or vectors for  u₀  one is allowed to provide  u₀  as arbitrary matrices  higher dimension tensors as well From this form a dynamical ODE v  f(v,u,p,t \nu  v  is generated Constructors Defines the ODE with the specified functions Fields f  The function for the second derivative du0  The initial derivative u0  The initial condition tspan  The timespan for the problem callback  A callback to be applied to every solver which uses the problem Defaults to nothing"},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.nice_parameters","title":"nice_parameters","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.get_type","title":"get_type","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.generate_affect_function","title":"generate_affect_function","text":""},{"doctype":"documentation","id":"references/Catalyst.USE_GV_JLL","title":"USE_GV_JLL","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.construct_t1","title":"construct_t1","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.add_dims","title":"add_dims","text":""},{"doctype":"document","id":"PolyChaos/pce_tutorial.md","title":"[Common Random Variables]( CommonRandomVariables)","text":"Common Random Variables  CommonRandomVariables Polynomial chaos expansion PCE is a Hilbert space technique for random variables with finite variance Mathematically equivalent to Fourier series expansions for periodic signals PCE allows to characterize a random variable in terms of its PCE coefficients aka Fourier coefficients That is the PCE of a random variable  mathsf{x  is given by mathsf{x  sum_{i=0}^L x_i phi_i where  x_i  are the so-called PCE coefficients and  phi_i  are the orthogonal polynomials that are orthogonal relative to the probability density function of  mathsf{x  This tutorial walks you through the PCE of common random variables namely Gaussian  gaussian  Beta  beta01  Uniform uniform01  Logistic  logistic  and shows how they are implemented in  PolyChaos  Construction of Basis The orthogonal polynomials are constructed using the  OrthoPoly type here of degree at most  d  For canonical measures special constructors are implemented For example let's evaluate the Gaussian basis polynomials at some points Finding PCE Coefficients Having constructed the orthogonal bases the question remains how to find the PCE coefficients for the common random variables Every random variable can be characterized exactly by two PCE coefficients For a Gaussian random variable this is familiar the mean and the variance suffice to describe a Gaussian random variable entirely The same is true for any random variable of finite variance given the right basis The function  convert2affinePCE  provides the first two PCE coefficients hence the name affine for the common random variables Gaussian Given the Gaussian random variable  mathsf{x sim mathcal{N}(\\mu sigma^2  with  sigma  0  the affine PCE coefficients are Uniform Given the uniform random variable  mathsf{x sim mathcal{U}(a b  with finite support  a<b  the affine PCE coefficients are Instead if the expected value and standard deviation are known the affine PCE coefficients of the uniform random variable are Beta Given the Beta random variable  mathsf{x sim mathcal{B}(a b alpha beta  with finite support  a<b  and shape parameters  alpha beta  0  the affine PCE coefficients are Instead if the expected value and standard deviation are known the affine PCE coefficients of the uniform random variable are Logistic Given the logstic random variable  mathsf{x sim mathcal{L}(a_1,a_2  where  a_2>0  with the probability density function rho(t  frac{1}{4 a_2  operatorname{sech}^2 left(\\frac{t-a_1}{2a_2}\\right the affine PCE coefficients of the uniform random variable are Moments It is a key feature of PCE to compute moments from the PCE coefficients alone no sampling is required Gaussian Uniform Beta Logistic Sampling Having found the PCE coefficients it may be useful to sample the random variables That means find  N  realizations of the random variable that obey the random variable's probability density function This is done in two steps Draw  N  samples from the measure  sampleMeasure  and then Evaluate the basis polynomials and multiply times the PCE coefficients i.e  sum_{i=0}^L x_i phi_i(\\xi_j  where  xi_j  is the  j th sample from the measure  evaluatePCE  Both steps are combined in the function  samplePCE  Gaussian Uniform Beta Logistic"},{"doctype":"documentation","id":"references/ModelingToolkit.collect_defaults!","title":"collect_defaults!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.has_structure","title":"has_structure","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.BoundaryPaddedArray","title":"BoundaryPaddedArray","text":"Higher-dimensional generalization of BoundaryPaddedVector pads an array of dimension N along the dimension D with 2 Arrays of dimension N-1 stored in lower and upper"},{"doctype":"document","id":"NeuralPDE/pinn/integro_diff.md","title":"Integro Differential Equations","text":"t i Ii Symbolics Integral t DomainSets ClosedInterval t Ix Integral x y DomainSets UnitSquare Ix Integral x y DomainSets ProductDomain ClosedInterval ClosedInterval x Flux GalacticOptimJL DomainSets Interval infimum supremum t i Di Differential t Ii Integral t DomainSets ClosedInterval t eq Di i t i t Ii i t bcs i domains t Interval Dense Flux σ Dense initθ Float64 strategy_ discretization strategy_ init_params nothing nothing derivative nothing pde_system eq bcs domains t i t prob pde_system discretization callback p l println l res prob BFGS callback callback maxiters ts infimum d domain supremum d domain d domains discretization u_predict first t res minimizer t ts analytic_sol_func t exp t sin t u_real analytic_sol_func t t ts Plots plot ts u_real label plot! ts u_predict label Integro Differential Equations The integral of function u(x int_{0}^{t}u(x)dx where x is variable of integral and t is variable of integro differential equation is defined as In multidimensional case The UnitSquare domain ranges both x and y from 0 to 1 Similarly a rectangular or cuboidal domain can be defined using  ProductDomain  of ClosedIntervals 1-dimensional example Lets take an example of an integro differential equation frac{∂}{∂t u(t   2u(t  5 int_{0}^{t}u(x)dx  1  text{for  t geq 0 and boundary condition u(0  0 Plotting the final solution and analytical solution IDE"},{"doctype":"documentation","id":"references/SciMLBase.LeftRootFind","title":"LeftRootFind","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.CompleteUpwindDifference","title":"CompleteUpwindDifference","text":"A helper function to compute the coefficients of a derivative operator including the boundary coefficients in the upwind scheme"},{"doctype":"documentation","id":"references/DiffEqFlux.FastLayer","title":"FastLayer","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.WHITE_NOISE_DIST","title":"WHITE_NOISE_DIST","text":""},{"doctype":"documentation","id":"references/NeuralPDE.get_vars","title":"get_vars","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations","title":"StructuralTransformations","text":""},{"doctype":"document","id":"NonlinearSolve/basics/NonlinearProblem.md","title":"Nonlinear Problems","text":"Nonlinear Problems"},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timeseries_steps_weighted_meancov","title":"timeseries_steps_weighted_meancov","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.fourier","title":"fourier","text":""},{"doctype":"documentation","id":"references/SciMLBase.QuadratureSolution","title":"QuadratureSolution","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractTimeseriesSolution","title":"AbstractTimeseriesSolution","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/RecursiveArrayTools.tuples","title":"tuples","text":""},{"doctype":"documentation","id":"references/PolyChaos.Tensor","title":"Tensor","text":""},{"doctype":"document","id":"ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction.md","title":"Automated Index Reduction of DAEs","text":"Automated Index Reduction of DAEs In many cases one may accidentally write down a DAE that is not easily solvable by numerical methods In this tutorial we will walk through an example of a pendulum which accidentally generates an index-3 DAE and show how to use the  modelingtoolkitize  to correct the model definition before solving Copy-Pastable Example Explanation Attempting to Solve the Equation In this tutorial we will look at the pendulum system begin{aligned}\n    x^\\prime  v_x\\\\\n    v_x^\\prime  Tx\\\\\n    y^\\prime  v_y\\\\\n    v_y^\\prime  Ty  g\\\\\n    0  x^2  y^2  L^2\n\\end{aligned As a good DifferentialEquations.jl user one would follow  the mass matrix DAE tutorial  to arrive at code for simulating the model However one will quickly be greeted with the unfortunate message Did you implement the DAE incorrectly No Is the solver broken No Understanding DAE Index It turns out that this is a property of the DAE that we are attempting to solve This kind of DAE is known as an index-3 DAE For a complete discussion of DAE index see  this article  Essentially the issue here is that we have 4 differential variables  x   v_x   y   v_y  and one algebraic variable  T  which we can know because there is no  D(T  term in the equations An index-1 DAE always satisfies that the Jacobian of the algebraic equations is non-singular Here the first 4 equations are differential equations with the last term the algebraic relationship However the partial derivative of  x^2  y^2  L^2  w.r.t  T  is zero and thus the Jacobian of the algebraic equations is the zero matrix and thus it's singular This is a very quick way to see whether the DAE is index 1 The problem with higher order DAEs is that the matrices used in Newton solves are singular or close to singular when applied to such problems Because of this fact the nonlinear solvers or Rosenbrock methods break down making them difficult to solve The classic paper  DAEs are not ODEs  goes into detail on this and shows that many methods are no longer convergent when index is higher than one So it's not necessarily the fault of the solver or the implementation this is known But that's not a satisfying answer so what do you do about it Transforming Higher Order DAEs to Index-1 DAEs It turns out that higher order DAEs can be transformed into lower order DAEs  If you differentiate the last equation two times and perform a substitution   you can arrive at the following set of equations  begin{aligned}\nx^\\prime  v_x \nv_x^\\prime  x T \ny^\\prime  v_y \nv_y^\\prime  y T  g \n0  2 left(v_x^{2  v_y^{2  y  y T  g   T x^2 right)\n\\end{aligned Note that this is mathematically-equivalent to the equation that we had before but the Jacobian w.r.t  T  of the algebraic equation is no longer zero because of the substitution This means that if you wrote down this version of the model it will be index-1 and solve correctly In fact this is how DAE index is commonly defined the number of differentiations it takes to transform the DAE into an ODE where an ODE is an index-0 DAE by substituting out all of the algebraic relationships Automating the Index Reduction However requiring the user to sit there and work through this process on potentially millions of equations is an unfathomable mental overhead But we can avoid this by using methods like  the Pantelides algorithm  for automatically performing this reduction to index 1 While this requires the ModelingToolkit symbolic form we use  modelingtoolkitize  to transform the numerical code into symbolic code run  dae_index_lowering  lowering then transform back to numerical code with  ODEProblem  and solve with a numerical solver Let's try that out Note that plotting using  states(traced_sys  is done so that any variables which are symbolically eliminated or any variable reorderings done for enhanced parallelism/performance still show up in the resulting plot and the plot is shown in the same order as the original numerical code Note that we can even go a little bit further If we use the  ODAEProblem  constructor we can remove the algebraic equations from the states of the system and fully transform the index-3 DAE into an index-0 ODE which can be solved via an explicit Runge-Kutta method And there you go this has transformed the model from being too hard to solve with implicit DAE solvers to something that is easily solved with explicit Runge-Kutta methods for non-stiff equations"},{"doctype":"documentation","id":"references/DiffEqOperators.convolve_BC_left!","title":"convolve_BC_left!","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.DerivativeOperator","title":"DerivativeOperator","text":"Represent a finite-difference derivative operator These operators implement the  DiffEqBase.AbstractDiffEqLinearOperator  interface Therefore  eltype  returns  T  These operators can be contracted over an arbitrary dimension given by the type parameter  N  The  CentredDifference  and  UpwindDifference  types serve as hooks to hang constructors but no objects are actually constructed with those types Their constructors return structures of type  DerivativeOperator  with stencils constructed by  calculate_weights  and other fields filled out appropriately The finite-difference methods are defined for  DerivativeOperator  In particular    which actually takes derivatives The key data are three stencils of coefficients stored in  SVector s The interior stencil stored in  stencil_coefs::S1  is the normal one used in the interior of the grid The others  low_boundary_coefs::S2  and  high_boundary_coefs::S2  are used where the normal stencil would jut out of the grid boundary These can have a different length than the interior stencil hence the two types When the operator is applied by  mul  these stencils are multiplied by the vector in the  coefficients  field It is set to  coeff_func.(1:len  in  UpwindDifference  which seems odd because  len  is not the stencil length Scalar multiplication is absorbed into this vector as required by the  DiffEqBase.AbstractDiffEqLinearOperator  interface The  coefficients  field appears to be a more general  DifferentialEquations  thing There is an  update_coefficients  method The term “left boundary” is used interchangeably with “low boundary” and “right” with “high” Type Parameters Presumably most of these are to force method specialization on:a the stencil lengths which are given by the parameters of the  SVector  types  S1  and  S2  and b even or arbitrary grid spacing T<:Real  Function range type N  Contraction dimension Wind  Flag  true  for upwind operators  false  for centered T2  Concrete type of  dx  field which might be  T  or  AbstractVector{T  S1  Concrete type of the interior stencil When is this not an  SVector  S2<:SVector  Concrete type of boundary coefficients T3   Vector{T  if a  coeff_func  was supplied  Nothing  otherwise F   typeof(coeff_func  if a  coeff_func  was supplied  Nothing  otherwise Fields derivative_order::Int  The  n  in  f⁽ⁿ⁾(x  approximation_order::Int  The degree of polynomial for which the operator is numerically exact dx::T2  Grid step len::Int  The number of interior points on the grid stencil_length::Int  Length of the interior stencil stencil_coefs::S1  An  SVector  of the interior stencil coefficients boundary_stencil_length::Int  boundary_point_count::Int  low_boundary_coefs::S2  An array of  SVector  stencils for the interior points where  stencil_coefs  juts over the left boundary high_boundary_coefs::S2  An array of  SVector  stencils for the interior points where  stencil_coefs  juts over the right boundary coefficients::T3  Multiplier for the stencil coeff_func::F  God only knows what this function is for See also  AbstractBC   CenteredDifference   UpwindDifference"},{"doctype":"documentation","id":"references/SciMLBase.__has_indepsym","title":"__has_indepsym","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.Stream","title":"Stream","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.generate_connection_set","title":"generate_connection_set","text":""},{"doctype":"document","id":"DiffEqSensitivity/Benchmark.md","title":"Benchmarks","text":"OrdinaryDiffEq Flux Optim Plots Zygote BenchmarkTools Random u0 Float32 datasize tspan tsteps range tspan tspan length datasize trueODEfunc du u p t true_A du u true_A prob_trueode trueODEfunc u0 tspan ode_data Array prob_trueode Tsit5 saveat tsteps dudt2 x p x tanh Random seed! p dudt2 prob_neuralode dudt2 tspan Tsit5 saveat tsteps loss_neuralode p pred Array prob_neuralode u0 p loss sum abs2 ode_data pred loss Zygote gradient loss_neuralode p prob_neuralode_interpolating dudt2 tspan Tsit5 saveat tsteps sensealg autojacvec loss_neuralode_interpolating p pred Array prob_neuralode_interpolating u0 p loss sum abs2 ode_data pred loss Zygote gradient loss_neuralode_interpolating p prob_neuralode_interpolating_zygote dudt2 tspan Tsit5 saveat tsteps sensealg autojacvec loss_neuralode_interpolating_zygote p pred Array prob_neuralode_interpolating_zygote u0 p loss sum abs2 ode_data pred loss Zygote gradient loss_neuralode_interpolating_zygote p prob_neuralode_backsolve dudt2 tspan Tsit5 saveat tsteps sensealg autojacvec loss_neuralode_backsolve p pred Array prob_neuralode_backsolve u0 p loss sum abs2 ode_data pred loss Zygote gradient loss_neuralode_backsolve p prob_neuralode_quad dudt2 tspan Tsit5 saveat tsteps sensealg autojacvec loss_neuralode_quad p pred Array prob_neuralode_quad u0 p loss sum abs2 ode_data pred loss Zygote gradient loss_neuralode_quad p prob_neuralode_backsolve_tracker dudt2 tspan Tsit5 saveat tsteps sensealg autojacvec loss_neuralode_backsolve_tracker p pred Array prob_neuralode_backsolve_tracker u0 p loss sum abs2 ode_data pred loss Zygote gradient loss_neuralode_backsolve_tracker p prob_neuralode_backsolve_zygote dudt2 tspan Tsit5 saveat tsteps sensealg autojacvec loss_neuralode_backsolve_zygote p pred Array prob_neuralode_backsolve_zygote u0 p loss sum abs2 ode_data pred loss Zygote gradient loss_neuralode_backsolve_zygote p prob_neuralode_backsolve_false dudt2 tspan Tsit5 saveat tsteps sensealg autojacvec loss_neuralode_backsolve_false p pred Array prob_neuralode_backsolve_false u0 p loss sum abs2 ode_data pred loss Zygote gradient loss_neuralode_backsolve_false p prob_neuralode_tracker dudt2 tspan Tsit5 saveat tsteps sensealg loss_neuralode_tracker p pred Array prob_neuralode_tracker u0 p loss sum abs2 ode_data pred loss Zygote gradient loss_neuralode_tracker p Benchmarks Vs Torchdiffeq 1 million and less ODEs A raw ODE solver benchmark showcases  30x performance advantage for DifferentialEquations.jl  for ODEs ranging in size from 3 to nearly 1 million Vs Torchdiffeq on neural ODE training A training benchmark using the spiral ODE from the original neural ODE paper  demonstrates a 100x performance advantage for DiffEqFlux in training neural ODEs  Vs torchsde on small SDEs Using the code from torchsde's README we demonstrated a  70,000x performance   advantage over torchsde  Further benchmarking is planned but was found to be computationally infeasible for the time being A bunch of adjoint choices on neural ODEs Quick summary BacksolveAdjoint  can be the fastest but use with caution about 25 faster Using  ZygoteVJP  is faster than other vjp choices with FastDense due to the overloads"},{"doctype":"documentation","id":"references/Catalyst.@reaction_network","title":"@reaction_network","text":"sir_model SIR c1 s i i c2 i r c1 c2 sir_model c1 s i i c2 i r c1 c2 emptyrn empty emptyrn Generates a  ReactionSystem  that encodes a chemical reaction network See  The Reaction DSL  documentation for details on parameters to the macro Examples"},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.is_number","title":"is_number","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.param2Wb","title":"param2Wb","text":""},{"doctype":"documentation","id":"references/Catalyst.linkageclasses","title":"linkageclasses","text":"sir SIR β S I I ν I R β ν complexes sir sir Given the incidence graph of a reaction network return a vector of the connected components of the graph i.e sub-groups of reaction complexes that are connected in the incidence graph Notes Requires the  incidencemat  to already be cached in  rn  by a previous call to  reactioncomplexes  For example gives"},{"doctype":"documentation","id":"references/DiffEqSensitivity.EnzymeVJP","title":"EnzymeVJP","text":"compile EnzymeVJP  VJPChoice Uses Enzyme.jl to compute vector-Jacobian products Is the fastest VJP whenever applicable though Enzyme.jl currently has low coverage over the Julia programming language for example restricting the user's defined  f  function to not do things like require garbage collection or calls to BLAS/LAPACK However mutation is supported meaning that in-place  f  with fully mutating non-allocating code will work with Enzyme provided no high level calls to C like BLAS/LAPACK are used and this will be the most efficient adjoint implementation Constructor"},{"doctype":"documentation","id":"references/SciMLBase.rerun_warn","title":"rerun_warn","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.integrate_sum","title":"integrate_sum","text":""},{"doctype":"documentation","id":"references/Catalyst.netstoich_stoichtype","title":"netstoich_stoichtype","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractSDDEAlgorithm","title":"AbstractSDDEAlgorithm","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/DiffEqOperators.Dirichlet0BC","title":"Dirichlet0BC","text":"l  and  r  are the BC coefficients i.e  αl βl γl  and  αl βl γl  tuples and vectors work and correspond to BCs of the form αl  u  βl  u  γl   αr  u  βr  u  γr imposed on the lower  l  and higher  r  index boundaries respectively RobinBC  implements a Robin boundary condition operator  Q  that acts on a vector to give an extended vector as a result see https://github.com/JuliaDiffEq/DiffEqOperators.jl/files/3267835/ghost_node.pdf Write vector b̄₁ as a vertical concatenation with b0 and the rest of the elements of b̄₁ denoted b̄ ₁ the same with ū into u0 and ū  b̄ ₁  b̄ 2  fill(β/Δx length(stencil)-1 Pull out the product of u0 and b0 from the dot product The stencil used to approximate u is denoted s b0  α+(β/Δx)*s[1 Rearrange terms to find a general formula for u0 b̄ ₁̇⋅ū b0  γ/b0 which is dependent on ū  the robin coefficients and Δx The non-identity part of Qa is qa b`₁/b0  β s[2:end]/(α+β s[1]/Δx The constant part is Qb  γ/(α+β*s[1]/Δx do the same at the other boundary amounts to a flip of s[2:end with the other set of boundary coefficients"},{"doctype":"document","id":"QuasiMonteCarlo/index.md","title":"QuasiMonteCarlo.jl: Quasi-Monte Carlo (QMC) Samples Made Easy","text":"Pkg Pkg add Distributions lb ub n d s n lb ub s n lb ub s n lb ub s n lb ub s n lb ub s n lb ub UnsafeArrays s i QuasiMonteCarlo.jl Quasi-Monte Carlo QMC Samples Made Easy QuasiMonteCarlo.jl is a lightweight package for generating Quasi-Monte Carlo QMC samples using various different methods Installation To install QuasiMonteCarlo.jl use the Julia package manager Example The output  s  is a matrix so one can use things like  uview  from  UnsafeArrays.jl  for a stack-allocated view of the  i th point Adding a new sampling method Adding a new sampling method is a two-step process Add a new SamplingAlgorithm type Overload the sample function with the new type All sampling methods are expected to return a matrix with dimension  d  by  n  where  d  is the dimension of the sample space and  n  is the number of samples Example Contributing Please refer to the  SciML ColPrac Contributor's Guide on Collaborative Practices for Community Packages  for guidance on PRs issues and other matters relating to contributing to SciML There are a few community forums the diffeq-bridged channel in the  Julia Slack JuliaDiffEq  on Gitter on the  Julia Discourse forums see also  SciML Community page"},{"doctype":"documentation","id":"references/SciMLBase.check_error!","title":"check_error!","text":"Same as  check_error  but also set solution's return code  integrator.sol.retcode  and run  postamble "},{"doctype":"documentation","id":"references/DiffEqSensitivity.ParamNonDiagNoiseJacobianWrapper","title":"ParamNonDiagNoiseJacobianWrapper","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.promote_to_concrete","title":"promote_to_concrete","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.nullspace","title":"nullspace","text":""},{"doctype":"document","id":"Optimization/API/optimization_function.md","title":"[OptimizationFunction]( optfunction)","text":"OptimizationFunction  optfunction Automatic Differentiation Construction Choice Recommendations The choices for the auto-AD fill-ins with quick descriptions are AutoForwardDiff  The fastest choice for small optimizations AutoReverseDiff(compile=false  A fast choice for large scalar optimizations AutoTracker  Like ReverseDiff but GPU-compatible AutoZygote  The fastest choice for non-mutating array-based BLAS functions AutoFiniteDiff  Finite differencing not optimal but always applicable AutoModelingToolkit  The fastest choice for large scalar optimizations Automatic Differentiation Choice API The following sections describe the Auto-AD choices in detail"},{"doctype":"documentation","id":"references/ModelingToolkit.ishistory","title":"ishistory","text":""},{"doctype":"documentation","id":"references/NeuralPDE.GridTraining","title":"GridTraining","text":"dx  the discretization of the grid"},{"doctype":"documentation","id":"references/MethodOfLines.x2i","title":"x2i","text":""},{"doctype":"document","id":"GlobalSensitivity/methods/rbdfast.md","title":"Random Balance Design FAST Method","text":"num_harmonics Int linear_batch X A B A X B X linear X A B A X B X lb ones π ub ones π rng StableRNG res1 linear num_params N res2 linear_batch num_params batch N Random Balance Design FAST Method RBDFAST  has the following keyword arguments num_harmonics  Number of harmonics to consider during power spectral density analysis Method Details In the Random Balance Designs RBD method similar to  eFAST    N  points are selected over a curve in the input space A fixed frequency equal to  1  is used for each factor Then independent random permutations are applied to the coordinates of the N points in order to generate the design points The input model for analysis is evaluated at each design point and the outputs are reordered such that the design points are in increasing order with respect to factor  Xi  The Fourier spectrum is calculated on the model output at the frequency 1 and at its higher harmonics 2 3 4 5 6 and yields the estimate of the sensitivity index of factor  Xi  API Example"},{"doctype":"document","id":"DiffEqSensitivity/ad_examples/direct_sensitivity.md","title":"[Direct Sensitivity Analysis Functionality]( direct_sensitivity)","text":"f du u p t du dx p u p u u du dy p u u u p prob f p sol prob DP8 x dp sol x dp sol i x dp sol t x dp sol da dp plot sol t da lw f du u p t du dx p u p u u du dy p u u u p prob f p sol prob Vern9 abstol reltol dg out u p t i out u ts res sol Vern9 dg ts abstol reltol ForwardDiff Calculus Tracker G p tmp_prob prob u0 convert eltype p prob u0 p p sol tmp_prob Vern9 abstol reltol saveat ts sensealg SensitivityADPassThrough A convert Array sol sum A G res2 ForwardDiff gradient G res3 Calculus gradient G res4 Tracker gradient G res5 ReverseDiff gradient G Direct Sensitivity Analysis Functionality  direct_sensitivity While sensitivity analysis tooling can be used implicitly via integration with automatic differentiation libraries one can often times obtain more speed and flexibility with the direct sensitivity analysis interfaces This tutorial demonstrates some of those functions Example using an ODEForwardSensitivityProblem Forward sensitivity analysis is performed by defining and solving an augmented ODE To define this augmented ODE use the  ODEForwardSensitivityProblem  type instead of an ODE type For example we generate an ODE with the sensitivity equations attached for the Lotka-Volterra equations by This generates a problem which the ODE solvers can solve Note that the solution is the standard ODE system and the sensitivity system combined We can use the following helper functions to extract the sensitivity information In each case  x  is the ODE values and  dp  is the matrix of sensitivities The first gives the full timeseries of values and  dp[i  contains the time series of the sensitivities of all components of the ODE with respect to  i th parameter The second returns the  i th time step while the third interpolates to calculate the sensitivities at time  t  For example if we do then  da  is the timeseries for  frac{\\partial u(t)}{\\partial p  We can plot this transposing so that the rows the timeseries is plotted Local Sensitivity Solution For more information on the internal representation of the  ODEForwardSensitivityProblem  solution see the direct forward sensitivity analysis manual page  forward_sense Example using  adjoint_sensitivities  for discrete adjoints In this example we will show solving for the adjoint sensitivities of a discrete cost functional First let's solve the ODE and get a high quality continuous solution Now let's calculate the sensitivity of the  ell_2  error against 1 at evenly spaced points in time that is L(u,p,t)=\\sum_{i=1}^{n}\\frac{\\Vert1-u(t_{i},p)\\Vert^{2}}{2 for  t_i  0.5i  This is the assumption that the data is  data[i]=1.0  For this function notice we have that begin{aligned}\ndg_{1}&=1-u_{1 \ndg_{2}&=1-u_{2 \n quad vdots\n\\end{aligned and thus Also we can omit  dgdp  because the cost function doesn't dependent on  p  If we had data we'd just replace  1.0  with  data[i  To get the adjoint sensitivities call This is super high accuracy As always there's a tradeoff between accuracy and computation time We can check this almost exactly matches the autodifferentiation and numerical differentiation results and see this gives the same values"},{"doctype":"document","id":"DiffEqSensitivity/hybrid_jump_fitting/hybrid_diffeq.md","title":"Training Neural Networks in Hybrid Differential Equations","text":"DifferentialEquations Plots u0 Float32 datasize tspan dosetimes affect! integrator integrator u integrator u cb_ PresetTimeCallback dosetimes affect! save_positions trueODEfunc du u p t du u t range tspan tspan length datasize prob trueODEfunc u0 tspan ode_data Array prob Tsit5 callback cb_ saveat t dudt2 Dense tanh Dense p re Flux destructure dudt2 dudt du u p t du u du end re p u z0 Float32 u0 u0 prob dudt z0 tspan affect! integrator integrator u integrator u end cb PresetTimeCallback dosetimes affect! save_positions predict_n_ode _prob prob p p Array _prob Tsit5 u0 z0 p p callback cb saveat t sensealg loss_n_ode pred predict_n_ode loss sum abs2 ode_data pred loss loss_n_ode cba doplot pred predict_n_ode display sum abs2 ode_data pred pl scatter t ode_data label scatter! pl t pred label display plot pl cba ps Flux p data Iterators repeated Flux train! loss_n_ode ps data ADAM cb cba Training Neural Networks in Hybrid Differential Equations Hybrid differential equations are differential equations with implicit or explicit discontinuities as specified by  callbacks  In the following example explicit dosing times are given for a pharmacometric model and the universal differential equation is trained to uncover the missing dynamical equations Hybrid Universal Differential Equation Note on Sensitivity Methods The continuous adjoint sensitivities  BacksolveAdjoint   InterpolatingAdjoint  and  QuadratureAdjoint  are compatible with events for ODEs  BacksolveAdjoint  and  InterpolatingAdjoint  can also handle events for SDEs Use  BacksolveAdjoint  if the event terminates the time evolution and several states are saved Currently the continuous adjoint sensitivities do not support multiple events per time point All methods based on discrete sensitivity analysis via automatic differentiation like  ReverseDiffAdjoint   TrackerAdjoint  or  ForwardDiffSensitivity  are the methods to use and  ReverseDiffAdjoint  is demonstrated above are compatible with events This applies to SDEs DAEs and DDEs as well"},{"doctype":"documentation","id":"references/ModelingToolkit.get_systems","title":"get_systems","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.apply_d_rules","title":"apply_d_rules","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.isisomorphic","title":"isisomorphic","text":""},{"doctype":"documentation","id":"references/SciMLBase.u_cache","title":"u_cache","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.build_jac_config","title":"build_jac_config","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.∇tmap","title":"∇tmap","text":""},{"doctype":"documentation","id":"references/QuasiMonteCarlo.GridSample","title":"GridSample","text":"T The grid is given by  lb:dx[i]:ub  in the ith direction"},{"doctype":"documentation","id":"references/ModelingToolkit.ode_order_lowering","title":"ode_order_lowering","text":"DocStringExtensions.TypedMethodSignatures Takes a Nth order ODESystem and returns a new ODESystem written in first order form by defining new variables which represent the N-1 derivatives"},{"doctype":"documentation","id":"references/NeuralOperators.transform","title":"transform","text":""},{"doctype":"documentation","id":"references/SciMLBase.set_t!","title":"set_t!","text":"Set current time point of the  integrator  to  t "},{"doctype":"documentation","id":"references/MethodOfLines.UpperBoundary","title":"UpperBoundary","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.LSSSensitivityFunction","title":"LSSSensitivityFunction","text":""},{"doctype":"documentation","id":"references/LabelledArrays.@SLVector","title":"@SLVector","text":"ABC a b c x ABC x a x b x c x The macro creates a labelled static vector with element type  ElType  and names from  Names  If no eltype is given then the eltype is determined from the values in the constructor The array size is found from the input data For example"},{"doctype":"documentation","id":"references/Catalyst.incidencemat","title":"incidencemat","text":"Calculate the incidence matrix of  rn  see  reactioncomplexes  Notes Is cached in  rn  so that future calls assuming the same sparsity will also be fast"},{"doctype":"document","id":"ExponentialUtilities/matrix_exponentials.md","title":"Matrix Exponentials","text":"Matrix Exponentials Methods Utilities"},{"doctype":"documentation","id":"references/DiffEqSensitivity.isnoisemixing","title":"isnoisemixing","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.connection2set!","title":"connection2set!","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractSecondOrderODEAlgorithm","title":"AbstractSecondOrderODEAlgorithm","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/DiffEqSensitivity.perp!","title":"perp!","text":""},{"doctype":"documentation","id":"references/SciMLBase.has_indepsym","title":"has_indepsym","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.MultiDimensionalBC","title":"MultiDimensionalBC","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.inform","title":"inform","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.derivative!","title":"derivative!","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.factor_rational","title":"factor_rational","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractRODEIntegrator","title":"AbstractRODEIntegrator","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.AbstractTail","title":"AbstractTail","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.ifgpufree","title":"ifgpufree","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.VecJacOperator","title":"VecJacOperator","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.collect_var_to_name!","title":"collect_var_to_name!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.OptimizationSystem","title":"OptimizationSystem","text":"x y z σ ρ β op σ y x x ρ z y x y β z os op x y z σ ρ β DocStringExtensions.TypeDefinition A scalar equation for optimization Fields DocStringExtensions.TypeFields(false Examples"},{"doctype":"documentation","id":"references/NeuralPDE.transform_inf_expr","title":"transform_inf_expr","text":""},{"doctype":"document","id":"ModelingToolkit/systems/ControlSystem.md","title":"ControlSystem","text":"ControlSystem System Constructors Composition and Accessor Functions get_eqs(sys  or  equations(sys  The equations that define the system get_states(sys  or  states(sys  The set of states in the system get_ps(sys  or  parameters(sys  The parameters of the system get_controls(sys  or  controls(sys  The control variables of the system Transformations Analyses"},{"doctype":"documentation","id":"references/ModelingToolkit.unbound_outputs","title":"unbound_outputs","text":"Return outputs that are not bound within the system i.e external outputs See also  bound_inputs   unbound_inputs   bound_outputs   unbound_outputs"},{"doctype":"documentation","id":"references/Catalyst.substoichmat","title":"substoichmat","text":"Returns the substrate stoichiometry matrix  S  with  S_{i j  the stoichiometric coefficient of the ith substrate within the jth reaction Note Set sparse=true for a sparse matrix representation"},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timestep_meancov","title":"timestep_meancov","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.TensorLayer","title":"TensorLayer","text":"model out p nothing Constructs the Tensor Product Layer which takes as input an array of n tensor product basis B_1 B_2  B_n a data point x computes z[i  W[i ⨀ B_1(x[1 ⨂ B_2(x[2 ⨂  ⨂ B_n(x[n where W is the layer's weight and returns z[1  z[out Arguments model  Array of TensorProductBasis B_1(n_1  B_k(n_k where k corresponds to the dimension of the input out  Dimension of the output p  Optional initialization of the layer's weight Initialized to standard normal by default"},{"doctype":"document","id":"ModelingToolkit/comparison.md","title":"Comparison of ModelingToolkit vs Equation-Based and Block Modeling Languages","text":"Comparison of ModelingToolkit vs Equation-Based and Block Modeling Languages Comparison Against Modelica Both Modelica and ModelingToolkit.jl are acausal modeling languages Modelica is a language with many different implementations such as  Dymola  and  OpenModelica  which have differing levels of performance and can give different results on the same model Many of the commonly used Modelica compilers are not open source ModelingToolkit.jl is a language with a single canonical open source implementation All current Modelica compiler implementations are fixed and not extendable by the users from the Modelica language itself For example the Dymola compiler  shares its symbolic processing pipeline  which is roughly equivalent to the  dae_index_lowering  and  structural_simplify  of ModelingToolkit.jl ModelingToolkit.jl is an open and hackable transformation system which allows users to add new non-standard transformations and control the order of application Modelica is a declarative programming language ModelingToolkit.jl is a declarative symbolic modeling language used from within the Julia programming language Its programming language semantics such as loop constructs and conditionals can be used to more easily generate models Modelica is an object-oriented single dispatch language ModelingToolkit.jl built on Julia uses multiple dispatch extensively to simplify code Many Modelica compilers supply a GUI ModelingToolkit.jl does not Modelica can be used to simulate ODE and DAE systems ModelingToolkit.jl has a much more expansive set of system types including nonlinear systems SDEs PDEs and more Comparison Against Simulink Simulink is a causal modeling environment whereas ModelingToolkit.jl is an acausal modeling environment For an overview of the differences consult academic reviews such as  this one  In this sense ModelingToolkit.jl is more similar to the Simscape sub-environment Simulink is used from MATLAB while ModelingToolkit.jl is used from Julia Thus any user defined functions have the performance of their host language For information on the performance differences between Julia and MATLAB consult  open benchmarks  which demonstrate Julia as an order of magnitude or more faster in many cases due to its JIT compilation Simulink uses the MATLAB differential equation solvers while ModelingToolkit.jl uses  DifferentialEquations.jl  For a systematic comparison between the solvers consult  open benchmarks  which demonstrate two orders of magnitude performance advantage for the native Julia solvers across many benchmark problems Simulink comes with a Graphical User Interface GUI ModelingToolkit.jl does not Simulink is a proprietary software meaning users cannot actively modify or extend the software ModelingToolkit.jl is built in Julia and used in Julia where users can actively extend and modify the software interactively in the REPL and contribute to its open source repositories Simulink covers ODE and DAE systems ModelingToolkit.jl has a much more expansive set of system types including SDEs PDEs optimization problems and more Comparison Against CASADI CASADI is written in C but used from Python/MATLAB meaning that it cannot be directly extended by users unless they are using the C interface and run a local build of CASADI ModelingToolkit.jl is both written and used from Julia meaning that users can easily extend the library on the fly even interactively in the REPL CASADI includes limited support for Computer Algebra System CAS functionality while ModelingToolkit.jl is built on the full  Symbolics.jl  CAS CASADI supports DAE and ODE problems via SUNDIALS IDAS and CVODES ModelingToolkit.jl supports DAE and ODE problems via  DifferentialEquations.jl  of which Sundials.jl is 1 of the total available solvers and is outperformed by the native Julia solvers on the vast majority of the benchmark equations In addition the DifferentialEquations.jl interface is confederated meaning that any user can dynamically extend the system to add new solvers to the interface by defining new dispatches of solve CASADI's DAEBuilder does not implement efficiency transformations like tearing which are standard in the ModelingToolkit.jl transformation pipeline CASADI supports special functionality for quadratic programming problems while ModelingToolkit only provides nonlinear programming via  OptimizationSystem  ModelingToolkit.jl integrates with its host language Julia so Julia code can be automatically converted into ModelingToolkit expressions Users of CASADI must explicitly create CASADI expressions Comparison Against Modia.jl Modia.jl uses Julia's expression objects for representing its equations ModelingToolkit.jl uses  Symbolics.jl  and thus the Julia expressions follow Julia semantics and can be manipulated using a computer algebra system CAS Modia's compilation pipeline is similar to the  Dymola symbolic processing pipeline  with some improvements ModelingToolkit.jl has an open transformation pipeline that allows for users to extend and reorder transformation passes where  structural_simplify  is an adaptation of the Modia.jl-improved alias elimination and tearing algorithms Both Modia and ModelingToolkit generate  DAEProblem  and  ODEProblem  forms for solving with  DifferentialEquations.jl  ModelingToolkit.jl integrates with its host language Julia so Julia code can be automatically converted into ModelingToolkit expressions Users of Modia must explicitly create Modia expressions Modia covers DAE systems ModelingToolkit.jl has a much more expansive set of system types including SDEs PDEs optimization problems and more Comparison Against Causal.jl Causal.jl is a causal modeling environment whereas ModelingToolkit.jl is an acausal modeling environment For an overview of the differences consult academic reviews such as  this one  Both ModelingToolkit.jl and Causal.jl use  DifferentialEquations.jl  as the backend solver library Causal.jl lets one add arbitrary equation systems to a given node and allow the output to effect the next node This means an SDE may drive an ODE These two portions are solved with different solver methods in tandem In ModelingToolkit.jl such connections promote the whole system to an SDE This results in better accuracy and stability though in some cases it can be less performant Causal.jl similar to Simulink breaks algebraic loops via inexact heuristics ModelingToolkit.jl treats algebraic loops exactly through algebraic equations in the generated model"},{"doctype":"documentation","id":"references/Surrogates.GEK","title":"GEK","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.store_y_w_vstar!","title":"store_y_w_vstar!","text":""},{"doctype":"documentation","id":"references/NeuralPDE.build_symbolic_equation","title":"build_symbolic_equation","text":""},{"doctype":"documentation","id":"references/Catalyst.jumpratelaw","title":"jumpratelaw","text":"Given a  Reaction  return the symbolic reaction rate law used in generated stochastic chemical kinetics model SSAs for the reaction Note for a reaction defined by k*X*Y X+Z  2X  Y the expression that is returned will be  k*X^2*Y*Z  For a reaction of the form k 2X+3Y  Z the expression that is returned will be  k  binomial(X,2  binomial(Y,3  Notes Allocates combinatoric_ratelaw=true  uses binomials in calculating the rate law i.e for  2S  0  at rate  k  the ratelaw would be  k*S*(S-1)/2  If  combinatoric_ratelaw=false  then the ratelaw is  k*S*(S-1  i.e the rate law is not normalized by the scaling factor"},{"doctype":"documentation","id":"references/MethodOfLines.PeriodicBoundary","title":"PeriodicBoundary","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.namespace_equations","title":"namespace_equations","text":""},{"doctype":"documentation","id":"references/SciMLBase.INITIALIZE_DEFAULT","title":"INITIALIZE_DEFAULT","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.tear_graph_block_modia!","title":"tear_graph_block_modia!","text":""},{"doctype":"documentation","id":"references/Catalyst.getsubsystypes","title":"getsubsystypes","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.one_over_sqrt2","title":"one_over_sqrt2","text":""},{"doctype":"documentation","id":"references/MethodOfLines.split","title":"split","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.SystemStructures","title":"SystemStructures","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.ODEFunctionClosure","title":"ODEFunctionClosure","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.alias_eliminate_graph!","title":"alias_eliminate_graph!","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.∇responsible_map","title":"∇responsible_map","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.ComposedBoundaryPadded3Tensor","title":"ComposedBoundaryPadded3Tensor","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.CompleteCenteredDifference","title":"CompleteCenteredDifference","text":"A helper function to compute the coefficients of a derivative operator including the boundary coefficients in the centered scheme"},{"doctype":"document","id":"MethodOfLines/devnotes.md","title":"Notes for developers","text":"Notes for developers Getting started First fork the repo and clone it locally Then type in the REPL Overview MethodOfLines.jl makes heavy use of  Symbolics.jl  and  SymbolicUtils.jl  especially the replacement rules from the latter Take a look at  src/discretization/MOL_discretization.jl  to get a high level overview of how the discretization works A more consise description can be found here  hiw Feel free to post an issue if you would like help understanding anything or want to know developer opinions on the best way to go about implementing something Adding new finite difference schemes If you know of a finite difference scheme which is better than what is currently implemented please first post an issue with a link to a paper A replacement rule is generated for each term which has a more specific higher stability/accuracy finite difference scheme than the general central difference which represents a base case Take a look at  src/discretization/generate_finite_difference_rules.jl  to see how the replacement rules are generated Note that the order that the rules are applied is important there may be schemes that are applied first that are special cases of more general rules for example the sphrical laplacian is a special case of the nonlinear lalacian First terms are split isolating particular cases Then rules are generated and applied Take a look at the docs for symbolic utils to get an idea of how these work Identify a rule which will match your case then write a function that will handle how to apply that scheme for each index in the interior for each combination of independant and dependant variables Initially don't worry if your scheme is only implemented for specific approximation orders it is sufficient just to warn when the requested approximation order does not match that supplied by the scheme We can work in future pull requests to generalize the scheme to higher approximation orders where possible Inspecting generated code To get the generated code for your system use  code  ODEFunctionExpr(prob  or  MethodOfLines.generate_code(pdesys discretization my_generated_code_filename.jl  which will create a file called  my_generated_code_filename.jl  in  pwd  This can be useful to find errors in the discretization but note that it is not recommended to use this code directly calling  solve(prob AppropriateSolver  will handle this for you"},{"doctype":"documentation","id":"references/ModelingToolkit.hasdist","title":"hasdist","text":"Determine whether or not symbolic variable  x  has a probability distribution associated with it"},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.gen_nlsolve!","title":"gen_nlsolve!","text":""},{"doctype":"documentation","id":"references/Catalyst.getsubsyseqs!","title":"getsubsyseqs!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.check_lhs","title":"check_lhs","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.PhivCache","title":"PhivCache","text":""},{"doctype":"documentation","id":"references/Catalyst.edge_attrs","title":"edge_attrs","text":""},{"doctype":"documentation","id":"references/RecursiveArrayTools.DiffEqArray","title":"DiffEqArray","text":"u AbstractVector t AbstractVector t f t t f2 t t vals f tval f2 tval tval t A vals t A A t This is a  VectorOfArray  which stores  A.t  that matches  A.u  This will plot  A.t[i],A[i  The function  tuples(diffeq_arr  returns tuples of  t,u  To construct a DiffEqArray"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.WienerProcess","title":"WienerProcess","text":"t0 W0 Z0 nothing kwargs t0 W0 Z0 nothing kwargs The  WienerProcess  also known as Brownian motion or the noise in the Langevin equation is the stationary process with white noise increments and a distribution  N(0,dt  The constructor is"},{"doctype":"documentation","id":"references/SciMLBase.SplitFunction","title":"SplitFunction","text":"iip recompile f1 f2 mass_matrix I analytic nothing tgrad nothing jac nothing jvp nothing vjp nothing jac_prototype nothing sparsity jac_prototype paramjac nothing syms nothing indepsym nothing colorvec nothing SplitFunction  AbstractODEFunction A representation of a split ODE function  f  defined by M frac{du}{dt  f_1(u,p,t  f_2(u,p,t and all of its related functions such as the Jacobian of  f  its gradient with respect to time and more For all cases  u0  is the initial condition  p  are the parameters and  t  is the independent variable Generally for ODE integrators the  f_1  portion should be considered the stiff portion of the model with larger time scale separation while the  f_2  portion should be considered the non-stiff portion This interpretation is directly used in integrators like IMEX implicit-explicit integrators and exponential integrators Constructor Note that only the functions  f_i  themselves are required These functions should be given as  f_i!(du,u,p,t  or  du  f_i(u,p,t  See the section on  iip  for more details on in-place vs out-of-place handling All of the remaining functions are optional for improving or accelerating  the usage of  f  These include mass_matrix  the mass matrix  M  represented in the ODE function Can be used to determine that the equation is actually a differential-algebraic equation DAE if  M  is singular Note that in this case special solvers are required see the DAE solver page for more details https://diffeq.sciml.ai/stable/solvers/dae_solve Must be an AbstractArray or an AbstractSciMLOperator analytic(u0,p,t  used to pass an analytical solution function for the analytical  solution of the ODE Generally only used for testing and development of the solvers tgrad(dT,u,p,t  or dT=tgrad(u,p,t returns  frac{\\partial f_1(u,p,t)}{\\partial t jac(J,u,p,t  or  J=jac(u,p,t  returns  frac{df_1}{du jvp(Jv,v,u,p,t  or  Jv=jvp(v,u,p,t  returns the directional derivative frac{df_1}{du v vjp(Jv,v,u,p,t  or  Jv=vjp(v,u,p,t  returns the adjoint derivative frac{df_1}{du}^\\ast v jac_prototype  a prototype matrix matching the type that matches the Jacobian For example if the Jacobian is tridiagonal then an appropriately sized  Tridiagonal  matrix can be used as the prototype and integrators will specialize on this structure where possible Non-structured sparsity patterns should use a  SparseMatrixCSC  with a correct sparsity pattern for the Jacobian The default is  nothing  which means a dense Jacobian paramjac(pJ,u,p,t  returns the parameter Jacobian  frac{df_1}{dp  syms  the symbol names for the elements of the equation This should match  u0  in size For example if  u0  0.0,1.0  and  syms  x y  this will apply a canonical naming to the values allowing  sol[:x  in the solution and automatically naming values in plots indepsym  the canonical naming for the independent variable Defaults to nothing which internally uses  t  as the representation in any plots colorvec  a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the  jac_prototype  This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern Defaults to  nothing  which means a color vector will be internally computed on demand when required The cost of this operation is highly dependent on the sparsity pattern Note on the Derivative Definition The derivatives such as the Jacobian are only defined on the  f1  portion of the split ODE This is used to treat the  f1  implicit while keeping the  f2  portion explicit iip In-Place vs Out-Of-Place For more details on this argument see the ODEFunction documentation recompile Controlling Compilation and Specialization For more details on this argument see the ODEFunction documentation Fields The fields of the SplitFunction type directly match the names of the inputs Symbolically Generating the Functions See the  modelingtoolkitize  function from  ModelingToolkit.jl  for automatically symbolically generating the Jacobian and more from the  numerically-defined functions See  ModelingToolkit.SplitODEProblem  for information on generating the SplitFunction from this symbolic engine"},{"doctype":"documentation","id":"references/SciMLBase.AbstractTimeseriesSolutionRow","title":"AbstractTimeseriesSolutionRow","text":""},{"doctype":"document","id":"Optimization/optimization_packages/gcmaes.md","title":"GCMAES.jl","text":"Pkg Pkg add rosenbrock x p p x p x x x0 zeros p f rosenbrock prob f x0 p lb ub sol prob GCMAESOpt rosenbrock x p p x p x x x0 zeros p f rosenbrock ForwardDiff prob f x0 p lb ub sol prob GCMAESOpt GCMAES.jl GCMAES  is a Julia package implementing the  Gradient-based Covariance Matrix Adaptation Evolutionary Strategy  which can utilize the gradient information to speed up the optimization process Installation OptimizationGCMAES.jl To use this package install the OptimizationGCMAES package Global Optimizer Without Constraint Equations The GCMAES algorithm is called by  GCMAESOpt  and the initial search variance is set as a keyword argument  σ0  default  σ0  0.2  The method in  GCMAES  is performing global optimization on problems without constraint equations However lower and upper constraints set by  lb  and  ub  in the  OptimizationProblem  are required Example The Rosenbrock function can optimized using the  GCMAESOpt  without utilizing the gradient information as follows We can also utilise the gradient information of the optimization problem to aid the optimization as follows"},{"doctype":"documentation","id":"references/PolyChaos.rec2coeff","title":"rec2coeff","text":"Get the coefficients of the orthogonal polynomial of degree up to  deg  specified by its recurrence coefficients  a,b  The function returns the values  c_i^{(k  from p_k t  t^d  sum_{i=0}^{k-1 c_i t^i where  k  runs from  1  to  deg  The call  rec2coeff(a,b  outputs all possible recurrence coefficients given  a,b "},{"doctype":"documentation","id":"references/MethodOfLines.generate_grid","title":"generate_grid","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.calculate_hessian","title":"calculate_hessian","text":"sys Calculate the hessian matrix of a scalar system Returns a matrix of  Num  instances The result from the first call will be cached in the system object"},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.extract_power","title":"extract_power","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_namespace","title":"get_namespace","text":"Return the namespace of a variable as a string If the variable is not namespaced the string is empty"},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.dummy_derivative","title":"dummy_derivative","text":"Perform index reduction and use the dummy derivative techinque to ensure that the system is balanced"},{"doctype":"documentation","id":"references/ModelingToolkit.DAEFunctionClosure","title":"DAEFunctionClosure","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.collect_powers","title":"collect_powers","text":""},{"doctype":"documentation","id":"references/Catalyst.@unpack_Graph","title":"@unpack_Graph","text":""},{"doctype":"documentation","id":"references/Catalyst.Node","title":"Node","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.SystemStructures.AbstractTearingState","title":"AbstractTearingState","text":""},{"doctype":"documentation","id":"references/SciMLBase.DEFAULT_PROB_FUNC","title":"DEFAULT_PROB_FUNC","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.ou_bridge","title":"ou_bridge","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.use_winding","title":"use_winding","text":""},{"doctype":"documentation","id":"references/SciMLBase.DynamicalDDEFunction","title":"DynamicalDDEFunction","text":"iip recompile f1 f2 mass_matrix I analytic nothing tgrad nothing jac nothing jvp nothing vjp nothing jac_prototype nothing sparsity jac_prototype paramjac nothing syms nothing indepsym nothing colorvec nothing DynamicalDDEFunction  AbstractDDEFunction A representation of a DDE function  f  defined by M frac{du}{dt  f(u,h,p,t as a partitioned ODE M_1 frac{du}{dt  f_1(u,h,p,t)\nM_2 frac{du}{dt  f_2(u,h,p,t and all of its related functions such as the Jacobian of  f  its gradient with respect to time and more For all cases  u0  is the initial condition  p  are the parameters and  t  is the independent variable Constructor Note that only the functions  f_i  themselves are required These functions should be given as  f_i!(du,u,h,p,t  or  du  f_i(u,h,p,t  See the section on  iip  for more details on in-place vs out-of-place handling The histroy function  h  acts as an interpolator over time i.e  h(t  with options matching the solution interface i.e  h(t save_idxs  2  All of the remaining functions are optional for improving or accelerating  the usage of  f  These include mass_matrix  the mass matrix  M_i  represented in the ODE function Can be used to determine that the equation is actually a differential-algebraic equation DAE if  M  is singular Note that in this case special solvers are required see the DAE solver page for more details https://diffeq.sciml.ai/stable/solvers/dae*solve Must be an AbstractArray or an AbstractSciMLOperator Should be given as a tuple of mass matrices i.e  M*1 M_2  for the mass matrices of equations 1 and 2 respectively analytic(u0,h,p,t  used to pass an analytical solution function for the analytical  solution of the ODE Generally only used for testing and development of the solvers tgrad(dT,u,h,p,t  or dT=tgrad(u,h,p,t returns  frac{\\partial f(u,p,t)}{\\partial t jac(J,u,h,p,t  or  J=jac(u,h,p,t  returns  frac{df}{du jvp(Jv,v,u,h,p,t  or  Jv=jvp(v,u,h,p,t  returns the directional derivative frac{df}{du v vjp(Jv,v,u,h,p,t  or  Jv=vjp(v,u,h,p,t  returns the adjoint derivative frac{df}{du}^\\ast v jac_prototype  a prototype matrix matching the type that matches the Jacobian For example if the Jacobian is tridiagonal then an appropriately sized  Tridiagonal  matrix can be used as the prototype and integrators will specialize on this structure where possible Non-structured sparsity patterns should use a  SparseMatrixCSC  with a correct sparsity pattern for the Jacobian The default is  nothing  which means a dense Jacobian paramjac(pJ,u,h,p,t  returns the parameter Jacobian  frac{df}{dp  syms  the symbol names for the elements of the equation This should match  u0  in size For example if  u0  0.0,1.0  and  syms  x y  this will apply a canonical naming to the values allowing  sol[:x  in the solution and automatically naming values in plots indepsym  the canonical naming for the independent variable Defaults to nothing which internally uses  t  as the representation in any plots colorvec  a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the  jac_prototype  This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern Defaults to  nothing  which means a color vector will be internally computed on demand when required The cost of this operation is highly dependent on the sparsity pattern iip In-Place vs Out-Of-Place For more details on this argument see the ODEFunction documentation recompile Controlling Compilation and Specialization For more details on this argument see the ODEFunction documentation Fields The fields of the DynamicalDDEFunction type directly match the names of the inputs"},{"doctype":"documentation","id":"references/Surrogates.RadialFunction","title":"RadialFunction","text":""},{"doctype":"documentation","id":"references/SciMLBase.OptimizationSolution","title":"OptimizationSolution","text":"DocStringExtensions.TypeDefinition Representation of the solution to an nonlinear optimization defined by an OptimizationProblem Fields u  the representation of the optimization's solution prob  the original NonlinearProblem/SteadyStateProblem that was solved alg  the algorithm type used by the solver original  if the solver is wrapped from an alternative solver ecosystem such as Optim.jl then this is the original return from said solver library retcode  the return code from the solver Used to determine whether the solver solved successfully  sol.retcode  Success  whether it terminated due to a user-defined callback  sol.retcode  Terminated  or whether it exited due to an error For more details see the return code section of the DifferentialEquations.jl documentation"},{"doctype":"document","id":"Surrogates/water_flow.md","title":"Water flow function","text":"Water flow function The water flow function is defined as  f(r_w,r,T_u,H_u,T_l,H_l,L,K_w  frac{2*\\pi*T_u(H_u  H_l)}{log(\\frac{r}{r_w})*[1  frac{2LT_u}{log(\\frac{r}{r_w})*r_w^2*K_w frac{T_u}{T_l  It has 8 dimension Define the objective function"},{"doctype":"documentation","id":"references/ModelingToolkit.bareiss!","title":"bareiss!","text":"Perform Bareiss's fraction-free row-reduction algorithm on the matrix  M  Optionally a specific pivoting method may be specified swap_strategy is an optional argument that determines how the swapping of rows and coulmns is performed bareiss_colswap the default swaps the columns and rows normally bareiss_virtcolswap pretends to swap the columns which can be faster for sparse matrices"},{"doctype":"documentation","id":"references/SciMLBase.AbstractAnalyticalProblem","title":"AbstractAnalyticalProblem","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.REAL_WHITE_NOISE_BRIDGE","title":"REAL_WHITE_NOISE_BRIDGE","text":""},{"doctype":"documentation","id":"references/SciMLBase.@def","title":"@def","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.has_var","title":"has_var","text":"Determine whether or not an equation or expression contains variable  x "},{"doctype":"documentation","id":"references/SciMLBase.EnsembleSplitThreads","title":"EnsembleSplitThreads","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/NeuralPDE.generate_quasi_random_points","title":"generate_quasi_random_points","text":""},{"doctype":"documentation","id":"references/PolyChaos.GammaMeasure","title":"GammaMeasure","text":""},{"doctype":"documentation","id":"references/LinearSolve.isopenblas","title":"isopenblas","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.bareiss_colswap","title":"bareiss_colswap","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.to_equation_vector","title":"to_equation_vector","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.tear_graph_modia","title":"tear_graph_modia","text":""},{"doctype":"documentation","id":"references/MethodOfLines.getvars","title":"getvars","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.AdjointLSS","title":"AdjointLSS","text":"chunk_size autodiff Val central LSSRegularizer g nothing AdjointLSS  AbstractShadowingSensitivityAlgorithm An implementation of the discrete adjoint-mode  least square shadowing  method LSS replaces the ill-conditioned initial value probem  ODEProblem  for chaotic systems by a well-conditioned least-squares problem This allows for computing sensitivities of long-time averaged quantities with respect to the parameters of the  ODEProblem  The computational cost of LSS scales as number of states x number of time steps Converges to the correct sensitivity at a rate of  T^(-1/2  where  T  is the time of the trajectory See  NILSS  and  NILSAS  for a more efficient non-intrusive formulation Constructor Keyword Arguments autodiff  Use automatic differentiation for constructing the Jacobian if the Jacobian needs to be constructed  Defaults to  true  chunk_size  Chunk size for forward-mode differentiation if full Jacobians are built  autojacvec=false  and  autodiff=true  Default is  0  for automatic choice of chunk size diff_type  The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with  autodiff=false  LSSregularizer  Using  LSSregularizer  one can choose between different regularization routines The default choice is  TimeDilation(10.0,0.0,0.0  TimeDilation(alpha::Number,t0skip::Number,t1skip::Number  Corresponds to a time dilation  alpha  controls the weight  t0skip  and  t1skip  indicate the times truncated at the beginnning and end of the trajectory respectively The default value for  t0skip  and  t1skip  is  zero(alpha  g  instantaneous objective function of the long-time averaged objective SciMLProblem Support This  sensealg  only supports  ODEProblem s This  sensealg  does not support events callbacks This  sensealg  assumes that the objective is a long-time averaged quantity and ergodic i.e the time evolution of the system behaves qualitatively the same over infinite time independent of the specified initial conditions such that only the sensitivity with respect to the parameters is of interest References Wang Q Hu R and Blonigan P Least squares shadowing sensitivity analysis of chaotic limit cycle oscillations Journal of Computational Physics 267 210-224 2014"},{"doctype":"document","id":"SciMLBase/interfaces/PDE.md","title":"The PDE Definition Interface","text":"t t x UnitDisk v w x y z VectorUnitBall The PDE Definition Interface While ODEs  u  f(u,p,t  can be defined by a user-function  f  for PDEs the function form can be different for every PDE How many functions and how many inputs This can always change The SciML ecosystem solves this problem by using  ModelingToolkit.jl  to define  PDESystem  a high-level symbolic description of the PDE to be consumed by other packages The vision for the common PDE interface is that a user should only have to specify their PDE once mathematically and have instant access to everything as simple as a finite difference method with constant grid spacing to something as complex as a distributed multi-GPU discontinuous Galerkin method The key to the common PDE interface is a separation of the symbolic handling from the numerical world All of the discretizers should not solve the PDE but instead be a conversion of the mathematical specification to a numerical problem Preferably the transformation should be to another ModelingToolkit.jl  AbstractSystem  via a  symbolic_discretize  dispatch but in some cases this cannot be done or will not be performant Thus in some cases only a  discretize  definition is given to a  SciMLProblem  with  symbolic_discretize  simply providing diagnostic or lower level information about the construction process These elementary problems such as solving linear systems  Ax=b  solving nonlinear systems  f(x)=0  ODEs etc are all defined by SciMLBase.jl which then numerical solvers can all target these common forms Thus someone who works on linear solvers doesn't necessarily need to be working on a Discontinuous Galerkin or finite element library but instead linear solvers that are good for matrices A with properties  which are then accessible by every other discretization method in the common PDE interface Similar to the rest of the  AbstractSystem  types transformation and analyses functions will allow for simplifying the PDE before solving it and constructing block symbolic functions like Jacobians Constructors Domains WIP Domains are specifying by saying  indepvar in domain  where  indepvar  is a single or a collection of independent variables and  domain  is the chosen domain type A 2-tuple can be used to indicate an  Interval  Thus forms for the  indepvar  can be like Domain Types WIP Interval(a,b  Defines the domain of an interval from  a  to  b  requires explicit import from  DomainSets.jl  but a 2-tuple can be used instead discretize  and  symbolic_discretize The only functions which act on a PDESystem are the following discretize(sys,discretizer  produces the outputted  AbstractSystem  or  SciMLProblem  symbolic_discretize(sys,discretizer  produces a debugging symbolic description of the discretized problem Boundary Conditions WIP Transformations Analyses Discretizer Ecosystem NeuralPDE.jl PhysicsInformedNN NeuralPDE.jl  defines the  PhysicsInformedNN  discretizer which uses a  DiffEqFlux.jl  neural network to solve the differential equation MethodOfLines.jl MOLFiniteDifference WIP MethodOfLines.jl  defines the  MOLFiniteDifference  discretizer which performs a finite difference discretization using the DiffEqOperators.jl stencils These stencils make use of NNLib.jl for fast operations on semi-linear domains"},{"doctype":"documentation","id":"references/DiffEqSensitivity._vecjacobian!","title":"_vecjacobian!","text":""},{"doctype":"documentation","id":"references/SciMLBase.BasicEnsembleAlgorithm","title":"BasicEnsembleAlgorithm","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/SciMLBase.__has_observed","title":"__has_observed","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.AbstractNewtonAlgorithm","title":"AbstractNewtonAlgorithm","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.max_tdir","title":"max_tdir","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.AbstractSysToExpr","title":"AbstractSysToExpr","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.JacobianWrapper","title":"JacobianWrapper","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.BracketingImmutableSolver","title":"BracketingImmutableSolver","text":""},{"doctype":"documentation","id":"references/RecursiveArrayTools.AbstractVectorOfArray","title":"AbstractVectorOfArray","text":""},{"doctype":"documentation","id":"references/NeuralOperators.pad_modes","title":"pad_modes","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.BrownianBridge!","title":"BrownianBridge!","text":"t0 tend W0 Wend Z0 nothing Zend nothing kwargs t0 tend W0 Wend Z0 nothing Zend nothing kwargs A  BrownianBridge  process is a Wiener process with a pre-defined start and end value This process is distribution exact and back be back interpolated exactly as well The constructor is where  W(t0)=W₀   W(tend)=Wend  and likewise for the  Z  process if defined"},{"doctype":"document","id":"Optimization/optimization_packages/blackboxoptim.md","title":"BlackBoxOptim.jl","text":"Pkg Pkg add rosenbrock x p p x p x x x0 zeros p f rosenbrock prob f x0 p lb ub sol prob BBO_adaptive_de_rand_1_bin_radiuslimited maxiters maxtime BlackBoxOptim.jl BlackBoxOptim  is a is a Julia package implementing  Meta-)heuristic/stochastic algorithms  that do not require for the optimized function to be differentiable Installation OptimizationBBO.jl To use this package install the OptimizationBBO package Global Optimizers Without Constraint Equations The algorithms in  BlackBoxOptim  are performing global optimization on problems without constraint equations However lower and upper constraints set by  lb  and  ub  in the  OptimizationProblem  are required A  BlackBoxOptim  algorithm is called by  BBO  prefix followed by the algorithm name Natural Evolution Strategies Separable NES  BBO_separable_nes Exponential NES  BBO_xnes Distance-weighted Exponential NES  BBO_dxnes Differential Evolution optimizers 5 different Adaptive DE/rand/1/bin  BBO_adaptive_de_rand_1_bin Adaptive DE/rand/1/bin with radius limited sampling  BBO_adaptive_de_rand_1_bin_radiuslimited DE/rand/1/bin  BBO_de_rand_1_bin DE/rand/1/bin with radius limited sampling a type of trivial geography  BBO_de_rand_1_bin_radiuslimited DE/rand/2/bin  de_rand_2_bin DE/rand/2/bin with radius limited sampling a type of trivial geography  BBO_de_rand_2_bin_radiuslimited Direct search Generating set search Compass/coordinate search  BBO_generating_set_search Direct search through probabilistic descent  BBO_probabilistic_descent Resampling Memetic Searchers Resampling Memetic Search RS  BBO_resampling_memetic_search Resampling Inheritance Memetic Search RIS  BBO_resampling_inheritance_memetic_search Stochastic Approximation Simultaneous Perturbation Stochastic Approximation SPSA  BBO_simultaneous_perturbation_stochastic_approximation RandomSearch to compare to  BBO_random_search The recommended optimizer is  BBO_adaptive_de_rand_1_bin_radiuslimited The currently available algorithms are listed  here Example The Rosenbrock function can optimized using the  BBO_adaptive_de_rand_1_bin_radiuslimited  as follows"},{"doctype":"document","id":"NeuralPDE/pinn/parm_estim.md","title":"Optimising Parameters of a Lorenz System","text":"Flux GalacticOptimJL OrdinaryDiffEq Plots Interval infimum supremum t σ_ β ρ x y z Dt Differential t eqs Dt x t σ_ y t x t Dt y t x t ρ z t y t Dt z t x t y t β z t bcs x y z domains t Interval dt input_ length domains n chain1 input_ n Flux σ n n Flux σ n n Flux σ n chain2 input_ n Flux σ n n Flux σ n n Flux σ n chain3 input_ n Flux σ n n Flux σ n n Flux σ n lorenz! du u p t du u u du u u u du u u u u0 tspan prob lorenz! u0 tspan sol prob Tsit5 dt ts infimum d domain dt supremum d domain d domains getData sol data us hcat sol ts u ts_ hcat sol ts t us ts_ data getData sol initθs chain1 chain2 chain3 acum accumulate length initθs sep acum i acum i i length acum u_ t_ data len length data additional_loss θ p sum sum abs2 i t_ θ sep i u_ i len i discretization chain1 chain2 chain3 dt param_estim additional_loss additional_loss pde_system eqs bcs domains t x t y t z t σ_ ρ β Dict p p σ_ ρ β prob pde_system discretization callback p l println l res prob BFGS callback callback maxiters p_ res minimizer end end initθ discretization init_params acum accumulate length initθ sep acum i acum i i length acum minimizers res minimizer s s sep ts infimum d domain dt supremum d domain d domains u_predict discretization i t minimizers i t ts i plot sol plot! ts u_predict label Optimising Parameters of a Lorenz System Consider a Lorenz System  begin{align*}\n    frac{\\mathrm{d x}{\\mathrm{d}t  sigma y x  \n    frac{\\mathrm{d y}{\\mathrm{d}t  x rho  z  y  \n    frac{\\mathrm{d z}{\\mathrm{d}t  x y  beta z  \n\\end{align with Physics-Informed Neural Networks Now we would consider the case where we want to optimise the parameters  sigma   beta  and  rho  We start by defining the the problem And the neural networks as We will add an additional loss term based on the data that we have in order to optimise the parameters Here we simply calculate the solution of the lorenz system with  OrdinaryDiffEq.jl  based on the adaptivity of the ODE solver This is used to introduce non-uniformity to the time series Then we define the additional loss funciton  additional_loss(phi θ  p  the function has three arguments  phi  the trial solution  θ  the parameters of neural networks and the hyperparameters  p   Then finally defining and optimising using the  PhysicsInformedNN  interface And then finally some analyisis by plotting Plot_Lorenz"},{"doctype":"documentation","id":"references/ModelingToolkit.has_systems","title":"has_systems","text":""},{"doctype":"documentation","id":"references/PolyChaos.rm_jacobi","title":"rm_jacobi","text":"Creates  N  recurrence coefficients for monic Jacobi polynomials that are orthogonal on  1,1  relative to  w(t  1-t)^a 1+t)^b  The call  rm_jacobi(N,a  is the same as  rm_jacobi(N,a,a  and  rm_jacobi(N  the same as  rm_jacobi(N,0,0 "},{"doctype":"documentation","id":"references/ModelingToolkit.get_tearing_state","title":"get_tearing_state","text":""},{"doctype":"documentation","id":"references/SciMLBase.is_constant","title":"is_constant","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.FLOATING_POINT_LIMIT","title":"FLOATING_POINT_LIMIT","text":""},{"doctype":"documentation","id":"references/SciMLOperators.NullOperator","title":"NullOperator","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/SciMLBase.AbstractSplitODEProblem","title":"AbstractSplitODEProblem","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/DiffEqOperators.affine","title":"affine","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.phiv_timestep","title":"phiv_timestep","text":"Evaluates the linear combination of phi-vector products using time stepping u  varphi_0(tA)b_0  t\\varphi_1(tA)b_1  cdots  t^p\\varphi_p(tA)b_p ts  is an array of time snapshots for u with  U[:,j ≈ u(ts[j   ts  can also be just one value in which case only the end result is returned and  U  is a vector The time stepping formula of Niesen  Wright is used   If the time step  tau  is not specified it is chosen according to 17 of Neisen  Wright If  adaptive==true  the time step and Krylov subsapce size adaptation scheme of Niesen  Wright is used the relative tolerance of which can be set using the keyword parameter  tol  The delta and gamma parameter of the adaptation scheme can also be adjusted Set  verbose=true  to print out the internal steps for debugging For the other keyword arguments consult  arnoldi  and  phiv  which are used internally Niesen J  Wright W 2009 A Krylov subspace algorithm for evaluating the φ-functions in exponential integrators arXiv preprint arXiv:0907.4631"},{"doctype":"documentation","id":"references/ExponentialUtilities.SubspaceCache","title":"SubspaceCache","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.solve_symbolic","title":"solve_symbolic","text":""},{"doctype":"document","id":"PolyChaos/gaussian_mixture_model.md","title":"Gaussian Mixture Models","text":"Gaussian Mixture Models Gaussian mixture models are popular for clustering data Generally speaking they are continuous random variables with a special probability density namely rho(x  sum_{i  1}^{n frac{w_i}{\\sqrt{2 pi sigma_i^2 exp left frac{(x  mu_i)^2}{2 sigma_i^2 right quad text{with quad sum_{i  1}^n w_i  1 where the pairs of means and standard deviations  mu_i sigma_i  and the weights  w_i  for all  i in  1 dots n   are given Let's consider a simple example This looks nice What are now the polynomials that are orthogonal relative to this specific density Let's add the quadrature rule and compute the square norms of the basis polynomials Great"},{"doctype":"documentation","id":"references/LinearSolve.GenericFactorization","title":"GenericFactorization","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.inputs","title":"inputs","text":"Return all variables that mare marked as inputs See also  unbound_inputs  See also  bound_inputs   unbound_inputs"},{"doctype":"documentation","id":"references/ModelingToolkit.LazyNamespace","title":"LazyNamespace","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.collect_vars_difference!","title":"collect_vars_difference!","text":""},{"doctype":"documentation","id":"references/SciMLBase.remake","title":"remake","text":"Re-construct  thing  with new field values specified by the keyword arguments"},{"doctype":"documentation","id":"references/ModelingToolkit.@nonamespace","title":"@nonamespace","text":"DocStringExtensions.MethodSignatures Rewrite  nonamespace a.b.c  to  getvar(getvar(a b namespace  false c namespace  false  This is the default behavior of  getvar  This should be used when inheriting states from a model"},{"doctype":"documentation","id":"references/PoissonRandom","title":"PoissonRandom","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.nlsolve_failure","title":"nlsolve_failure","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.defaults","title":"defaults","text":""},{"doctype":"documentation","id":"references/NeuralOperators.AbstractTransform","title":"AbstractTransform","text":"Interface Base.ndims(<:AbstractTransform  N dims of modes transform(<:AbstractTransform 𝐱::AbstractArray  Apply the transform to 𝐱 truncate_modes(<:AbstractTransform 𝐱_transformed::AbstractArray  Truncate modes that contribute to the noise inverse(<:AbstractTransform 𝐱_transformed::AbstractArray  Apply the inverse transform to 𝐱_transformed"},{"doctype":"documentation","id":"references/SciMLOperators.AbstractSciMLCompositeOperator","title":"AbstractSciMLCompositeOperator","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/SciMLBase.StandardSDEProblem","title":"StandardSDEProblem","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/ModelingToolkit.throw_invalid_operator","title":"throw_invalid_operator","text":"Throw error when difference/derivative operation occurs in the R.H.S"},{"doctype":"documentation","id":"references/PolyChaos.LegendreMeasure","title":"LegendreMeasure","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_states","title":"get_states","text":""},{"doctype":"document","id":"GlobalSensitivity/tutorials/parallelized_gsa.md","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"Statistics OrdinaryDiffEq f du u p t du p u p u u du p u p u u u0 tspan p prob f u0 tspan p t collect range stop length f1 p prob1 prob p p sol prob1 Tsit5 saveat t mean sol maximum sol m f1 total_num_trajectory num_trajectory scatter m means m variances series_annotations a b c d color gray scatter m means m variances series_annotations a b c d color gray m f1 N Plots N lb ub sampler A B N lb ub sampler sobol_result f1 A B p1 bar sobol_result ST title legend p2 bar sobol_result S1 title legend p1_ bar sobol_result ST title legend p2_ bar sobol_result S1 title legend plot p1 p2 p1_ p2_ OrdinaryDiffEq f du u p t du p u p u u du p u p u u u0 tspan p prob f u0 tspan p t collect range stop length f1 p prob_func prob i repeat prob p p i ensemble_prob prob prob_func prob_func sol ensemble_prob Tsit5 saveat t trajectories size p out zeros size p i size p out i mean sol i out i maximum sol i out sobol_result f1 A B batch Parallelized Morris and Sobol Sensitivity Analysis of an ODE Let's run GSA on the Lotka-Volterra model to and study the sensitivity of the maximum of predator population and the average prey population First let's define our model Now let's create a function that takes in a parameter set and calculates the maximum of the predator population and the average of the prey population for those parameter values To do this we will make use of the  remake  function which creates a new  ODEProblem  and use the  p  keyword argument to set the new parameters Now let's perform a Morris global sensitivity analysis on this model We specify that the parameter range is  1,5  for each of the parameters and thus call Let's get the means and variances from the  MorrisResult  struct Let's plot the result For the Sobol method we can similarly do Direct Use of Design Matrices For the Sobol Method we can have more control over the sampled points by generating design matrices Doing it in this manner lets us directly specify a quasi-Monte Carlo sampling method for the parameter space Here we use  QuasiMonteCarlo.jl  to generate the design matrices as follows and now we tell it to calculate the Sobol indices on these designs for the function  f1  we defined in the Lotka Volterra example We plot the first order and total order Sobol Indices for the parameters  a  and  b  sobolbars Parallelizing the Global Sensitivity Analysis In all of the previous examples  f(p  was calculated serially However we can parallelize our computations by using the batch interface In the batch interface each column  p[:,i  is a set of parameters and we output a column for each set of parameters Here we showcase using the  Ensemble Interface  to use  EnsembleGPUArray  to perform automatic multithreaded-parallelization of the ODE solves And now to do the parallelized calls we simply add the  batch=true  keyword argument This user-side parallelism thus allows you to take control and thus for example you can use  DiffEqGPU.jl  for automated GPU-parallelism of the ODE-based global sensitivity analysis"},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.partial_state_selection","title":"partial_state_selection","text":"Perform partial state selection and tearing"},{"doctype":"documentation","id":"references/PolyChaos.nw","title":"nw","text":"returns nodes and weights in matrix form"},{"doctype":"document","id":"NeuralPDE/pinn/wave.md","title":"1D Wave Equation with Dirichlet boundary conditions","text":"Flux GalacticOptimJL Interval infimum supremum t x u Dxx Differential x Dtt Differential t Dt Differential t C eq Dtt u t x C Dxx u t x bcs u t u t u x x x Dt u x domains t Interval x Interval dx Flux σ Flux σ initθ Float64 discretization dx init_params initθ pde_system eq bcs domains t x u t x prob pde_system discretization callback p l println l opt GalacticOptimJL BFGS res prob opt callback callback maxiters discretization Plots ts xs infimum d domain dx supremum d domain d domains analytic_sol_func t x sum k pi sin k pi x cos C k pi t k u_predict reshape first t x res minimizer t ts x xs length ts length xs u_real reshape analytic_sol_func t x t ts x xs length ts length xs diff_u abs u_predict u_real p1 plot ts xs u_real linetype contourf title p2 plot ts xs u_predict linetype contourf title p3 plot ts xs diff_u linetype contourf title plot p1 p2 p3 Flux GalacticOptimJL Plots Printf Interval infimum supremum t x u Dxu Dtu O1 O2 Dxx Differential x Dtt Differential t Dx Differential x Dt Differential t v b L b b π L v eq Dx Dxu t x v Dt Dtu t x b Dtu t x bcs_ u t u t L u x x x Dtu x x ep cbrt eps eltype Float64 der Dxu t x Dx u t x ep O1 t x Dtu t x Dt u t x ep O2 t x bcs bcs_ der domains t Interval L x Interval L inn innd inn Flux tanh inn inn Flux tanh inn inn Flux tanh inn _ innd Flux tanh innd _ initθ map c Float64 c strategy discretization strategy init_params initθ pde_system eq bcs domains t x u t x Dxu t x Dtu t x O1 t x O2 t x prob pde_system discretization pde_inner_loss_functions prob f f loss_function pde_loss_function pde_loss_functions contents inner_loss_functions prob f f loss_function bcs_loss_function bc_loss_functions contents bcs_inner_loss_functions inner_loss_functions callback p l println l println map l_ l_ p pde_inner_loss_functions println map l_ l_ p bcs_inner_loss_functions res prob BFGS callback callback maxiters prob prob u0 res minimizer res prob BFGS callback callback maxiters discretization ts xs infimum d domain supremum d domain d domains μ_n k v sqrt k π b L v L b_n k L L π L π k sin π k π π L k L cos π k L π k a_n k L μ_n k L π L π L b k sin π k π L π L b k L b cos π k L b v π L k sin π k π π L k cos π k π k π k analytic_sol_func t x sum sin k π x L exp v b t a_n k sin μ_n k t b_n k cos μ_n k t k anim t ts t sol analytic_sol_func t x x xs sol_p first t x res minimizer x xs plot sol label ylims title t plot! sol_p label ylims title title gif anim fps ts xs infimum d domain supremum d domain d domains u_predict reshape first t x res minimizer t ts x xs length ts length xs u_real reshape analytic_sol_func t x t ts x xs length ts length xs diff_u abs u_predict u_real p1 plot ts xs u_real linetype contourf title p2 plot ts xs u_predict linetype contourf title p3 plot ts xs diff_u linetype contourf title plot p1 p2 p3 1D Wave Equation with Dirichlet boundary conditions Let's solve this 1-dimensional wave equation begin{align*}\n∂^2_t u(x t  c^2 ∂^2_x u(x t quad  textsf{for all  0  x  1 text and  t  0   \nu(0 t  u(1 t  0 quad  textsf{for all  t  0   \nu(x 0  x 1-x     quad  textsf{for all  0  x  1   \n∂_t u(x 0  0       quad  textsf{for all  0  x  1   \n\\end{align with grid discretization  dx  0.1  and physics-informed neural networks Further the solution of this equation with the given boundary conditions is presented We can plot the predicted solution of the PDE and compare it with the analytical solution in order to plot the relative error waveplot 1D Damped Wave Equation with Dirichlet boundary conditions Now let's solve the 1-dimensional wave equation with damping begin{aligned}\n\\frac{\\partial^2 u(t,x)}{\\partial x^2  frac{1}{c^2 frac{\\partial^2 u(t,x)}{\\partial t^2  v frac{\\partial u(t,x)}{\\partial t \nu(t 0  u(t L  0 \nu(0 x  x(1-x \nu_t(0 x  1  2x \n\\end{aligned with grid discretization  dx  0.05  and physics-informed neural networks Here we take advantage of adaptive derivative to increase accuracy We can see the results here Damped_wave_sol_adaptive_u Plotted as a line one can see the analytical solution and the prediction here 1Dwave_damped_adaptive"},{"doctype":"documentation","id":"references/MethodOfLines.generate_boundary_matching_rules","title":"generate_boundary_matching_rules","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.nonlinear_diffusion!","title":"nonlinear_diffusion!","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timepoint_weighted_meancov","title":"timepoint_weighted_meancov","text":""},{"doctype":"documentation","id":"references/SciMLBase.add_saveat!","title":"add_saveat!","text":"Adds a  saveat  time point at  t "},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.substitute_x","title":"substitute_x","text":""},{"doctype":"documentation","id":"references/SciMLBase.du_cache","title":"du_cache","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.getparams","title":"getparams","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.check_consistency","title":"check_consistency","text":""},{"doctype":"document","id":"NeuralPDE/solvers/ode.md","title":"ODE-Specialized Physics-Informed Neural Solver","text":"nnode opt ODE-Specialized Physics-Informed Neural Solver The ODE-specialized physics-informed neural network PINN solver is a method for the  DifferentialEquations.jl common interface  of  ODEProblem  which generates the solution via a neural network Thus the standard  ODEProblem  is used but a new algorithm  NNODE  is used to solve the problem The algorithm type is where  chain  is a DiffEqFlux  sciml_train compatible Chain or FastChain representing a neural network and  opt  is an optimization method for  sciml_train  For more details see  the DiffEqFlux documentation   on  sciml_train  Lagaris Isaac E Aristidis Likas and Dimitrios I Fotiadis Artificial neural networks for solving ordinary and partial differential equations IEEE Transactions on Neural Networks 9 no 5 1998 987-1000"},{"doctype":"documentation","id":"references/NeuralOperators.OperatorConv","title":"OperatorConv","text":"Arguments ch  Input and output channel size e.g  64=>64  modes  The modes to be preserved Transform  The trafo to operate the transformation permuted  Whether the dim is permuted If  permuted=true  layer accepts data in the order of  ch  batch  otherwise the order is   ch batch  Example"},{"doctype":"documentation","id":"references/SciMLBase.AbstractContinuousCallback","title":"AbstractContinuousCallback","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/MethodOfLines.axiesvals","title":"axiesvals","text":"A function that returns what to replace independent variables with in boundary equations"},{"doctype":"documentation","id":"references/DiffEqOperators.MultiDimBC","title":"MultiDimBC","text":"A multidimensional BC supporting arbitrary BCs at each boundary point To construct an arbitrary BC pass an Array of BCs with dimension  N-1  if  N  is the dimensionality of your domain  u  with a size of  size(u)[setdiff(1:N dim  where dim is the dimension orthogonal to the boundary that you want to extend It is also possible to call Q_dim  MultiDimBC(YourBC size(u dim to use YourBC for the whole boundary orthogonal to that dimension Further it is possible to call Qx Qy Qz  MultiDimBC(YourBC size(u to use YourBC for the whole boundary for all dimensions Valid for any number of dimensions greater than 1 However this is only valid for Robin/General type BCs including neummann/dirichlet when the grid steps are equal in each dimension  including uniform grid case In the case where you want to extend the same Robin/GeneralBC to the whole boundary with a non-uniform grid please use Qx Qy Qz  RobinBC(l r dx::Vector dy::Vector dz::Vector  approximation_order size(u or Qx Qy Qz  GeneralBC(αl αr dx::Vector dy::Vector dz::Vector  approximation_order size(u There are also constructors for NeumannBC DirichletBC and Dirichlet0BC Simply replace  dx  in the call with the tuple dxyz as above and append size(u to the argument signature The order is a required argument in this case where dx dy and dz are vectors of grid steps For Neumann0BC please use Qx Qy Qz  Neumann0BC(T::Type dx::Vector dy::Vector dz::Vector  approximation_order size(u where T is the element type of the domain to be extended"},{"doctype":"documentation","id":"references/DiffEqOperators.NeumannBC","title":"NeumannBC","text":"l  and  r  are the BC coefficients i.e  αl βl γl  and  αl βl γl  tuples and vectors work and correspond to BCs of the form αl  u  βl  u  γl   αr  u  βr  u  γr imposed on the lower  l  and higher  r  index boundaries respectively RobinBC  implements a Robin boundary condition operator  Q  that acts on a vector to give an extended vector as a result see https://github.com/JuliaDiffEq/DiffEqOperators.jl/files/3267835/ghost_node.pdf Write vector b̄₁ as a vertical concatenation with b0 and the rest of the elements of b̄₁ denoted b̄ ₁ the same with ū into u0 and ū  b̄ ₁  b̄ 2  fill(β/Δx length(stencil)-1 Pull out the product of u0 and b0 from the dot product The stencil used to approximate u is denoted s b0  α+(β/Δx)*s[1 Rearrange terms to find a general formula for u0 b̄ ₁̇⋅ū b0  γ/b0 which is dependent on ū  the robin coefficients and Δx The non-identity part of Qa is qa b`₁/b0  β s[2:end]/(α+β s[1]/Δx The constant part is Qb  γ/(α+β*s[1]/Δx do the same at the other boundary amounts to a flip of s[2:end with the other set of boundary coefficients"},{"doctype":"documentation","id":"references/SciMLBase.tmap","title":"tmap","text":""},{"doctype":"document","id":"DiffEqOperators/operators/derivative_operators.md","title":"Derivative Operators","text":"N derivative_order Int approximation_order Int dx len Int coeff_func nothing N derivative_order Int approximation_order Int dx len Int coeff_func nothing offside Int l AbstractArray T r AbstractArray T dx AbstractArray T order one T αl T αr T T Type zero T zero T dx Union AbstractVector T T order α AbstractVector T dx AbstractVector T order αl AbstractArray T αr AbstractArray T dx AbstractArray T order Derivative Operators As shown in the figure the operators act on a set of samples  f_j  f(x_j  for a function  f  at a grid of points  x_j  The grid has  n  interior points at  x_j  jh  for  j  1  to  n  and 2 boundary points at  x_0  0  and  x_{n+1  n+1 h  The input to the numerical operators is a vector  u  f_1 f_2 … f_N  and they output a vector of sampled derivatives  du ≈ f'(x_1 f'(x_2 … f'(x_N  or a higher-order derivative as requested A numerical derivative operator  D  of order  m  can be constructed for this grid with  D  CenteredDifference(1 m h n  The argument  1  indicates that this is the first derivative Order  m  means that the operator is exact up to rounding when  f  is a polynomial of degree  m  or lower The derivative operator  D  is used along with a boundary condition operator  Q  to compute derivatives at the interior points of the grid A simple boundary condition  f(x_0  f(x_n+1  0  is constructed with  Q  Dirichlet0BC(eltype(u  Given these definitions the derivatives are calculated as if the operators  D  and  Q  were matrices  du  D*Q*u  This is an abuse of notation The particular  Q  in this example is a linear operator but in general boundary conditions are affine operators They have the form  Q(x  M*x  c  where  M  is a matrix and  c  is a constant vector As a consequence  Q  cannot be concretized to a matrix Actions of DiffEqOperators on interior points and ghost points The operator  D  works by interpolating a polynomial of degree  m  through  m+1  adjacent points on the grid Near the middle of the grid the derivative is approximated at  x_j  by interpolating a polynomial of order  m  with  x_j  at its centre To define an order m  polynomial values are required at  m+1  points When  x_j  is too close to the boundary for that to fit the polynomial is interpolated through the leftmost or rightmost  m+1  points including two “ghost” points that  Q  appends on the boundaries The numerical derivatives are linear combinations of the values through which the polynomials are interpolated The vectors of the coefficients in these linear combinations are called “stencils” Because  D  takes values at the ghost points and returns values at the interior points it is an  n×(n+2  matrix The boundary condition operator  Q  acts as an  n+2)×n  matrix The output  Q*u  is a vector of values on the  n  interior and the 2 boundary points  a f(x_1 … f(x_N b  The interior points take the values of  u  The values  a  and  b  are samples at “ghost” points on the grid boundaries As shown these values are assigned so that an interpolated polynomial  P(x  satisfies the left hand boundary condition and  Q(x  satisfies the right-hand boundary condition The boundary conditions provided by the library are precisely those for which the values  a  and  b  are affine functions of the interior values  f_j  so that  Q  is an affine operator Higher dimensions In one dimension  u  is naturally stored as a  Vector  and the derivative and boundary condition operators are similar to matrices In two dimensions the values  f(x_j  are naturally stored as a matrix Taking derivatives along the downwards axis is easy because matrices act columnwise Horizontal derivatives can be taken by transposing the matrices The derivative along the rightward axis is  D*F  F*D   This is easy to code but less easy to read for those who haven't seen it before When a function has three or more arguments its values are naturally stored in a higher-dimensional array Julia's multiplication operator is only defined for  Vector  and  Matrix  so applying an operator matrix to these arrays would require a complicated and error prone series of  reshape  and axis permutation functions Therefore the types of derivative and boundary condition operators are parameterised by the axis along which the operator acts With derivative operators the axis is supplied as a type parameter The simple case  CenteredDifference(…  is equivalent to  CenteredDifference{1}(…  row-wise derivatives are taken by  CenteredDifference{2}(…  sheet-wise by  CenteredDifference{3}(…  and along the  N th axis by  CenteredDifference{N}(…  Boundary conditions are more complicated See  doc MultiDimBC  for how they are supposed to work in multiple dimensions They don't currently work that way Constructors The constructors are as follows The arguments are N  The directional dimension of the discretization If  N  is not given it is assumed to be 1 i.e differencing occurs along columns derivative_order  the order of the derivative to discretize approximation_order  the order of the discretization in terms of O(dx^order dx  the spacing of the discretization If  dx  is a  Number  the operator is a uniform discretization If  dx  is an array then the operator is a non-uniform discretization Its type needs to match the one from the Array to be differentiated len  the length of the discretization in the direction of the operator coeff_func  An operational argument for a coefficient function  f(du,u,p,t  which sets the coefficients of the operator If  coeff_func  is a  Number  then the coefficients are set to be constant with that number If  coeff_func  is an  AbstractArray  with length matching  len  then the coefficients are constant but spatially dependent offside  A keyword argument for  UpwindDifference  which sets the number of offside points against the primary wind direction allowing it to have some bias/offset Number of points used for approximation remain same By default its  0  N dimensional derivative operators need to act against a value of at least  N  dimensions Derivative Operator Actions These operators are lazy meaning the memory is not allocated Similarly the operator actions    can be performed without ever building the operator matrices Additionally  mul!(y,L,x  can be performed for non-allocating applications of the operator Concretizations The following concretizations are provided Array SparseMatrixCSC BandedMatrix BlockBandedMatrix Additionally the function  sparse  is overloaded to give the most efficient matrix type for a given operator For one-dimensional derivatives this is a  BandedMatrix  while for higher-dimensional operators this is a  BlockBandedMatrix  The concretizations are made to act on  vec(u  A contraction operator concretizes to an ordinary matrix no matter which dimension the contraction acts along by doing the Kronecker product formulation I.e the action of the built matrix will match the action on  vec(u  Boundary Condition Operators Boundary conditions are implemented through a ghost node approach The discretized values  u  should be the interior of the domain so that for the boundary value operator  Q   Q*u  is the discretization on the closure of the domain By using it like this  L*Q*u  is the  NxN  operator which satisfies the boundary conditions Periodic Boundary Conditions The constructor  PeriodicBC  provides the periodic boundary condition operator Robin Boundary Conditions The variables in l are  αl βl γl  and correspond to a BC of the form  al*u(0  bl*u'(0  cl  and similarly  r  for the right boundary  ar*u(N  br*u'(N  cl  Additionally the following helpers exist for the Neumann  u'(0  α  and Dirichlet  u(0  α  cases This fixes  u  αl  at the first point of the grid and  u  αr  at the last point General Boundary Conditions Implements a generalization of the Robin boundary condition where α is a vector of coefficients Represents a condition of the form α[1  α[2]u[0  α[3]u'[0  α[4]u''[0  0 Operator Actions The boundary condition operators act lazily by appending the appropriate values to the end of the array building the ghost-point extended version for the derivative operator to act on This utilizes special array types to not require copying the interior data Concretizations The following concretizations are provided Array SparseMatrixCSC Additionally the function  sparse  is overloaded to give the most efficient matrix type for a given operator For these operators it's  SparseMatrixCSC  The concretizations are made to act on  vec(u  GhostDerivative Operators When  L  is a  DerivativeOperator  and  Q  is a boundary condition operator  L*Q  produces a  GhostDerivative  operator which is the composition of the two operations Concretizations The following concretizations are provided Array SparseMatrixCSC BandedMatrix Additionally the function  sparse  is overloaded to give the most efficient matrix type for a given operator For these operators it's  BandedMatrix  unless the boundary conditions are  PeriodicBC  in which case it's  SparseMatrixCSC  The concretizations are made to act on  vec(u "},{"doctype":"documentation","id":"references/Surrogates._scaled_chebyshev","title":"_scaled_chebyshev","text":""},{"doctype":"documentation","id":"references/SciMLBase.terminate!","title":"terminate!","text":"Terminates the integrator by emptying  tstops  This can be used in events and callbacks to immediately end the solution process  Optionally  retcode  may be specified see Return Codes RetCodes  retcodes"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.OrnsteinUhlenbeckProcess","title":"OrnsteinUhlenbeckProcess","text":"Θ μ σ t0 W0 Z0 nothing kwargs Θ μ σ t0 W0 Z0 nothing kwargs a  Ornstein-Uhlenbeck  process which is a Wiener process defined by the stochastic differential equation dX_t  theta mu  X_t dt  sigma dW_t The  OrnsteinUhlenbeckProcess  is distribution exact meaning not a numerical solution of the stochastic differential equation and instead follows the exact distribution properties The constructor is"},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.masked_cumsum!","title":"masked_cumsum!","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.cpp_bridge!","title":"cpp_bridge!","text":""},{"doctype":"documentation","id":"references/SciMLBase.add_tstop!","title":"add_tstop!","text":"Adds a  tstop  at time  t "},{"doctype":"documentation","id":"references/DiffEqOperators.BoundaryConditionError","title":"BoundaryConditionError","text":""},{"doctype":"document","id":"SciMLBase/fundamentals/FAQ.md","title":"Frequently Asked Questions","text":"Frequently Asked Questions What are the code styling rules for SciML All SciML libraries are supposed to follow  SciMLStyle  Any deviation from that style is something to be fixed Where do I find more information on the internals of some packages The  SciML Developer Documentation  describes the internals of some of the larger solver libraries at length What are the community practices that SciML developers should use See  ColPrac Contributor's Guide on Collaborative Practices for Community Packages Are there developer programs to help fund parties interested in helping develop SciML Yes See  the SciML Developer Programs  webpage"},{"doctype":"documentation","id":"references/PolyChaos.clenshaw_curtis","title":"clenshaw_curtis","text":"Clenshaw-Curtis quadrature according to  Waldvogel J Bit Numer Math 2006 46 195 "},{"doctype":"documentation","id":"references/GlobalSensitivity.MorrisResult","title":"MorrisResult","text":""},{"doctype":"documentation","id":"references/Catalyst.as_attributes","title":"as_attributes","text":""},{"doctype":"documentation","id":"references/PolyChaos.LaguerreMeasure","title":"LaguerreMeasure","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.OptimizationProblemExpr","title":"OptimizationProblemExpr","text":"Generates a Julia expression for an OptimizationProblem from an OptimizationSystem and allows for automatically symbolically calculating numerical enhancements"},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.pantelides_reassemble","title":"pantelides_reassemble","text":""},{"doctype":"documentation","id":"references/NeuralPDE.get_variables","title":"get_variables","text":""},{"doctype":"documentation","id":"references/SciMLBase.last_step_failed","title":"last_step_failed","text":""},{"doctype":"documentation","id":"references/SciMLOperators.InvertibleOperator","title":"InvertibleOperator","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractLinearProblem","title":"AbstractLinearProblem","text":"DocStringExtensions.TypeDefinition Base for types which define linear systems"},{"doctype":"documentation","id":"references/RecursiveArrayTools.unpack_args","title":"unpack_args","text":""},{"doctype":"documentation","id":"references/SciMLBase.DESensitivity","title":"DESensitivity","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/SciMLOperators.ScaledOperator","title":"ScaledOperator","text":""},{"doctype":"documentation","id":"references/SciMLBase.NonlinearFunction","title":"NonlinearFunction","text":"iip recompile f analytic nothing jac nothing jvp nothing vjp nothing jac_prototype nothing sparsity jac_prototype paramjac nothing syms nothing indepsym nothing colorvec nothing NonlinearFunction  AbstractNonlinearFunction A representation of an nonlinear system of equations  f  defined by 0  f(u,p and all of its related functions such as the Jacobian of  f  its gradient with respect to time and more For all cases  u0  is the initial condition  p  are the parameters and  t  is the independent variable Constructor Note that only the function  f  itself is required This function should be given as  f!(du,u,p  or  du  f(u,p  See the section on  iip  for more details on in-place vs out-of-place handling All of the remaining functions are optional for improving or accelerating  the usage of  f  These include analytic(u0,p  used to pass an analytical solution function for the analytical  solution of the ODE Generally only used for testing and development of the solvers jac(J,u,p  or  J=jac(u,p  returns  frac{df}{du jvp(Jv,v,u,p  or  Jv=jvp(v,u,p  returns the directional derivative frac{df}{du v vjp(Jv,v,u,p  or  Jv=vjp(v,u,p  returns the adjoint derivative frac{df}{du}^\\ast v jac_prototype  a prototype matrix matching the type that matches the Jacobian For example if the Jacobian is tridiagonal then an appropriately sized  Tridiagonal  matrix can be used as the prototype and integrators will specialize on this structure where possible Non-structured sparsity patterns should use a  SparseMatrixCSC  with a correct sparsity pattern for the Jacobian The default is  nothing  which means a dense Jacobian paramjac(pJ,u,p  returns the parameter Jacobian  frac{df}{dp  syms  the symbol names for the elements of the equation This should match  u0  in size For example if  u0  0.0,1.0  and  syms  x y  this will apply a canonical naming to the values allowing  sol[:x  in the solution and automatically naming values in plots indepsym  the canonical naming for the independent variable Defaults to nothing which internally uses  t  as the representation in any plots colorvec  a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the  jac_prototype  This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern Defaults to  nothing  which means a color vector will be internally computed on demand when required The cost of this operation is highly dependent on the sparsity pattern iip In-Place vs Out-Of-Place For more details on this argument see the ODEFunction documentation recompile Controlling Compilation and Specialization For more details on this argument see the ODEFunction documentation Fields The fields of the NonlinearFunction type directly match the names of the inputs"},{"doctype":"document","id":"PolyChaos/orthogonal_polynomials_canonical.md","title":"[Univariate Monic Orthogonal Polynomials]( UnivariateMonicOrthogonalPolynomials)","text":"Univariate Monic Orthogonal Polynomials  UnivariateMonicOrthogonalPolynomials Univariate monic orthogonal polynomials make up the core building block of the package These are real polynomials   pi_k k geq 0  which are univariate  pi_k mathbb{R rightarrow mathbb{R  and orthogonal relative to a nonnegative weight function  w mathbb{R rightarrow mathbb{R}_{\\geq 0  and which have a leading coefficient equal to one begin{aligned}\n\\pi_k(t  t^k  a_{k-1 t^{k-1  dots  a_1 t  a_0 quad forall k  0 1 dots \n\\langle pi_k pi_l rangle  int_{\\mathbb{R pi_k(t pi_l(t w(t mathrm{d}t \n\\begin{cases}\n0  k neq l text and k,l geq 0 \n pi_k 2  0  k  l geq 0\n\\end{cases}\n\\end{aligned These univariate monic orthogonal polynomials satisfy the paramount three-term recurrence relation begin{aligned}\n\\pi_{k+1}(t  t  alpha_k pi_k(t  beta_k pi_{k-1}(t quad k 0 1 dots \n\\pi_o(t  1 \n\\pi_{-1}(t  0.\n\\end{aligned Hence every system of  n  univariate monic orthogonal polynomials   pi_k k=0}^n  is isomorphic to its recurrence coefficients   alpha_k beta_k k=0}^n  Canonical Orthogonal Polynomials The so-called  classical  or  canonical  orthogonal polynomials are polynomials named after famous mathematicians who each discovered a special family of orthogonal polynomials for example  Hermite polynomials  or  Jacobi polynomials  For  classical  orthogonal polynomials there exist closed-form expressions of---among others---the recurrence coefficients Also quadrature rules for  classical  orthogonal polynomials are well-studied with dedicated packages such as  FastGaussQuadrature.jl  However more often than not these  classical  orthogonal polynomials are neither monic nor orthogonal hence not normalized in any sense For example there is a distinction between the  probabilists  Hermite polynomials  and the  physicists  Hermite polynomials  The difference is in the weight function  w(t  relative to which the polynomials are orthogonal begin{aligned}\n&\\text{Probabilists w(t  frac{1}{\\sqrt{2 pi  exp left  frac{t^2}{2 right \n&\\text{Physicists w(t   exp left  t^2 right).\n\\end{aligned To streamline the computations all  classical  orthogonal polynomials are converted to  monic  orthogonal polynomials for which of course the closed-form expressions persist Currently the following weight functions hence  classical  orthogonal polynomials are supported Name Weight  w(t Parameters Support  Classical  polynomial  hermite   exp left  t^2 right    infty infty  Hermite  genhermite   lvert t rvert^{2 mu}\\exp left  t^2 right   mu  frac{1}{2   infty infty  Generalized Hermite  legendre   1    1,1  Legendre  jacobi   1-t)^{\\alpha 1+t)^{\\beta   alpha beta  1   1,1  Jacobi  laguerre   exp(-t    0,\\infty  Laguerre  genlaguerre   t^{\\alpha}\\exp(-t   alpha>-1   0,\\infty  Generalized Laguerre  meixnerpollaczek   frac{1}{2 pi exp((2\\phi-\\pi)t lvert\\Gamma(\\lambda  mathrm{i}t)\\rvert^2  lambda  0 0<\\phi<\\pi   infty,\\infty  Meixner-Pollaczek Additionally the following weight functions that are equivalent to probability density functions are supported Name Weight  w(t Parameters Support  Classical  polynomial  gaussian   frac{1}{\\sqrt{2 pi  exp left  frac{t^2}{2 right    infty infty  Probabilists Hermite  uniform01   1    0,1  Legendre  beta01   frac{1}{B(\\alpha,\\beta  t^{\\alpha-1 1-t)^{\\beta-1  alpha beta  0   0,1  Jacobi  gamma   frac{\\beta^\\alpha}{\\Gamma(\\alpha t^{\\alpha-1 exp(-\\beta t   alpha beta  0   0,\\infty  Laguerre  logistic   frac{\\exp(-t)}{(1+\\exp(-t))^2    infty,\\infty   To generate the orthogonal polynomials up to maximum degree  deg  simply call This generates  op as a  GaussOrthoPoly  type with the underlying Gaussian measure  op.measure  The recurrence coefficients are accessible via  coeffs  By default the constructor for  OrthoPoly  generates  deg+1  recurrence coefficients Sometimes some other number  Nrec  may be required This is why  Nrec  is a keyword for the constructor  OrthoPoly  Let's check whether we truly have more coefficients Arbitrary Weights If you are given a weight function  w  that does not belong to the Table above it is still possible to generate the respective univariate monic orthogonal polynomials First we define the measure by specifying a name the weight the support symmetry and parameters Notice it is advisable to define the weight such that an error is thrown for arguments outside of the support Now we want to construct the univariate monic orthogonal polynomials up to degree  deg  relative to  my_meas  The constructor is By default the recurrence coefficients are computed using the  Stieltjes procuedure  with  Clenshaw-Curtis  quadrature with  Nquad  nodes and weights Hence the choice of  Nquad  influences accuracy Multivariate Monic Orthogonal Polynomials  MultivariateMonicOrthogonalPolynomials Suppose we have  p  systems of univariate monic orthogonal polynomials  pi_k^{(1 k\\geq 0   pi_k^{(2 k\\geq 0 dots  pi_k^{(p k\\geq 0 each system being orthogonal relative to the weights  w^{(1 w^{(2 dots w^{(p  with supports  mathcal{W}^{(1 mathcal{W}^{(2 dots mathcal{W}^{(p  Also let  d^{(i  be the maximum degree of the  i th system of univariate orthogonal polynomials We would like to construct a  p variate monic basis   psi_k k geq 0  with  psi mathbb{R}^p rightarrow mathbb{R  of degree at most  0 leq d leq min_{i=1,\\dots,k d^{(i  Further this basis shall be orthogonal relative to the product measure  w mathcal{W  mathcal{W}^{(1 otimes mathcal{W}^{(2 mathcal{W}^{(1 cdots otimes mathcal{W}^{(p rightarrow mathbb{R}_{\\geq 0  given by w(t  prod_{i=1}^{p w^{(i)}(t_i hence satisfies langle psi_k psi_l rangle  int_{\\mathcal{W psi_k(t psi_l(t w(t mathrm{d t \n\\begin{cases}\n0  k neq l text and k,l geq 0 \n psi_k 2  0  k  l geq 0\n\\end{cases For this there exists the composite struct  MultiOrthoPoly  Let's consider an example where we mix  classical  orthogonal polynomials with an arbitrary weight The total number of  basis polynomials is stored in the field  dim  The univariate basis polynomials making up the multivariate basis are stored in the field  uni  The field  ind  contains the multi-index i.e row  i  stores what combination of univariate polynomials makes up the  i th multivariate polynomial For example translates mathematically to psi_{11}(t  pi_0^{(1)}(t_1 pi_1^{(2)}(t_2 pi_0^{(3)}(t_3 pi_1^{(4)}(t_4 Notice that there is an offset by one because the basis counting starts at 0 but Julia is 1-indexed The underlying measure of  mop  is now of type  ProductMeasure  and stored in the field  measure  The weight  w  can be evaluated as one would expect"},{"doctype":"documentation","id":"references/DiffEqSensitivity.jacobianvec!","title":"jacobianvec!","text":"jacobianvec!(Jv f x v alg buffer seed  nothing Jv  J(f(x))v"},{"doctype":"document","id":"DiffEqFlux/layers/CNFLayer.md","title":"CNF Layer Functions","text":"CNF Layer Functions The following layers are helper functions for easily building neural differential equation architectures specialized for the task of density estimation through Continuous Normalizing Flows CNF"},{"doctype":"documentation","id":"references/MethodOfLines.valmaps","title":"valmaps","text":""},{"doctype":"documentation","id":"references/Surrogates.RadialBasisStructure","title":"RadialBasisStructure","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.SystemStructures.isalgvar","title":"isalgvar","text":""},{"doctype":"documentation","id":"references/QuasiMonteCarlo.LowDiscrepancySample","title":"LowDiscrepancySample","text":"base[i  is the base in the ith direction"},{"doctype":"documentation","id":"references/NeuralPDE.pair","title":"pair","text":"Finds which dependent variables are being used in an equation"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.create_VBT_cache","title":"create_VBT_cache","text":""},{"doctype":"documentation","id":"references/SciMLBase.allowscomplex","title":"allowscomplex","text":"allowscomplex(alg::DEAlgorithm Trait declaration for whether an algorithm is compatible with having complex numbers as the state variables Defaults to false"},{"doctype":"documentation","id":"references/DiffEqSensitivity.b!","title":"b!","text":""},{"doctype":"documentation","id":"references/SciMLBase.DAEFunction","title":"DAEFunction","text":"iip recompile f analytic nothing jac nothing jvp nothing vjp nothing jac_prototype nothing sparsity jac_prototype syms nothing indepsym nothing colorvec nothing f J du u p gamma t testjac res du u p t res du u u u res du u u u testjac J du u p gamma t J gamma u J u J u J gamma u nothing DAEFunction  AbstractDAEFunction A representation of an implicit DAE function  f  defined by 0  f(\\frac{du}{dt},u,p,t and all of its related functions such as the Jacobian of  f  its gradient with respect to time and more For all cases  u0  is the initial condition  p  are the parameters and  t  is the independent variable Constructor Note that only the function  f  itself is required This function should be given as  f!(out,du,u,p,t  or  out  f(du,u,p,t  See the section on  iip  for more details on in-place vs out-of-place handling All of the remaining functions are optional for improving or accelerating  the usage of  f  These include analytic(u0,p,t  used to pass an analytical solution function for the analytical  solution of the ODE Generally only used for testing and development of the solvers jac(J,du,u,p,gamma,t  or  J=jac(du,u,p,gamma,t  returns the implicit DAE Jacobian defined as  gamma frac{dG}{d(du  frac{dG}{du jvp(Jv,v,du,u,p,gamma,t  or  Jv=jvp(v,du,u,p,gamma,t  returns the directional  derivative frac{df}{du v vjp(Jv,v,du,u,p,gamma,t  or  Jv=vjp(v,du,u,p,gamma,t  returns the adjoint  derivative frac{df}{du}^\\ast v jac_prototype  a prototype matrix matching the type that matches the Jacobian For example if the Jacobian is tridiagonal then an appropriately sized  Tridiagonal  matrix can be used as the prototype and integrators will specialize on this structure where possible Non-structured sparsity patterns should use a  SparseMatrixCSC  with a correct sparsity pattern for the Jacobian The default is  nothing  which means a dense Jacobian syms  the symbol names for the elements of the equation This should match  u0  in size For example if  u0  0.0,1.0  and  syms  x y  this will apply a canonical naming to the values allowing  sol[:x  in the solution and automatically naming values in plots indepsym  the canonical naming for the independent variable Defaults to nothing which internally uses  t  as the representation in any plots colorvec  a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the  jac_prototype  This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern Defaults to  nothing  which means a color vector will be internally computed on demand when required The cost of this operation is highly dependent on the sparsity pattern iip In-Place vs Out-Of-Place For more details on this argument see the ODEFunction documentation recompile Controlling Compilation and Specialization For more details on this argument see the ODEFunction documentation Fields The fields of the DAEFunction type directly match the names of the inputs Examples Declaring Explicit Jacobians for DAEs For fully implicit ODEs  DAEProblem s a slightly different Jacobian function is necessary For the DAE G(du,u,p,t  res The Jacobian should be given in the form  gamma*dG/d(du  dG/du  where  gamma  is given by the solver This means that the signature is For example for the equation we would define the Jacobian as Symbolically Generating the Functions See the  modelingtoolkitize  function from  ModelingToolkit.jl  for automatically symbolically generating the Jacobian and more from the  numerically-defined functions"},{"doctype":"document","id":"Optimization/optimization_packages/evolutionary.md","title":"Evolutionary.jl","text":"Pkg Pkg add rosenbrock x p p x p x x x0 zeros p f rosenbrock prob f x0 p lb ub sol prob Evolutionary CMAES μ λ Evolutionary.jl Evolutionary  is a Julia package implementing various evolutionary and genetic algorithm Installation OptimizationCMAEvolutionStrategy.jl To use this package install the OptimizationCMAEvolutionStrategy package Global Optimizer Without Constraint Equations The methods in  Evolutionary  are performing global optimization on problems without constraint equations These methods work both with and without lower and upper constraints set by  lb  and  ub  in the  OptimizationProblem  A  Evolutionary  algorithm is called by one of the following Evolutionary.GA   Genetic Algorithm optimizer Evolutionary.DE   Differential Evolution optimizer Evolutionary.ES   Evolution Strategy algorithm Evolutionary.CMAES   Covariance Matrix Adaptation Evolution Strategy algorithm Algorithm specific options are defined as  kwargs  See the respective documentation for more detail Example The Rosenbrock function can optimized using the  Evolutionary.CMAES  as follows"},{"doctype":"document","id":"DiffEqSensitivity/sde_fitting/neural_sde.md","title":"Neural Stochastic Differential Equations","text":"dudt u p t model u g u p t model2 u prob dudt g x tspan nothing dudt! u h p t model u h t p tau prob dudt_ u0 h tspan nothing Plots Statistics Lux OptimizationFlux StochasticDiffEq DiffEqBase Random rng Random default_rng u0 Float32 datasize tspan tsteps range tspan tspan length datasize trueSDEfunc du u p t true_A du u true_A mp Float32 true_noise_func du u p t du mp u prob_truesde trueSDEfunc true_noise_func u0 tspan ensemble_prob prob_truesde ensemble_sol ensemble_prob SOSRI trajectories ensemble_sum ensemble_sol sde_data sde_data_vars Array ensemble_sol tsteps drift_dudt Lux ActivationFunction x x Lux Dense tanh Lux Dense p1 st1 Lux setup rng drift_dudt diffusion_dudt Lux Lux Dense p2 st2 Lux setup rng diffusion_dudt p1 Lux ComponentArray p1 p2 Lux ComponentArray p2 p Lux ComponentArray p1 p1 p Lux ComponentArray p p2 neuralsde drift_dudt diffusion_dudt tspan SOSRI saveat tsteps reltol abstol prediction0 st1 st2 neuralsde u0 p p1 p p2 st1 st2 drift_ u p t drift_dudt u p p1 st1 diffusion_ u p t diffusion_dudt u p p2 st2 prob_neuralsde drift_ diffusion_ u0 p ensemble_nprob prob_neuralsde ensemble_nsol ensemble_nprob SOSRI trajectories saveat tsteps ensemble_nsum ensemble_nsol plt1 plot ensemble_nsum title scatter! plt1 tsteps sde_data lw scatter tsteps sde_data label scatter! tsteps prediction0 label predict_neuralsde p u u0 Array neuralsde u p p1 p p2 st1 st2 loss_neuralsde p n u repeat reshape u0 n samples predict_neuralsde p u means mean samples dims samples dims mean means means means loss sum abs2 sde_data means sum abs2 sde_data_vars loss means list_plots iter callback p loss means doplot list_plots iter iter list_plots iter display loss plt Plots scatter tsteps sde_data yerror sde_data_vars ylim label Plots scatter! plt tsteps means ribbon label push! list_plots plt doplot display plt opt ADAM adtype optf x p loss_neuralsde x n adtype optprob optf p result1 optprob opt cb callback maxiters optf2 x p loss_neuralsde x n adtype optprob2 optf2 result1 u result2 optprob2 opt cb callback maxiters _ means loss_neuralsde result2 u n plt2 Plots scatter tsteps sde_data yerror sde_data_vars label title xlabel plot! plt2 tsteps means lw ribbon label plt plot plt1 plt2 layout savefig plt nothing Neural Stochastic Differential Equations With neural stochastic differential equations there is once again a helper form  neural_dmsde  which can be used for the multiplicative noise case consult the layers API documentation or  this full example using the layer   function  However since there are far too many possible combinations for the API to support in many cases you will want to performantly define neural differential equations for non-ODE systems from scratch For these systems it is generally best to use  TrackerAdjoint  with non-mutating out-of-place forms For example the following defines a neural SDE with neural networks for both the drift and diffusion terms where  model  and  model2  are different neural networks The same can apply to a neural delay differential equation Its out-of-place formulation is  f(u,h,p,t  Thus for example if we want to define a neural delay differential equation which uses the history value at  p.tau  in the past we can define First let's build training data from the same example as the neural ODE For our dataset we will use DifferentialEquations.jl's  parallel ensemble   interface  to generate data from the average of 10,000 runs of the SDE Now we build a neural SDE For simplicity we will use the  NeuralDSDE  neural SDE with diagonal noise layer function Let's see what that looks like Now just as with the neural ODE we define a loss function that calculates the mean and variance from  n  runs at each time point and uses the distance from the data values Now we train using this loss function We can pre-train a little bit using a smaller  n  and then decrease it after it has had some time to adjust towards the right mean behavior We resume the training with a larger  n  WARNING  this step is a couple of orders of magnitude longer than the previous one And now we plot the solution to an ensemble of the trained neural SDE Try this with GPUs as well"},{"doctype":"documentation","id":"references/SciMLBase.DiscreteFunction","title":"DiscreteFunction","text":"iip recompile f analytic nothing syms nothing DiscreteFunction  AbstractDiscreteFunction A representation of an discrete dynamical system  f  defined by u_{n+1  f(u,p,t_{n+1 and all of its related functions such as the Jacobian of  f  its gradient with respect to time and more For all cases  u0  is the initial condition  p  are the parameters and  t  is the independent variable Constructor Note that only the function  f  itself is required This function should be given as  f!(du,u,p,t  or  du  f(u,p,t  See the section on  iip  for more details on in-place vs out-of-place handling All of the remaining functions are optional for improving or accelerating  the usage of  f  These include analytic(u0,p,t  used to pass an analytical solution function for the analytical  solution of the ODE Generally only used for testing and development of the solvers syms  the symbol names for the elements of the equation This should match  u0  in size For example if  u0  0.0,1.0  and  syms  x y  this will apply a canonical naming to the values allowing  sol[:x  in the solution and automatically naming values in plots iip In-Place vs Out-Of-Place For more details on this argument see the ODEFunction documentation recompile Controlling Compilation and Specialization For more details on this argument see the ODEFunction documentation Fields The fields of the DiscreteFunction type directly match the names of the inputs"},{"doctype":"documentation","id":"references/ModelingToolkit.EMPTY_JAC","title":"EMPTY_JAC","text":""},{"doctype":"documentation","id":"references/QuasiMonteCarlo.LatticeRuleSample","title":"LatticeRuleSample","text":"Samples using a randomly-shifted rank-1 lattice rule"},{"doctype":"documentation","id":"references/ModelingToolkit.ODEProblemExpr","title":"ODEProblemExpr","text":"Generates a Julia expression for constructing an ODEProblem from an ODESystem and allows for automatically symbolically calculating numerical enhancements"},{"doctype":"documentation","id":"references/NeuralPDE.SciMLSolution","title":"SciMLSolution","text":""},{"doctype":"documentation","id":"references/MethodOfLines.PeriodicMap","title":"PeriodicMap","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.construct_correlated_noisefunc!","title":"construct_correlated_noisefunc!","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.out_and_ts","title":"out_and_ts","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.next_variable!","title":"next_variable!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.generate_factorized_W","title":"generate_factorized_W","text":"sys dvs sys ps sys expression Val sparse kwargs Generates a function for the factorized W-matrix matrix of a system Extra arguments control the arguments to the internal  build_function  call"},{"doctype":"documentation","id":"references/ModelingToolkit.expand_instream","title":"expand_instream","text":""},{"doctype":"documentation","id":"references/SciMLBase.update_coefficients!","title":"update_coefficients!","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.wB!","title":"wB!","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.LogisticKernel","title":"LogisticKernel","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.∂","title":"∂","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.phiv_dense!","title":"phiv_dense!","text":"Non-allocating version of  phiv_dense "},{"doctype":"documentation","id":"references/SciMLOperators.expmv!","title":"expmv!","text":""},{"doctype":"documentation","id":"references/SciMLBase.TwoPointBVProblem","title":"TwoPointBVProblem","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/Catalyst.ReactionStruct","title":"ReactionStruct","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractDynamicalDDEProblem","title":"AbstractDynamicalDDEProblem","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/ModelingToolkit.@swap","title":"@swap","text":""},{"doctype":"documentation","id":"references/Surrogates._backward_pass_nd","title":"_backward_pass_nd","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.CorrelatedWienerProcess!","title":"CorrelatedWienerProcess!","text":"Γ t0 W0 Z0 nothing kwargs Γ t0 W0 Z0 nothing kwargs One can define a  CorrelatedWienerProcess  which is a Wiener process with correlations between the Wiener processes The constructor is where  Γ  is the constant covariance matrix"},{"doctype":"documentation","id":"references/Catalyst.symmap_to_varmap","title":"symmap_to_varmap","text":"sir sir β S I I ν I R β ν subsys subsys k A B k sys sir subsys symmap S I R subsys₊A subsys₊B u0map sys symmap pmap sys β ν subsys₊k Given a system and map of  Symbol s to values generates a map from corresponding symbolic variables/parameters to the values that can be used to pass initial conditions and parameter mappings For example gives to specify initial condition and parameter mappings from  symbols  we can use u0map  and  pmap  can then be used as input to various problem types Notes Any  Symbol   sym  within  symmap  must be a valid field of  sys  i.e  sys.sym  must be defined"},{"doctype":"documentation","id":"references/DiffEqFlux.cnf","title":"cnf","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.mydiv","title":"mydiv","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_controls","title":"get_controls","text":""},{"doctype":"documentation","id":"references/SciMLBase.DAEProblem","title":"DAEProblem","text":"DiffEqProblemLibrary DAEProblemLibrary DAEProblemLibrary importdaeproblems prob DAEProblemLibrary prob_dae_resrob sol prob IDA Defines an implicit ordinary differential equation ODE or  differential-algebraic equation DAE problem Documentation Page https://diffeq.sciml.ai/stable/types/dae_types Mathematical Specification of an DAE Problem To define a DAE Problem you simply need to give the function  f  and the initial condition  u_0  which define an ODE 0  f(du,u,p,t f  should be specified as  f(du,u,p,t  or in-place as  f(resid,du,u,p,t  Note that we are not limited to numbers or vectors for  u₀  one is allowed to provide  u₀  as arbitrary matrices  higher dimension tensors as well Problem Type Constructors DAEProblem(f::DAEFunction,du0,u0,tspan,p=NullParameters();kwargs DAEProblem{isinplace}(f,du0,u0,tspan,p=NullParameters();kwargs   Defines the DAE with the specified functions  isinplace  optionally sets whether the function is inplace or not This is determined automatically but not inferred Parameters are optional and if not given then a  NullParameters  singleton will be used which will throw nice errors if you try to index non-existent parameters Any extra keyword arguments are passed on to the solvers For example if you set a  callback  in the problem then that  callback  will be added in every solve call For specifying Jacobians and mass matrices see the DiffEqFunctions  performance_overloads page Fields f  The function in the ODE du0  The initial condition for the derivative u0  The initial condition tspan  The timespan for the problem differential_vars  A logical array which declares which variables are the differential non algebraic vars i.e  du  is in the equations for this variable Defaults to nothing Some solvers may require this be set if an initial condition needs to be determined p  The parameters for the problem Defaults to  NullParameters kwargs  The keyword arguments passed onto the solves Example Problems Examples problems can be found in  DiffEqProblemLibrary.jl  To use a sample problem such as  prob_dae_resrob  you can do something like"},{"doctype":"documentation","id":"references/SciMLBase.__solve","title":"__solve","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.ZygoteNoise","title":"ZygoteNoise","text":"ZygoteNoise  NoiseChoice Uses Zygote.jl to compute vector-Jacobian products for the noise term for SDE adjoints only Tends to be the fastest VJP method if the ODE/DAE/SDE/DDE is written with mostly vectorized functions like neural networks and other layers from Flux.jl and the  f  functions is given out-of-place If the  f  function is in-place then  Zygote.Buffer  arrays are used internally which can greatly reduce the performance of the VJP method Constructor"},{"doctype":"documentation","id":"references/DiffEqSensitivity.seed_duals","title":"seed_duals","text":""},{"doctype":"document","id":"LinearSolve/basics/LinearProblem.md","title":"Linear Problems","text":"Linear Problems"},{"doctype":"documentation","id":"references/Catalyst.filter_nonrxsys","title":"filter_nonrxsys","text":""},{"doctype":"document","id":"PolyChaos/quadrature_rules.md","title":"[Quadrature Rules]( QuadratureRules)","text":"Quadrature Rules  QuadratureRules In this tutorial we investigate how recurrence coefficients of orthogonal polynomials lead to quadrature rules We want to solve the integral I  int_{-1}^{1 f(t w(t mathrm{d t with the weight function w(t  1-t)^a 1+t)^b for all  t in 1 1  and  a b  1  For the function  f  we choose f(t  t^2 To solve the integral we do the following Choose number of nodes  N  Generate recurrence coefficients Generate quadrature rule from those recurrence coefficients We will compare Gauss quadrature to Gauss-Radau quadrature and Gauss-Lobatto quadrature Make sure to check out this tutorial  NumericalIntegration too Let's begin Now we compute  N  recurrence coefficients Gauss The first quadrature rule is Gauss quadrature This method goes back to  Golub and Welsch  Since Gauss quadrature has a degree of exactness of  2N-1  the value of the integral is exact Gauss-Radau Gauss-Radau quadrature is a variant of Gauss quadrature that allows to specify a value of a node that  has to be included  We choose to include the right end point  t  1.0  Gauss-Lobatto Next we look at Gauss-Lobatto quadrature which allows to include two points We choose to include the left and end point of the interval which are  t in 1.0 1.0  There are other quadratures that we subsume as  all-purpose  quadrature rules These include Fejér's first and second rule and Clenshaw-Curtis quadrature Fejér's First Rule Fejér's first rule does  not  include the end points of the interval Fejér's Second Rule Fejér's second  rule does include the end points of the interval Clenshaw-Curtis Clenshaw-Curtis quadrature  is similar to Féjer's second rule as in it includes the end points of the integration interval For the same number of nodes it is also more accurate than Féjer's rules generally speaking As we can see for the same number of nodes  N  the quadrature rules based on the recurrence coefficients can greatly outperform the all-purpose quadratures So whenever possible use quadrature rules based on recurrence coefficients of the orthogonal polynomials relative to the underlying measure Make sure to check out this tutorial  NumericalIntegration too"},{"doctype":"documentation","id":"references/GlobalSensitivity._generate_hadamard","title":"_generate_hadamard","text":""},{"doctype":"documentation","id":"references/Optimization.default_logger","title":"default_logger","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.full_equations","title":"full_equations","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.wiener_randn!","title":"wiener_randn!","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities","title":"ExponentialUtilities","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.DirichletBC","title":"DirichletBC","text":"l  and  r  are the BC coefficients i.e  αl βl γl  and  αl βl γl  tuples and vectors work and correspond to BCs of the form αl  u  βl  u  γl   αr  u  βr  u  γr imposed on the lower  l  and higher  r  index boundaries respectively RobinBC  implements a Robin boundary condition operator  Q  that acts on a vector to give an extended vector as a result see https://github.com/JuliaDiffEq/DiffEqOperators.jl/files/3267835/ghost_node.pdf Write vector b̄₁ as a vertical concatenation with b0 and the rest of the elements of b̄₁ denoted b̄ ₁ the same with ū into u0 and ū  b̄ ₁  b̄ 2  fill(β/Δx length(stencil)-1 Pull out the product of u0 and b0 from the dot product The stencil used to approximate u is denoted s b0  α+(β/Δx)*s[1 Rearrange terms to find a general formula for u0 b̄ ₁̇⋅ū b0  γ/b0 which is dependent on ū  the robin coefficients and Δx The non-identity part of Qa is qa b`₁/b0  β s[2:end]/(α+β s[1]/Δx The constant part is Qb  γ/(α+β*s[1]/Δx do the same at the other boundary amounts to a flip of s[2:end with the other set of boundary coefficients"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.CorrelatedWienerProcess","title":"CorrelatedWienerProcess","text":"Γ t0 W0 Z0 nothing kwargs Γ t0 W0 Z0 nothing kwargs One can define a  CorrelatedWienerProcess  which is a Wiener process with correlations between the Wiener processes The constructor is where  Γ  is the constant covariance matrix"},{"doctype":"documentation","id":"references/SciMLBase.AbstractNonlinearSolution","title":"AbstractNonlinearSolution","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/ModelingToolkit.isdifferential","title":"isdifferential","text":""},{"doctype":"documentation","id":"references/NeuralPDE.logvector","title":"logvector","text":"This function is defined here as stubs to be overriden by the subpackage NeuralPDELogging if imported"},{"doctype":"documentation","id":"references/GlobalSensitivity.RBDFAST","title":"RBDFAST","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.entropy","title":"entropy","text":""},{"doctype":"documentation","id":"references/SciMLBase.AffineDiffEqOperator","title":"AffineDiffEqOperator","text":"AffineDiffEqOperator  AbstractDiffEqOperator Ex A₁(t    Aₙ(t))*u  B₁(t    Bₘ(t AffineDiffEqOperator(As,Bs,du_cache=nothing Takes in two tuples for split Affine DiffEqs update_coefficients works by updating the coefficients of the component operators Function calls L(u p t and L(du u p t are fallbacks interpretted in this form This will allow them to work directly in the nonlinear ODE solvers without modification f(du u p t is only allowed if a du_cache is given B(t can be Union in which case they are constants Otherwise they are interpreted they are functions v=B(t and B(v,t Solvers will see this operator from integrator.f and can interpret it by checking the internals of As and Bs For example it can check isconstant(As[1 etc"},{"doctype":"documentation","id":"references/SciMLBase.DEFAULT_OUTPUT_FUNC","title":"DEFAULT_OUTPUT_FUNC","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.AdjointSensitivityParameterCompatibilityError","title":"AdjointSensitivityParameterCompatibilityError","text":""},{"doctype":"document","id":"NeuralPDE/pinn/fp.md","title":"Fokker-Planck Equation","text":"Flux GalacticOptimJL Interval infimum supremum x p Dx Differential x Dxx Differential x α β _σ x_0 x_end dx eq Dx α x β x p x _σ Dxx p x bcs p x_0 p x_end domains x Interval x_0 x_end inn inn Flux σ inn inn Flux σ inn inn Flux σ inn initθ Float64 lb x_0 ub x_end norm_loss_function θ p inner_f x θ dx x θ prob inner_f lb ub θ norm2 prob reltol abstol maxiters abs norm2 discretization dx init_params initθ additional_loss norm_loss_function pde_system eq bcs domains x p x prob pde_system discretization pde_inner_loss_functions prob f f loss_function pde_loss_function pde_loss_functions contents bcs_inner_loss_functions prob f f loss_function bcs_loss_function bc_loss_functions contents discretization cb_ p l println l println map l_ l_ p pde_inner_loss_functions println map l_ l_ p bcs_inner_loss_functions println norm_loss_function p nothing res prob LBFGS callback cb_ maxiters prob prob u0 res minimizer res prob BFGS callback cb_ maxiters Plots C analytic_sol_func x C exp _σ α x β x xs infimum d domain dx supremum d domain d domains u_real analytic_sol_func x x xs u_predict first x res minimizer x xs plot xs u_real label plot! xs u_predict label Fokker-Planck Equation Let's consider the Fokker-Planck equation  frac{∂}{∂x left  left alpha x  beta x^3\\right p(x)\\right   frac{\\sigma^2}{2 frac{∂^2}{∂x^2 p(x  0   which must satisfy the normalization condition Delta t  p(x  1 with the boundary conditions p(-2.2  p(2.2  0 with Physics-Informed Neural Networks And some analysis fp"},{"doctype":"documentation","id":"references/DiffEqFlux.basic_tgrad","title":"basic_tgrad","text":""},{"doctype":"document","id":"NeuralPDE/index.md","title":"NeuralPDE.jl: Scientific Machine Learning for Partial Differential Equations","text":"NeuralPDE.jl Scientific Machine Learning for Partial Differential Equations NeuralPDE.jl is a solver package which consists of neural network solvers for partial differential equations using scientific machine learning SciML techniques such as physics-informed neural networks PINNs and deep BSDE solvers This package utilizes deep neural networks and neural stochastic differential equations to solve high-dimensional PDEs at a greatly reduced cost and greatly increased generality compared with classical methods Features Physics-Informed Neural Networks for automated PDE solving Forward-Backwards Stochastic Differential Equation FBSDE methods for parabolic PDEs Deep-learning-based solvers for optimal stopping time and Kolmogorov backwards equations Citation If you use NeuralPDE.jl in your research please cite  this paper "},{"doctype":"documentation","id":"references/DiffEqFlux.FFJORDDistribution","title":"FFJORDDistribution","text":"FFJORD can be used as a distribution to generate new samples by  rand  or estimate densities by  pdf  or  logpdf  from  Distributions.jl  Arguments model  A FFJORD instance regularize  Whether we use regularization default  false  monte_carlo  Whether we use monte carlo default  true "},{"doctype":"documentation","id":"references/DiffEqOperators.MatrixFreeOperator","title":"MatrixFreeOperator","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.ExtraVariablesSystemException","title":"ExtraVariablesSystemException","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.throw_missingvars_in_sys","title":"throw_missingvars_in_sys","text":""},{"doctype":"documentation","id":"references/Catalyst.deficiency","title":"deficiency","text":"sir SIR β S I I ν I R β ν rcs sir δ sir Calculate the deficiency of a reaction network Here the deficiency  delta  of a network with  n  reaction complexes  ell  linkage classes and a rank  s  stoichiometric matrix is delta  n  ell  s Notes Requires the  incidencemat  to already be cached in  rn  by a previous call to  reactioncomplexes  For example"},{"doctype":"documentation","id":"references/NeuralPDE.StochasticTraining","title":"StochasticTraining","text":"points  number of points in random select training set bcs_points  number of points in random select training set for boundry conditions by default it equals  points "},{"doctype":"documentation","id":"references/PolyChaos._checkNumberOfSamples","title":"_checkNumberOfSamples","text":""},{"doctype":"document","id":"PolyChaos/multiple_discretization.md","title":"Multiple Discretization","text":"Multiple Discretization This tutorial shows how to compute recurrence coefficients for non-trivial weight functions and how they are being used for quadrature The method we use is called  multiple discretization  and follows W Gautschi's book Orthogonal Polynomials Computation and Approximation specifically Section 2.2.4 and Example 2.38 Suppose we have the weight function forall t in 1,1 gamma in 0,1 quad w(t;\\gamma  gamma  1-\\gamma frac{1}{\\sqrt{1-t^2 and we would like to solve int_{-1}^{1 f(t w(t;c mathrm{d}t  sum_{\\nu=1}^{N f(\\tau_\\nu w_\\nu by some quadrature rule We will see that ad-hoc quadrature rules will fail to solve the integral even for the simplest choice  f equiv 1  However finding the recurrence coefficients of the underlying orthogonal polynomials and then finding the quadrature rule will do just fine Let us first try to solve the integral for  f equiv 1  by Féjer's rule Clearly that is not satisfying Well the term  gamma  of the weight  w  makes us think of Gauss-Legendre integration so let's try it instead Even worse Well we can factor out  frac{1}{\\sqrt{1-t^2  making the integral amenable to a Gauss-Chebyshev rule So let's give it anothery try Okay that's better but it took us a lot of nodes to get this result Is there a different way Indeed there is As we have noticed the weight  w  has a lot in common with Gauss-Legendre  and  Gauss-Chebyshev We can decompose the integral as follows int_{-1}^1 f(t w(t mathrm{d}t  sum_{i=1}^{m int_{-1}^{1 f(t w_i(t mathrm{d t with begin{align*}\nw_1(t  gamma \nw_2(t  1-\\gamma frac{1}{\\sqrt{1-t^2}}.\n\\end{align To the weight  w_1  we can apply Gauss-Legendre quadrature to the weight  w_2  we can apply Gauss-Chebyshev quadrature with tiny modifications This  discretization  of the measure can be used in our favor The function  mcdiscretization  takes the  m  discretization rules as an input Et voilà no error with fewer nodes For this example we'd need in fact just a single node The function  mcdiscretization  is able to construct the recurrence coefficients of the orthogonal polynomials relative to the weight  w  Let's inspect the values of the recurrence coefficients a little more For  gamma  0  we are in the world of Chebyshev polynomials for  gamma  1  we enter the realm of Legendre polynomials And in between That's exactly where the weight  w  comes in it can be thought of as an interpolatory weight interpolating Legendre polynomials and Chebyshev polynomials Let's verify this by plotting the recurrence coefficients for several values of  gamma  Let's plot these values to get a better feeling The crosses denote the values of the β recursion coefficients for Chebyshev polynomials the circles the β recursion coefficients for Legendre polynomials The interpolating line in between stands for the β recursion coefficients of  w(t gamma "},{"doctype":"documentation","id":"references/DiffEqSensitivity.dgdt","title":"dgdt","text":""},{"doctype":"documentation","id":"references/SciMLBase.getobserved","title":"getobserved","text":""},{"doctype":"documentation","id":"references/PolyChaos.rm_jacobi01","title":"rm_jacobi01","text":"Creates  N  recurrence coefficients for monic Jacobi polynomials that are orthogonal on  0,1  relative to  w(t  1-t)^a t^b  The call  rm_jacobi01(N,a  is the same as  rm_jacobi01(N,a,a  and  rm_jacobi01(N  the same as  rm_jacobi01(N,0,0 "},{"doctype":"document","id":"Surrogates/gramacylee.md","title":"gramacylee","text":"Gramacy  Lee Function Gramacy  Lee Function is a continues function It is not convex The function is defined on 1-dimensional space It is an unimodal The function can be defined on any input domain but it is usually evaluated on  x in 0.5 2.5  The Gramacy  Lee is as follows  f(x  frac{sin(10\\pi x)}{2x  x-1)^4  Let's import these two packages  Surrogates  and  Plots  Now let's define our objective function Let's sample f in 25 points between 0.5 and 2.5 using the  sample  function The sampling points are chosen using a Sobol Sample this can be done by passing  SobolSample  to the  sample  function Now let's fit Gramacy  Lee Function with different Surrogates"},{"doctype":"documentation","id":"references/ExponentialUtilities.phi!","title":"phi!","text":"Non-allocating version of  phi  for non-diagonal matrix inputs"},{"doctype":"documentation","id":"references/DiffEqFlux.backward_ffjord","title":"backward_ffjord","text":""},{"doctype":"documentation","id":"references/Catalyst.netstoichmat","title":"netstoichmat","text":"Returns the net stoichiometry matrix  N  with  N_{i j  the net stoichiometric coefficient of the ith species within the jth reaction Notes Set sparse=true for a sparse matrix representation Caches the matrix internally within  rn  so subsequent calls are fast"},{"doctype":"documentation","id":"references/GlobalSensitivity.eFASTResult","title":"eFASTResult","text":""},{"doctype":"documentation","id":"references/Catalyst.mm","title":"mm","text":"A Michaelis-Menten rate function"},{"doctype":"documentation","id":"references/DiffEqFlux.legendre_poly","title":"legendre_poly","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.algebraic_variables_scc","title":"algebraic_variables_scc","text":"Find strongly connected components of algebraic variables in a system"},{"doctype":"documentation","id":"references/SciMLBase.AbstractSDDEIntegrator","title":"AbstractSDDEIntegrator","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/Catalyst.@reaction","title":"@reaction","text":"rx k v A B C D k v t A t B t C t D t rx k v A B C D k b t A t ex k A t rx b ex A A C Generates a single  Reaction  object Examples Here  k  and  v  will be parameters and  A   B   C  and  D  will be variables Interpolation of existing parameters/variables also works Notes Any symbols arising in the rate expression that aren't interpolated are treated as parameters In the reaction part  α*A  B  C  D  coefficients are treated as parameters e.g  α  and rightmost symbols as species e.g  A,B,C,D  Works with any  single  arrow types supported by  reaction_network  Interpolation of Julia variables into the macro works similar to the  reaction_network  macro See  The Reaction DSL  tutorial for more details"},{"doctype":"documentation","id":"references/ModelingToolkit.generate_isouter","title":"generate_isouter","text":""},{"doctype":"documentation","id":"references/SciMLOperators.⊗","title":"⊗","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.AdjointLSSProblem","title":"AdjointLSSProblem","text":""},{"doctype":"documentation","id":"references/SciMLBase.remaker_of","title":"remaker_of","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.DEFAULT","title":"DEFAULT","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.linearize_eqs","title":"linearize_eqs","text":""},{"doctype":"document","id":"DiffEqOperators/index.md","title":"DiffEqOperators.jl","text":"DiffEqOperators.jl DiffEqOperators.jl is a package for finite difference discretization of partial differential equations It is for building fast lazy operators for high order non-uniform finite differences Note For automated finite difference discretization of symbolically-defined PDEs see  MethodOfLines.jl  Warn This library is not complete especially for higher dimensional operators Use with caution For the operators both centered and  upwind  operators are provided for domains of any dimension arbitrarily spaced grids and for any order of accuracy The cases of 1 2 and 3 dimensions with an evenly spaced grid are optimized with a convolution routine from  NNlib.jl  Care is taken to give efficiency by avoiding unnecessary allocations using purpose-built stencil compilers allowing GPUs and parallelism etc Any operator can be concretized as an  Array  a  BandedMatrix  or a sparse matrix"},{"doctype":"document","id":"ModelingToolkit/basics/AbstractSystem.md","title":"The AbstractSystem Interface","text":"The AbstractSystem Interface Overview The  AbstractSystem  interface is the core of the system level of ModelingToolkit.jl It establishes a common set of functionality that is used between systems representing ODEs PDEs SDEs and more allowing users to have a common framework for model manipulation and compilation Subtypes There are three immediate subtypes of  AbstractSystem  classified by how many independent variables each type has AbstractTimeIndependentSystem  has no independent variable eg  NonlinearSystem  AbstractTimeDependentSystem  has a single independent variable eg  ODESystem  AbstractMultivariateSystem  may have multiple independent variables eg  PDESystem  Constructors and Naming The  AbstractSystem  interface has a consistent method for constructing systems Generally it follows the order of Equations Independent Variables Dependent Variables or States Parameters All other pieces are handled via keyword arguments  AbstractSystem s share the same keyword arguments which are system  This is used for specifying subsystems for hierarchical modeling with reusable components For more information see the components page  components Defaults Keyword arguments like  defaults  are used for specifying default values which are used If a value is not given at the  SciMLProblem  construction time its numerical value will be the default Composition and Accessor Functions Each  AbstractSystem  has lists of variables in context such as distinguishing parameters vs states In addition an  AbstractSystem  also can hold other  AbstractSystem  types Direct accessing of the values such as  sys.states  gives the immediate list while the accessor functions  states(sys  gives the total set which includes that of all systems held inside The values which are common to all  AbstractSystem s are equations(sys  All equations that define the system and its subsystems states(sys  All the states in the system and its subsystems parameters(sys  All parameters of the system and its subsystems nameof(sys  The name of the current-level system get_eqs(sys  Equations that define the current-level system get_states(sys  States that are in the current-level system get_ps(sys  Parameters that are in the current-level system get_systems(sys  Subsystems of the current-level system Optionally a system could have observed(sys  All observed equations of the system and its subsystems get_observed(sys  Observed equations of the current-level system get_continuous_events(sys   SymbolicContinuousCallback s of the current-level system get_defaults(sys  A  Dict  that maps variables into their default values independent_variables(sys  The independent variables of a system get_noiseeqs(sys  Noise equations of the current-level system Note that if you know a system is an  AbstractTimeDependentSystem  you could use  get_iv  to get the unique independent variable directly rather than using  independent_variables(sys)[1  which is clunky and may cause problems if  sys  is an  AbstractMultivariateSystem  because there may be more than one independent variable  AbstractTimeIndependentSystem s do not have a method  get_iv  and  independent_variables(sys  will return a size-zero result for such For an  AbstractMultivariateSystem   get_ivs  is equivalent A system could also have caches get_jac(sys  The Jacobian of a system get_tgrad(sys  The gradient with respect to time of a system Transformations Transformations are functions which send a valid  AbstractSystem  definition to another  AbstractSystem  These are passes like optimizations e.g Block-Lower Triangle transformations or changes to the representation which allow for alternative numerical methods to be utilized on the model e.g DAE index reduction Analyses Analyses are functions on a system which return information about the corresponding properties like whether its parameters are structurally identifiable or whether it's linear Function Calculation and Generation The calculation and generation functions allow for calculating additional quantities to enhance the numerical methods applied to the resulting system The calculations like  calculate_jacobian  generate ModelingToolkit IR for the Jacobian of the system while the generations like  generate_jacobian  generate compiled output for the numerical solvers by applying  build_function  to the generated code Additionally many systems have function-type outputs which cobble together the generation functionality for a system for example  ODEFunction  can be used to generate a DifferentialEquations-based  ODEFunction  with compiled version of the ODE itself the Jacobian the mass matrix etc Below are the possible calculation and generation functions Additionally  jacobian_sparsity(sys  and  hessian_sparsity(sys  exist on the appropriate systems for fast generation of the sparsity patterns via an abstract interpretation without requiring differentiation Problem Constructors At the end the system types have  DEProblem  constructors like  ODEProblem  which allow for directly generating the problem types required for numerical methods The first argument is always the  AbstractSystem  and the proceeding arguments match the argument order of their original constructors Whenever an array would normally be provided such as  u0  the initial condition of an  ODEProblem  it is instead replaced with a variable map i.e an array of pairs  var=>value  which allows the user to designate the values without having to know the order that ModelingToolkit is internally using For the value maps the parameters are allowed to be functions of each other and value maps of states can be functions of the parameters i.e you can do Default Value Handling The  AbstractSystem  types allow for specifying default values for example  defaults  inside of them At problem construction time these values are merged into the value maps where for any repeats the value maps override the default In addition defaults of a higher level in the system override the defaults of a lower level in the system"},{"doctype":"documentation","id":"references/LinearSolve.KLUFactorization","title":"KLUFactorization","text":""},{"doctype":"documentation","id":"references/RecursiveArrayTools.VectorOfArray","title":"VectorOfArray","text":"u AbstractVector A  VectorOfArray  is an array which has the underlying data structure  Vector{AbstractArray{T  but hopefully concretely typed This wrapper over such data structures allows one to lazily act like it's a higher-dimensional vector and easily convert to different forms The indexing structure is which presents itself as a column-major matrix with the columns being the arrays from the vector The  AbstractArray  interface is implemented giving access to  copy   push   append  etc functions which act appropriately Points to note are The length is the number of vectors or  length(A.u  where  u  is the vector of arrays Iteration follows the linear index and goes over the vectors Additionally the  convert(Array,VA::AbstractVectorOfArray  function is provided which transforms the  VectorOfArray  into a matrix/tensor Also  vecarr_to_vectors(VA::AbstractVectorOfArray  returns a vector of the series for each component that is  A[i  for each  i  A plot recipe is provided which plots the  A[i  series"},{"doctype":"documentation","id":"references/MethodOfLines.edge_align","title":"edge_align","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timeseries_steps_meancor","title":"timeseries_steps_meancor","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.tovar","title":"tovar","text":"Maps the variable to a state"},{"doctype":"document","id":"Surrogates/surrogate.md","title":"Surrogate","text":"Surrogate Every surrogate has a different definition depending on the parameters needed However they have in common add_point!(::AbstractSurrogate,x_new,y_new AbstractSurrogate(value  The first function adds a sample point to the surrogate thus changing the internal coefficients The second one calculates the approximation at value Linear surrogate Radial basis function surrogate Kriging surrogate Lobachevsky surrogate Support vector machine surrogate requires  using LIBSVM Random forest surrogate requires  using XGBoost Neural network surrogate requires  using Flux Creating another surrogate It's great that you want to add another surrogate to the library You will need to Define a new mutable struct and a constructor function Define add point!(your surrogate::AbstactSurrogate,x new,y new Define your surrogate(value for the approximation Example"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.Tail2","title":"Tail2","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.SplineLayer","title":"SplineLayer","text":"time_span time_step spline_basis saved_points nothing Constructs a Spline Layer At a high-level it performs the following Takes as input a one-dimensional training dataset a time span a time step and an interpolation method During training adjusts the values of the function at multiples of the time-step such that the curve interpolated through these points has minimum loss on the corresponding one-dimensional dataset Arguments time_span  Tuple of real numbers corresponding to the time span time_step  Real number corresponding to the time step spline_basis  Interpolation method to be used yb the basis current supported interpolation methods ConstantInterpolation LinearInterpolation QuadraticInterpolation QuadraticSpline CubicSpline saved_points values of the function at multiples of the time step Initialized by default to a random vector sampled from the unit normal"},{"doctype":"documentation","id":"references/DiffEqOperators.compose","title":"compose","text":"A  compose(padded_arrays::BoundaryPaddedArray Example A  compose(Ax Ay Az  3D domain A  compose(Ax Ay  2D Domain Composes BoundaryPaddedArrays that extend the same u for each different dimension that u has in to a ComposedBoundaryPaddedArray Ax Ay and Az can be passed in any order as long as there is exactly one BoundaryPaddedArray that extends each dimension Q  compose(BCs Example Q  compose(Qx Qy Qz  3D domain Q  compose(Qx Qy  2D Domain Creates a ComposedMultiDimBC operator Q that extends every boundary when applied to a  u  with a compatible size and number of dimensions Qx Qy and Qz can be passed in any order as long as there is exactly one BC operator that extends each dimension"},{"doctype":"documentation","id":"references/ExponentialUtilities.kiops","title":"kiops","text":"Evaluate a linear combinaton of the  φ  functions evaluated at  tA  acting on vectors from  u  that is   w(i  φ_0(t[i A u 1  φ_1(t[i A u 2  φ_2(t[i A u 3   The size of the Krylov subspace is changed dynamically during the integration The Krylov subspace is computed using the incomplete orthogonalization method Arguments tstops       Array of  tstop A            the matrix argument of the  φ  functions u            the matrix with columns representing the vectors to be multiplied by the  φ  functions Keyword arguments tol          the convergence tolerance required default 1e-7 mmin   mmax   let the Krylov size vary between mmin and mmax default 10 128 m            an estimate of the appropriate Krylov size default mmin iop          length of incomplete orthogonalization procedure default 2 ishermitian    whether  A  is Hermitian default ishermitian(A opnorm        the operator norm of  A  default opnorm(A Inf task1        if true divide the result by 1/T^p Returns w          the linear combination of the  φ  functions evaluated at  tA  acting on the vectors from  u stats[1   number of substeps stats[2   number of rejected steps stats[3   number of Krylov steps stats[4   number of matrix exponentials stats[5   the Krylov size of the last substep n  is the size of the original problem  p  is the highest index of the  φ  functions References Gaudreault S Rainwater G and Tokman M 2018 KIOPS A fast adaptive Krylov subspace solver for exponential integrators Journal of Computational Physics Based on the PHIPM and EXPMVP codes http://www1.maths.leeds.ac.uk/~jitse/software.html https://gitlab.com/stephane.gaudreault/kiops Niesen J and Wright W.M 2011 A Krylov subspace method for option pricing SSRN 1799124 Niesen J and Wright W.M 2012 Algorithm 919 A Krylov subspace algorithm for evaluating the  φ functions appearing in exponential integrators ACM Transactions on Mathematical Software TOMS 38(3 p.22"},{"doctype":"documentation","id":"references/Surrogates._calc_loba_coeffND","title":"_calc_loba_coeffND","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.jacobian","title":"jacobian","text":""},{"doctype":"documentation","id":"references/NeuralPDE.generate_training_sets","title":"generate_training_sets","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.Curl","title":"Curl","text":""},{"doctype":"documentation","id":"references/SciMLBase.DAEInitializationAlgorithm","title":"DAEInitializationAlgorithm","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/Catalyst.getsubsyseqs","title":"getsubsyseqs","text":""},{"doctype":"documentation","id":"references/SciMLBase.TimeDerivativeWrapper","title":"TimeDerivativeWrapper","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.REVERSEDIFF_ADJOINT_GPU_COMPATABILITY_MESSAGE","title":"REVERSEDIFF_ADJOINT_GPU_COMPATABILITY_MESSAGE","text":""},{"doctype":"documentation","id":"references/Surrogates","title":"Surrogates","text":""},{"doctype":"documentation","id":"references/SciMLBase.RODEFunction","title":"RODEFunction","text":"iip recompile f mass_matrix I analytic nothing tgrad nothing jac nothing jvp nothing vjp nothing jac_prototype nothing sparsity jac_prototype paramjac nothing syms nothing indepsym nothing colorvec nothing RODEFunction  AbstractRODEFunction A representation of an RODE function  f  defined by M frac{du}{dt  f(u,p,t,W)dt and all of its related functions such as the Jacobian of  f  its gradient with respect to time and more For all cases  u0  is the initial condition  p  are the parameters and  t  is the independent variable Constructor Note that only the function  f  itself is required This function should be given as  f!(du,u,p,t  or  du  f(u,p,t  See the section on  iip  for more details on in-place vs out-of-place handling All of the remaining functions are optional for improving or accelerating  the usage of  f  These include mass_matrix  the mass matrix  M  represented in the ODE function Can be used to determine that the equation is actually a differential-algebraic equation DAE if  M  is singular Note that in this case special solvers are required see the DAE solver page for more details https://diffeq.sciml.ai/stable/solvers/dae_solve Must be an AbstractArray or an AbstractSciMLOperator analytic(u0,p,t  used to pass an analytical solution function for the analytical  solution of the ODE Generally only used for testing and development of the solvers tgrad(dT,u,p,t  or dT=tgrad(u,p,t returns  frac{\\partial f(u,p,t)}{\\partial t jac(J,u,p,t  or  J=jac(u,p,t  returns  frac{df}{du jvp(Jv,v,u,p,t  or  Jv=jvp(v,u,p,t  returns the directional derivative frac{df}{du v vjp(Jv,v,u,p,t  or  Jv=vjp(v,u,p,t  returns the adjoint derivative frac{df}{du}^\\ast v jac_prototype  a prototype matrix matching the type that matches the Jacobian For example if the Jacobian is tridiagonal then an appropriately sized  Tridiagonal  matrix can be used as the prototype and integrators will specialize on this structure where possible Non-structured sparsity patterns should use a  SparseMatrixCSC  with a correct sparsity pattern for the Jacobian The default is  nothing  which means a dense Jacobian paramjac(pJ,u,p,t  returns the parameter Jacobian  frac{df}{dp  syms  the symbol names for the elements of the equation This should match  u0  in size For example if  u0  0.0,1.0  and  syms  x y  this will apply a canonical naming to the values allowing  sol[:x  in the solution and automatically naming values in plots indepsym  the canonical naming for the independent variable Defaults to nothing which internally uses  t  as the representation in any plots colorvec  a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the  jac_prototype  This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern Defaults to  nothing  which means a color vector will be internally computed on demand when required The cost of this operation is highly dependent on the sparsity pattern iip In-Place vs Out-Of-Place For more details on this argument see the ODEFunction documentation recompile Controlling Compilation and Specialization For more details on this argument see the ODEFunction documentation Fields The fields of the RODEFunction type directly match the names of the inputs"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.save_noise!","title":"save_noise!","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.unpadded_size","title":"unpadded_size","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.has_ctrls","title":"has_ctrls","text":""},{"doctype":"documentation","id":"references/ParameterizedFunctions.ode_def_opts","title":"ode_def_opts","text":"name Symbol opts Dict Symbol Bool curmod ex Expr t opts Dict Symbol Bool build_tgrad build_jac build_expjac build_invjac build_invW build_invW_t build_hes build_invhes build_dpfuncs The core internal Users should only interact with this through the  ode_def  macros Options are self-explanatory by name mapping to  ODEFunction  build_tgrad build_jac build_expjac build_invjac build_invW build_invW_t build_hes build_invhes build_dpfuncs depvar  sets the symbol for the dependent variable Example"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.GeometricBrownianBridge","title":"GeometricBrownianBridge","text":""},{"doctype":"documentation","id":"references/SciMLOperators.has_ldiv!","title":"has_ldiv!","text":""},{"doctype":"documentation","id":"references/GlobalSensitivity.allsame","title":"allsame","text":""},{"doctype":"document","id":"DiffEqFlux/utilities/MultipleShooting.md","title":"Multiple Shooting Functionality","text":"Multiple Shooting Functionality"},{"doctype":"documentation","id":"references/SciMLBase.AbstractRODEProblem","title":"AbstractRODEProblem","text":"DocStringExtensions.TypeDefinition Base for types which define RODE problems"},{"doctype":"documentation","id":"references/SciMLBase.AbstractOptimizationSolution","title":"AbstractOptimizationSolution","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.get_power","title":"get_power","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.has_eqs","title":"has_eqs","text":""},{"doctype":"documentation","id":"references/LinearSolve.needs_concrete_A","title":"needs_concrete_A","text":""},{"doctype":"documentation","id":"references/PolyChaos.rm_meixner_pollaczek","title":"rm_meixner_pollaczek","text":"Creates  N  recurrence coefficients for monic Meixner-Pollaczek polynomials with parameters λ and ϕ These are orthogonal on  infty,\\infty  relative to the weight function  w(t)=(2 pi)^{-1 exp{(2 phi-\\pi)t Gamma(\\lambda i t)|^2  The call  rm_meixner_pollaczek(n,lambda  is the same as  rm_meixner_pollaczek(n,lambda,pi/2 "},{"doctype":"documentation","id":"references/MethodOfLines.isperiodic","title":"isperiodic","text":""},{"doctype":"documentation","id":"references/NeuralPDE.get_bounds","title":"get_bounds","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.REAL_INPLACE_WHITE_NOISE_BRIDGE","title":"REAL_INPLACE_WHITE_NOISE_BRIDGE","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.namespace_variables","title":"namespace_variables","text":""},{"doctype":"documentation","id":"references/Catalyst.run_graphviz","title":"run_graphviz","text":"Run a Graphviz program Invokes Graphviz through its command-line interface If the  Graphviz_jll  package is installed and loaded it is used otherwise Graphviz must be installed on the local system For bindings to the Graphviz C API see the the package  GraphViz.jl  At the time of this writing GraphViz.jl is unmaintained"},{"doctype":"document","id":"LinearSolve/tutorials/caching_interface.md","title":"Linear Solve with Caching Interface","text":"A b1 A b2 lu! A A b1 A b2 n A rand n n b1 rand n b2 rand n prob A b1 linsolve prob sol1 linsolve sol1 u linsolve sol1 cache b2 sol2 linsolve sol2 u A2 rand n n linsolve sol2 cache A2 sol3 linsolve sol3 u Linear Solve with Caching Interface In many cases one may want to cache information that is reused between different linear solves For example if one is going to perform then it would be more efficient to LU-factorize one time and reuse the factorization LinearSolve.jl's caching interface automates this process to use the most efficient means of solving and resolving linear systems To do this with LinearSolve.jl you simply  init  a cache  solve  replace  b  and solve again This looks like Then refactorization will occur when a new  A  is given The factorization occurs on the first solve and it stores the factorization in the cache You can retrieve this cache via  sol.cache  which is the same object as the  init  but updated to know not to re-solve the factorization The advantage of course with using LinearSolve.jl in this form is that it is efficient while being agnostic to the linear solver One can easily swap in iterative solvers sparse solvers etc and it will do all of the tricks like caching symbolic factorizations if the sparsity pattern is unchanged"},{"doctype":"documentation","id":"references/ModelingToolkit.has_continuous_events","title":"has_continuous_events","text":""},{"doctype":"documentation","id":"references/MethodOfLines.get_all_depvars","title":"get_all_depvars","text":""},{"doctype":"documentation","id":"references/PolyChaos.build_w_beta","title":"build_w_beta","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_domain","title":"get_domain","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.OrnsteinUhlenbeckProcess!","title":"OrnsteinUhlenbeckProcess!","text":"Θ μ σ t0 W0 Z0 nothing kwargs Θ μ σ t0 W0 Z0 nothing kwargs A  Ornstein-Uhlenbeck  process which is a Wiener process defined by the stochastic differential equation dX_t  theta mu  X_t dt  sigma dW_t The  OrnsteinUhlenbeckProcess  is distribution exact meaning not a numerical solution of the stochastic differential equation and instead follows the exact distribution properties The constructor is"},{"doctype":"documentation","id":"references/RecursiveArrayTools.ArrayPartition","title":"ArrayPartition","text":"x AbstractArray A y z An  ArrayPartition   A  is an array which is made up of different arrays  A.x  These index like a single array but each subarray may have a different type However broadcast is overloaded to loop in an efficient manner meaning that  A  2.+B  is type-stable in its computations even if  A.x[i  and  A.x[j  do not match types A full array interface is included for completeness which allows this array type to be used in place of a standard array where such a type stable broadcast may be needed One example is in heterogeneous differential equations for  DifferentialEquations.jl  An  ArrayPartition  acts like a single array  A[i  indexes through the first array then the second etc all linearly But  A.x  is where the arrays are stored Thus for we would have  A.x[1]==y  and  A.x[2]==z  Broadcasting like  f.(A  is efficient"},{"doctype":"documentation","id":"references/MethodOfLines.subsmatch","title":"subsmatch","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.default_p","title":"default_p","text":""},{"doctype":"documentation","id":"references/SciMLBase.isrecompile","title":"isrecompile","text":""},{"doctype":"documentation","id":"references/SciMLBase.NoRootFind","title":"NoRootFind","text":""},{"doctype":"document","id":"ModelingToolkit/index.md","title":"ModelingToolkit.jl: High-Performance Symbolic-Numeric Equation-Based Modeling","text":"Pkg Pkg add ModelingToolkit.jl High-Performance Symbolic-Numeric Equation-Based Modeling ModelingToolkit.jl is a modeling language for high-performance symbolic-numeric computation in scientific computing and scientific machine learning It then mixes ideas from symbolic computational algebra systems with causal and acausal equation-based modeling frameworks to give an extendable and parallel modeling system It allows for users to give a high-level description of a model for symbolic preprocessing to analyze and enhance the model Automatic transformations such as index reduction can be applied to the model before solving in order to make it easily handle equations would could not be solved when modeled without symbolic intervention Installation To install ModelingToolkit.jl use the Julia package manager Citation If you use ModelingToolkit in your work please cite the following Feature Summary ModelingToolkit.jl is a symbolic-numeric modeling package Thus it combines some of the features from symbolic computing packages like SymPy or Mathematica with the ideas of equation-based modeling systems like the causal Simulink and the acausal Modelica It bridges the gap between many different kinds of equations allowing one to quickly and easily transform systems of DAEs into optimization problems or vice-versa and then simplify and parallelize the resulting expressions before generating code Feature List Causal and acausal modeling Simulink/Modelica Automated model transformation simplification and composition Automatic conversion of numerical models into symbolic models Composition of models through the components a lazy connection system and tools for expanding/flattening Pervasive parallelism in symbolic computations and generated functions Transformations like alias elimination and tearing of nonlinear systems for efficiently numerically handling large-scale systems of equations The ability to use the entire Symbolics.jl Computer Algebra System CAS as part of the modeling process Import models from common formats like SBML CellML BioNetGen and more Extendability the whole system is written in pure Julia so adding new functions simplification rules and model transformations has no barrier For information on how to use the Symbolics.jl CAS system that ModelingToolkit.jl is built on consult the  Symbolics.jl documentation Equation Types Ordinary differential equations Stochastic differential equations Partial differential equations Nonlinear systems Optimization problems Continuous-Time Markov Chains Chemical Reactions via  Catalyst.jl  Nonlinear Optimal Control Standard Library For quick development ModelingToolkit.jl includes  ModelingToolkitStandardLibrary.jl  a standard library of prebuilt components for the ModelingToolkit ecosystem Model Import Formats CellMLToolkit.jl  Import  CellML  models into ModelingToolkit Repository of more than a thousand pre-made models Focus on biomedical models in areas such as Calcium Dynamics Cardiovascular Circulation Cell Cycle Cell Migration Circadian Rhythms Electrophysiology Endocrine Excitation-Contraction Coupling Gene Regulation Hepatology Immunology Ion Transport Mechanical Constitutive Laws Metabolism Myofilament Mechanics Neurobiology pH Regulation PKPD Protein Modules Signal Transduction and Synthetic Biology SBMLToolkit.jl  Import  SBML  models into ModelingToolkit Uses the robust libsbml library for parsing and transforming the SBML ReactionNetworkImporters.jl  Import various models into ModelingToolkit Supports the BioNetGen  net  file Supports importing networks specified by stoichiometric matrices Extension Libraries Because ModelingToolkit.jl is the core foundation of a equation-based modeling ecosystem there is a large set of libraries adding features to this system Below is an incomplete list of extension libraries one may want to be aware of Catalyst.jl  Symbolic representations of chemical reactions Symbolically build and represent large systems of chemical reactions Generate code for ODEs SDEs continuous-time Markov Chains and more Simulate the models using the SciML ecosystem with O(1 Gillespie methods DataDrivenDiffEq.jl  Automatic identification of equations from data Automated construction of ODEs and DAEs from data Representations of Koopman operators and Dynamic Mode Decomposition DMD MomentClosure.jl  Automatic transformation of ReactionSystems into deterministic systems Generates ODESystems for the moment closures Allows for geometrically-distributed random reaction rates ReactionMechanismSimulator.jl  Simulating and analyzing large chemical reaction mechanisms Ideal gas and dilute liquid phases Constant T and P and constant V adiabatic ideal gas reactors Constant T and V dilute liquid reactors Diffusion limited rates Sensitivity analysis for all reactors Flux diagrams with molecular images if molecular information is provided NumCME.jl  High-performance simulation of chemical master equations CME Transient solution of the CME Dynamic state spaces Accepts reaction systems defined using Catalyst.jl DSL FiniteStateProjection.jl  High-performance simulation of chemical master equations CME via finite state projections Accepts reaction systems defined using Catalyst.jl DSL Compatible Numerical Solvers All of the symbolic systems have a direct conversion to a numerical system which can then be handled through the SciML interfaces For example after building a model and performing symbolic manipulations an  ODESystem  can be converted into an  ODEProblem  to then be solved by a numerical ODE solver Below is a list of the solver libraries which are the numerical targets of the ModelingToolkit system DifferentialEquations.jl Multi-package interface of high performance numerical solvers for  ODESystem   SDESystem  and  JumpSystem NonlinearSolve.jl High performance numerical solving of  NonlinearSystem GalacticOptim.jl Multi-package interface for numerical solving  OptimizationSystem NeuralPDE.jl Physics-Informed Neural Network PINN training on  PDESystem MethodOfLines.jl Automated finite difference method FDM discretization of  PDESystem Contributing Please refer to the  SciML ColPrac Contributor's Guide on Collaborative Practices for Community Packages  for guidance on PRs issues and other matters relating to contributing to ModelingToolkit There are a few community forums The diffeq-bridged channel in the  Julia Slack JuliaDiffEq  on Gitter On the Julia Discourse forums look for the  modelingtoolkit tag See also  SciML Community page"},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.componentwise_mean","title":"componentwise_mean","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.integrate","title":"integrate","text":""},{"doctype":"documentation","id":"references/SciMLBase.DynamicalSDEFunction","title":"DynamicalSDEFunction","text":"iip recompile f1 f2 mass_matrix I analytic nothing tgrad nothing jac nothing jvp nothing vjp nothing ggprime nothing jac_prototype nothing sparsity jac_prototype paramjac nothing syms nothing indepsym nothing colorvec nothing DynamicalSDEFunction  AbstractSDEFunction A representation of an SDE function  f  and  g  defined by M du  f(u,p,t dt  g(u,p,t dW_t as a partitioned ODE M_1 du  f_1(u,p,t dt  g(u,p,t dW_t\nM_2 du  f_2(u,p,t dt  g(u,p,t dW_t and all of its related functions such as the Jacobian of  f  its gradient with respect to time and more For all cases  u0  is the initial condition  p  are the parameters and  t  is the independent variable Constructor Note that only the functions  f_i  themselves are required These functions should be given as  f_i!(du,u,p,t  or  du  f_i(u,p,t  See the section on  iip  for more details on in-place vs out-of-place handling All of the remaining functions are optional for improving or accelerating  the usage of  f  These include mass_matrix  the mass matrix  M_i  represented in the ODE function Can be used to determine that the equation is actually a differential-algebraic equation DAE if  M  is singular Note that in this case special solvers are required see the DAE solver page for more details https://diffeq.sciml.ai/stable/solvers/dae*solve Must be an AbstractArray or an AbstractSciMLOperator Should be given as a tuple of mass matrices i.e  M*1 M_2  for the mass matrices of equations 1 and 2 respectively analytic(u0,p,t  used to pass an analytical solution function for the analytical  solution of the ODE Generally only used for testing and development of the solvers tgrad(dT,u,p,t  or dT=tgrad(u,p,t returns  frac{\\partial f(u,p,t)}{\\partial t jac(J,u,p,t  or  J=jac(u,p,t  returns  frac{df}{du jvp(Jv,v,u,p,t  or  Jv=jvp(v,u,p,t  returns the directional derivative frac{df}{du v vjp(Jv,v,u,p,t  or  Jv=vjp(v,u,p,t  returns the adjoint derivative frac{df}{du}^\\ast v ggprime(J,u,p,t  or  J  ggprime(u,p,t  returns the Milstein derivative   frac{dg(u,p,t)}{du g(u,p,t jac_prototype  a prototype matrix matching the type that matches the Jacobian For example if the Jacobian is tridiagonal then an appropriately sized  Tridiagonal  matrix can be used as the prototype and integrators will specialize on this structure where possible Non-structured sparsity patterns should use a  SparseMatrixCSC  with a correct sparsity pattern for the Jacobian The default is  nothing  which means a dense Jacobian paramjac(pJ,u,p,t  returns the parameter Jacobian  frac{df}{dp  syms  the symbol names for the elements of the equation This should match  u0  in size For example if  u0  0.0,1.0  and  syms  x y  this will apply a canonical naming to the values allowing  sol[:x  in the solution and automatically naming values in plots indepsym  the canonical naming for the independent variable Defaults to nothing which internally uses  t  as the representation in any plots colorvec  a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the  jac_prototype  This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern Defaults to  nothing  which means a color vector will be internally computed on demand when required The cost of this operation is highly dependent on the sparsity pattern iip In-Place vs Out-Of-Place For more details on this argument see the ODEFunction documentation recompile Controlling Compilation and Specialization For more details on this argument see the ODEFunction documentation Fields The fields of the DynamicalSDEFunction type directly match the names of the inputs"},{"doctype":"documentation","id":"references/GlobalSensitivity.gsa_efast_all_y_analysis","title":"gsa_efast_all_y_analysis","text":""},{"doctype":"documentation","id":"references/Surrogates.I_tier_ranking_1D","title":"I_tier_ranking_1D","text":""},{"doctype":"documentation","id":"references/NeuralPDE.transform_expression","title":"transform_expression","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.DiCMOBiGraph","title":"DiCMOBiGraph","text":"This data structure implements a directed contracted matching-oriented view of an original undirected bipartite graph It has two modes depending on the  Transposed  flag which switches the direction of the induced matching Essentially the graph adapter performs two largely orthogonal functions  Transposed  true  differences are indicated in square brackets It pairs an undirected bipartite graph with a matching of the destination vertex This matching is used to induce an orientation on the otherwise undirected graph Matched edges pass from destination to source source to desination all other edges pass in the opposite direction It exposes the graph view obtained by contracting the destination source vertices along the matched edges The result of this operation is an induced directed graph on the source destination vertices The resulting graph has a few desirable properties In particular this graph is acyclic if and only if the induced directed graph on the original bipartite graph is acyclic Hypergraph interpretation Consider the bipartite graph  B  as the incidence graph of some hypergraph  H  Note that a maching  M  on  B  in the above sense is equivalent to determining an 1,n)-orientation on the hypergraph i.e each directed hyperedge has exactly one head but any arbitrary number of tails In this setting this is simply the graph formed by expanding each directed hyperedge into  n  ordinary edges between the same vertices"},{"doctype":"documentation","id":"references/ExponentialUtilities.exp_generic","title":"exp_generic","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractDynamicalSDEProblem","title":"AbstractDynamicalSDEProblem","text":"DocStringExtensions.TypeDefinition"},{"doctype":"document","id":"Surrogates/kriging.md","title":"kriging","text":"Kriging surrogate tutorial 1D Kriging or Gaussian process regression is a method of interpolation for which the interpolated values are modeled by a Gaussian process We are going to use a Kriging surrogate to optimize  f(x)=(6x-2)^2sin(12x-4  function from Forrester et al 2008 First of all import  Surrogates  and  Plots  Sampling We choose to sample f in 4 points between 0 and 1 using the  sample  function The sampling points are chosen using a Sobol sequence this can be done by passing  SobolSample  to the  sample  function Building a surrogate With our sampled points we can build the Kriging surrogate using the  Kriging  function kriging_surrogate  behaves like an ordinary function which we can simply plot A nice statistical property of this surrogate is being able to calculate the error of the function at each point we plot this as a confidence interval using the  ribbon  argument Optimizing Having built a surrogate we can now use it to search for minimas in our original function  f  To optimize using our surrogate we call  surrogate_optimize  method We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique Kriging surrogate tutorial ND First of all let's define the function we are going to build a surrogate for Notice how its argument is a vector of numbers one for each coordinate and its output is a scalar Sampling Let's define our bounds this time we are working in two dimensions In particular we want our first dimension  x  to have bounds  5 10  and  0 15  for the second dimension We are taking 50 samples of the space using Sobol Sequences We then evaluate our function on all of the sampling points Building a surrogate Using the sampled points we build the surrogate the steps are analogous to the 1-dimensional case Optimizing With our surrogate we can now search for the minimas of the branin function Notice how the new sampled points which were created during the optimization process are appended to the  xys  array This is why its size changes"},{"doctype":"documentation","id":"references/SciMLBase.EnsembleSolution","title":"EnsembleSolution","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/DiffEqSensitivity.AbstractSecondOrderSensitivityAlgorithm","title":"AbstractSecondOrderSensitivityAlgorithm","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.AbstractConnectType","title":"AbstractConnectType","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.ForwardDiffSensitivity","title":"ForwardDiffSensitivity","text":"chunk_size nothing ForwardDiffSensitivity  AbstractForwardSensitivityAlgorithm An implementation of discrete forward sensitivity analysis through ForwardDiff.jl When used within adjoint differentiation i.e via Zygote this will cause forward differentiation of the  solve  call within the reverse-mode automatic differentiation environment Constructor Keyword Arguments chunk_size  the chunk size used by ForwardDiff for computing the Jacobian i.e the number of simultaneous columns computed convert_tspan  whether to convert time to also be  Dual  valued By default this is  nothing  which will only convert if callbacks are found Conversion is required in order to accurately differentiate callbacks hybrid equations SciMLProblem Support This  sensealg  supports any  SciMLProblem s provided that the solver algorithms is  SciMLBase.isautodifferentiable  Note that  ForwardDiffSensitivity  can accurately differentiate code with callbacks only when  convert_tspan=true "},{"doctype":"documentation","id":"references/SciMLBase.DEFAULT_PLOT_FUNC","title":"DEFAULT_PLOT_FUNC","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.find_roots","title":"find_roots","text":""},{"doctype":"documentation","id":"references/SciMLBase.has_analytic","title":"has_analytic","text":""},{"doctype":"document","id":"DiffEqSensitivity/index.md","title":"DiffEqSensitivity: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"prob args sensealg checkpoints sol t kwargs DiffEqSensitivity Automatic Differentiation and Adjoints for Differential Equation Solvers DiffEqSensitivity.jl is the automatic differentiation and adjoints system for the SciML ecosystem Also known as local sensitivity analysis these methods allow for calculation of fast derivatives of SciML problem types which are commonly used to analyze model sensitivities callibrate models to data train neural ODEs perform automated model discovery via universal differential equations and more SciMLSensitivity.jl is a high level interface that pulls together all of the tools with heuristics and helper functions to make solving inverse problems and inferring models as easy as possible without losing efficiency Thus what DiffEqSensitivity.jl provides is Automatic differentiation overloads for improving the performance and flexibility of AD calls over  solve  A bunch of tutorials documentation and test cases for this combination with parameter estimation data fitting  model calibration neural network libraries and GPUs Note This documentation assumes familiarity with the solver packages for the respective problem types If one is not familiar with the solver packages please consult the documentation for pieces like  DifferentialEquations.jl   NonlinearSolve.jl   LinearSolve.jl  etc first High Level Interface  sensealg The highest level interface is provided by the function  solve  solve  is fully compatible with automatic differentiation libraries like Zygote.jl ReverseDiff.jl Tracker.jl ForwardDiff.jl and will automatically replace any calculations of the solution's derivative with a fast method The keyword argument  sensealg  controls the dispatch to the  AbstractSensitivityAlgorithm  used for the sensitivity calculation Note that  solve  in an AD context does not allow higher order interpolations unless  sensealg=DiffEqBase.SensitivityADPassThrough  is used i.e going back to the AD mechanism Note The behavior of ForwardDiff.jl is different from the other automatic differentiation libraries mentioned above The  sensealg  keyword is ignored Instead the differential equations are solved using  Dual  numbers for  u0  and  p  If only  p  is perturbed in the sensitivity analysis but not  u0  the state is still implemented as a  Dual  number ForwardDiff.jl will thus not dispatch into continuous forward nor adjoint sensitivity analysis even if a  sensealg  is provided Equation Scope SciMLSensitivity.jl supports all of the equation types of the  SciML Common Interface  extending the problem types by adding overloads for automatic differentiation to improve the performance and flexibility of the differentiation system This includes Linear systems  LinearProblem  Direct methods for dense and sparse Iterative solvers with preconditioning Nonlinear Systems  NonlinearProblem  Systems of nonlinear equations Scalar bracketing systems Integrals quadrature  QuadratureProblem  Differential Equations Discrete equations function maps discrete stochastic Gillespie/Markov simulations  DiscreteProblem  Ordinary differential equations ODEs  ODEProblem  Split and Partitioned ODEs Symplectic integrators IMEX Methods  SplitODEProblem  Stochastic ordinary differential equations SODEs or SDEs  SDEProblem  Stochastic differential-algebraic equations SDAEs  SDEProblem  with mass matrices Random differential equations RODEs or RDEs  RODEProblem  Differential algebraic equations DAEs  DAEProblem  and  ODEProblem  with mass matrices Delay differential equations DDEs  DDEProblem  Neutral retarded and algebraic delay differential equations NDDEs RDDEs and DDAEs Stochastic delay differential equations SDDEs  SDDEProblem  Experimental support for stochastic neutral retarded and algebraic delay differential equations SNDDEs SRDDEs and SDDAEs Mixed discrete and continuous equations Hybrid Equations Jump Diffusions  DEProblem s with callbacks Optimization  OptimizationProblem  Nonlinear constrained optimization Stochastic/Delay/Differential-Algebraic Partial Differential Equations  PDESystem  Finite difference and finite volume methods Interfaces to finite element methods Physics-Informed Neural Networks PINNs Integro-Differential Equations Fractional Differential Equations SciMLSensitivity and Universal Differential Equations SciMLSensitivity is for universal differential equations where these can include delays physical constraints stochasticity events and all other kinds of interesting behavior that shows up in scientific simulations Neural networks can be all or part of the model They can be around the differential equation in the cost function or inside of the differential equation Neural networks representing unknown portions of the model or functions can go anywhere you have uncertainty in the form of the scientific simulator Forward sensitivity and adjoint equations are automatically generated with checkpointing and stabilization to ensure it works for large stiff equations while specializations on static objects allows for high efficiency on small equations For an overview of the topic with applications consult the paper  Universal Differential Equations for Scientific Machine   Learning  You can efficiently use the package for Parameter estimation of scientific models ODEs SDEs DDEs DAEs etc Neural ODEs Neural SDE Neural DAEs Neural DDEs etc Nonlinear optimal control including training neural controllers Stiff universal ordinary differential equations universal ODEs Universal stochastic differential equations universal SDEs Universal delay differential equations universal DDEs Universal partial differential equations universal PDEs Universal jump stochastic differential equations universal jump diffusions Hybrid universal differential equations universal DEs with event handling with high order adaptive implicit GPU-accelerated Newton-Krylov etc methods For examples please refer to  the DiffEqFlux release blog   post  which we try to keep updated for changes to the libraries Additional demonstrations like neural PDEs and neural jump SDEs can be found  at this blog   post  among many others All of these features are only part of the advantage as this library routinely benchmarks orders of magnitude faster than competing libraries like torchdiffeq  Benchmarks Use with GPUs is highly optimized by  recompiling the solvers to GPUs to remove all CPU-GPU data transfers  while use with CPUs uses specialized kernels for accelerating differential equation solves Many different training techniques are supported by this package including Optimize-then-discretize backsolve adjoints checkpointed adjoints quadrature adjoints Discretize-then-optimize forward and reverse mode discrete sensitivity analysis This is a generalization of  ANODE  and  ANODEv2  to all  DifferentialEquations.jl ODE solvers Hybrid approaches adaptive time stepping  AD for adaptive discretize-then-optimize O(1 memory backprop of ODEs via BacksolveAdjoint and Virtual Brownian Trees for O(1 backprop of SDEs Continuous adjoints for integral loss functions  continuous_loss Probabilistic programming and variational inference on ODEs/SDEs/DAEs/DDEs/hybrid equations etc is provided by integration with  Turing.jl  and  Gen.jl  Reproduce  variational loss functions  by plugging  composible libraries together  all while mixing forward mode and reverse mode approaches as appropriate for the most speed For more details on the adjoint sensitivity analysis methods for computing fast gradients see the adjoints details page  sensitivity_diffeq With this package you can explore various ways to integrate the two methodologies Neural networks can be defined where the “activations” are nonlinear functions described by differential equations Neural networks can be defined where some layers are ODE solves ODEs can be defined where some terms are neural networks Cost functions on ODEs can define neural networks Note on Modularity and Composability with Solvers Note that DiffEqSensitivity.jl purely built on composable and modular infrastructure DiffEqSensitivity provides high level helper functions and documentation for the user but the code generation stack is modular and composes in many different ways For example one can use and swap out the ODE solver between any common interface compatible library like Sundials.jl OrdinaryDiffEq.jl LSODA.jl IRKGaussLegendre.jl SciPyDiffEq.jl  etc many other choices In addition due to the composability of the system none of the components are directly tied to the Flux.jl machine learning framework For example you can  use DiffEqSensitivity.jl   to generate TensorFlow graphs and train the neural network with TensorFlow.jl   use PyTorch arrays via Torch.jl  and more all with single line code changes by utilizing the underlying code generation The tutorials shown here are thus mostly a guide on how to use the ecosystem as a whole only showing a small snippet of the possible ways to compose the thousands of differentiable libraries together Swap out ODEs for SDEs DDEs DAEs etc put quadrature libraries or  Tullio.jl  in the loss function the world is your oyster As a proof of composability note that the implementation of Bayesian neural ODEs required zero code changes to the library and instead just relied on the composability with other Julia packages Citation If you use DiffEqSensitivity.jl or are influenced by its ideas please cite"},{"doctype":"documentation","id":"references/Catalyst.get_reaction","title":"get_reaction","text":""},{"doctype":"documentation","id":"references/Surrogates.WendlandStructure","title":"WendlandStructure","text":""},{"doctype":"documentation","id":"references/MethodOfLines.AbstractTruncatingBoundary","title":"AbstractTruncatingBoundary","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.make_operation","title":"make_operation","text":""},{"doctype":"documentation","id":"references/NeuralOperators.truncate_modes","title":"truncate_modes","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_var_to_name","title":"get_var_to_name","text":""},{"doctype":"documentation","id":"references/SciMLBase.has_mul!","title":"has_mul!","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.SilvermanKernel","title":"SilvermanKernel","text":""},{"doctype":"documentation","id":"references/Surrogates.NeuralStructure","title":"NeuralStructure","text":""},{"doctype":"documentation","id":"references/Optimization._check_and_convert_maxiters","title":"_check_and_convert_maxiters","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.search_VBT!","title":"search_VBT!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.dummy_derivative_graph!","title":"dummy_derivative_graph!","text":""},{"doctype":"documentation","id":"references/SciMLBase.@CSI_str","title":"@CSI_str","text":"Create an ANSI escape sequence string for the CSI command  cmd "},{"doctype":"documentation","id":"references/DiffEqSensitivity.E!","title":"E!","text":""},{"doctype":"documentation","id":"references/SciMLBase.solution_new_retcode","title":"solution_new_retcode","text":""},{"doctype":"documentation","id":"references/RecursiveArrayTools.unpack_voa","title":"unpack_voa","text":""},{"doctype":"documentation","id":"references/NeuralPDE.KeywordArgSilent","title":"KeywordArgSilent","text":""},{"doctype":"document","id":"Surrogates/tutorials.md","title":"tutorials","text":"Surrogates 101 Let's start with something easy to get our hands dirty I want to build a surrogate for  f(x  log(x cdot x^2+x^3  Let's choose the radial basis surrogate Let's now see an example in 2D Kriging standard error Let's now use the Kriging surrogate which is a single-output Gaussian process This surrogate has a nice feature not only does it approximate the solution at a point it also calculates the standard error at such point Let's see an example Let's now optimize the Kriging surrogate using Lower confidence bound method this is just a one-liner Surrogate optimization methods have two purposes they both sample the space in unknown regions and look for the minima at the same time Lobachevsky integral The Lobachevsky surrogate has the nice feature of having a closed formula for its integral which is something that other surrogates are missing Let's compare it with QuadGK Example of NeuralSurrogate Basic example of fitting a neural network on a simple function of two variables"},{"doctype":"documentation","id":"references/DiffEqFlux.NeuralODEMM","title":"NeuralODEMM","text":"model constraints_model tspan mass_matrix alg nothing args kwargs model tspan mass_matrix alg nothing args sensealg autojacvec kwargs Constructs a physically-constrained continuous-time recurrant neural network also known as a neural differential-algebraic equation neural DAE with a mass matrix and a fast gradient calculation via adjoints 1 The mass matrix formulation is Mu  f(u,p,t where  M  is semi-explicit i.e singular with zeros for rows corresponding to the constraint equations Arguments model  A Chain or FastChain neural network that defines the ̇ f(u,p,t constraints_model  A function  constraints_model(u,p,t  for the fixed constaints to impose on the algebraic equations tspan  The timespan to be solved on mass_matrix  The mass matrix associated with the DAE alg  The algorithm used to solve the ODE Defaults to  nothing  i.e the default algorithm from DifferentialEquations.jl This method requires an implicit ODE solver compatible with singular mass matrices Consult the  DAE solvers  documentation for more details sensealg  The choice of differentiation algorthm used in the backpropogation Defaults to an adjoint method and with  FastChain  it defaults to utilizing a tape-compiled ReverseDiff vector-Jacobian product for extra efficiency Seee the  Local Sensitivity Analysis  documentation for more details kwargs  Additional arguments splatted to the ODE solver See the  Common Solver Arguments  documentation for more details"},{"doctype":"document","id":"MethodOfLines/tutorials/params.md","title":"Adding parameters","text":"Adding parameters We can also build up more complicated systems with multiple dependent variables and parameters as follows Remake with different parameter values The system does not need to be re-discretized every time we want to plot with different parameters the system can be remade with new parameters with  remake  See the  ModelingToolkit.jl   docs  for more ways to manipulate a  prob  post discretization"},{"doctype":"documentation","id":"references/DiffEqSensitivity._setup_reverse_callbacks","title":"_setup_reverse_callbacks","text":""},{"doctype":"documentation","id":"references/RecursiveArrayTools.common_length","title":"common_length","text":""},{"doctype":"documentation","id":"references/Surrogates.II_tier_ranking_1D","title":"II_tier_ranking_1D","text":""},{"doctype":"documentation","id":"references/SciMLBase.DECallback","title":"DECallback","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.NoiseGrid","title":"NoiseGrid","text":"t W Z nothing reset dt t dt brownian_values cumsum sqrt dt randn i length t W t brownian_values A noise grid builds a noise process from arrays of points For example you can generate your desired noise process as an array  W  with timepoints  t  and use the constructor to build the associated noise process This process comes with a linear interpolation of the given points and thus the grid does not have to match the grid of integration Thus this can be used for adaptive solutions as well However one must make note that the fidelity of the noise process is linked to how fine the noise grid is determined if the noise grid is sparse on points compared to the integration then its distributional properties may be slightly perturbed by the linear interpolation Thus its suggested that the grid size at least approximately match the number of time steps in the integration to ensure accuracy For a one-dimensional process  W  should be an  AbstractVector  of  Number s For multi-dimensional processes  W  should be an  AbstractVector  of the  noise_prototype  NoiseGrid In this example we will show you how to define your own version of Brownian motion using an array of pre-calculated points In normal usage you should use  WienerProcess  instead since this will have distributionally-exact interpolations while the noise grid uses linear interpolations but this is a nice example of the workflow To define a  NoiseGrid  you need to have a set of time points and a set of values for the process Let's define a Brownian motion on  0.0,1.0  with a  dt=0.001  To do this Now we build the  NoiseGrid  using these values We can then pass  W  as the  noise  argument of an  SDEProblem  to use it in an SDE"},{"doctype":"document","id":"Surrogates/samples.md","title":"Samples","text":"Samples Sampling methods are provided by the  QuasiMonteCarlo package  The syntax for sampling in an interval or region is the following where lb and ub are respectively the lower and upper bounds There are many sampling algorithms to choose from Grid sample Uniform sample Sobol sample Latin Hypercube sample Low Discrepancy sample Sample on section Adding a new sampling method Adding a new sampling method is a two step process Adding a new SamplingAlgorithm type Overloading the sample function with the new type Example"},{"doctype":"documentation","id":"references/ModelingToolkit.connector_type","title":"connector_type","text":""},{"doctype":"documentation","id":"references/QuasiMonteCarlo.UniformSample","title":"UniformSample","text":"Uniformly distributed random numbers"},{"doctype":"documentation","id":"references/PolyChaos._checkKind","title":"_checkKind","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.loss_correction!","title":"loss_correction!","text":""},{"doctype":"documentation","id":"references/SciMLBase.check_keywords","title":"check_keywords","text":""},{"doctype":"document","id":"DiffEqSensitivity/ode_fitting/second_order_adjoints.md","title":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","text":"Lux OptimizationFlux DifferentialEquations Plots Random rng Random default_rng u0 Float32 datasize tspan tsteps range tspan tspan length datasize trueODEfunc du u p t true_A du u true_A prob_trueode trueODEfunc u0 tspan ode_data Array prob_trueode Tsit5 saveat tsteps dudt2 Lux ActivationFunction x x Lux Dense tanh Lux Dense prob_neuralode dudt2 tspan Tsit5 saveat tsteps p st Lux setup rng dudt2 predict_neuralode p Array prob_neuralode u0 p st loss_neuralode p pred predict_neuralode p loss sum abs2 ode_data pred loss pred list_plots iter cb p l pred doplot list_plots iter iter list_plots iter display l plt scatter tsteps ode_data label scatter! plt tsteps pred label push! list_plots plt doplot display plot plt l adtype optf x p loss_neuralode x adtype optprob1 optf Lux ComponentArray p pstart optprob1 ADAM cb cb maxiters u optprob2 optf pstart pmin optprob2 NewtonTrustRegion cb cb maxiters pmin optprob2 Optim KrylovTrustRegion cb cb maxiters Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis In many cases it may be more optimal or more stable to fit using second order Newton-based optimization techniques Since DiffEqSensitivity.jl provides second order sensitivity analysis for fast Hessians and Hessian-vector products via forward-over-reverse we can utilize these in our neural/universal differential equation training processes sciml_train  is setup to automatically use second order sensitivity analysis methods if a second order optimizer is requested via Optim.jl Thus  Newton  and  NewtonTrustRegion  optimizers will use a second order Hessian-based optimization while  KrylovTrustRegion  will utilize a Krylov-based method with Hessian-vector products never forming the Hessian for large parameter optimizations Note that we do not demonstrate  Newton  because we have not found a single case where it is competitive with the other two methods  KrylovTrustRegion  is generally the fastest due to its use of Hessian-vector products"},{"doctype":"documentation","id":"references/NeuralOperators.pad_modes!","title":"pad_modes!","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities._phiv_timestep_estimate_flops","title":"_phiv_timestep_estimate_flops","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.wE!","title":"wE!","text":""},{"doctype":"documentation","id":"references/Catalyst.modifystrcomp","title":"modifystrcomp","text":""},{"doctype":"documentation","id":"references/SciMLBase.StandardODEProblem","title":"StandardODEProblem","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/DiffEqFlux.sciml_train","title":"sciml_train","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.cartesian_to_linear","title":"cartesian_to_linear","text":""},{"doctype":"documentation","id":"references/Catalyst.ReactantStruct","title":"ReactantStruct","text":""},{"doctype":"documentation","id":"references/QuasiMonteCarlo.sample","title":"sample","text":"A n lb ub sample_method where n  is the number of points to sample lb  is the lower bound for each variable The length determines the dimensionality ub  is the upper bound sample_method  is the quasi-Monte Carlo sampling strategy Note that any Distributions.jl distribution can be used in addition to any  SamplingAlgorithm  sample(n,lb,ub,S::GridSample Returns a tuple containing numbers in a grid sample(n,lb,ub,::UniformRandom Returns a tuple containing uniform random numbers sample(n,lb,ub,::SobolSampling Returns a tuple containing Sobol sequences sample(n,lb,ub,T::LatinHypercubeSample Returns a tuple containing LatinHypercube sequences sample(n,lb,ub,::LatticeRuleSample Returns a matrix with the  n  rank-1 lattice points in each column if  lb  is a vector or a vector with the  n  rank-1 lattice points if  lb  is a number sample(n,lb,ub,S::LowDiscrepancySample Low-discrepancy sample Dimension 1 Van der Corput sequence Dimension  1 Halton sequence If dimension d  1 all bases must be coprime with one other sample(n,d,K::KroneckerSample Returns a Tuple containing numbers following the Kronecker sample sample(n,lb,ub,K::SectionSample Returns Tuples constrained to a section In surrogate-based identification and control optimization can alternate between unconstrained sampling in the full-dimensional parameter space and sampling constrained on specific sections e.g a planes in a 3D volume A SectionSampler allows sampling and optimizing on a subset of free dimensions while keeping fixed ones constrained The sampler is defined as in e.g section_sampler_y_is_10  SectionSample([NaN64 NaN64 10.0 10.0 UniformSample where the first argument is a Vector in which numbers are fixed coordinates and  NaN s correspond to free dimensions and the second argument is a SamplingAlgorithm which is used to sample in the free dimensions sample(n,d,D::Distribution Returns a tuple containing numbers distributed as D"},{"doctype":"documentation","id":"references/SciMLBase.parameterless_type","title":"parameterless_type","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.substitute_expr!","title":"substitute_expr!","text":""},{"doctype":"document","id":"GlobalSensitivity/methods/efast.md","title":"eFAST Method","text":"num_harmonics Int ishi X A B sin X A sin X B X sin X lb ones π ub ones π res1 ishi lb i ub i i n ishi_batch X A B sin X A sin X B X sin X res2 ishi_batch lb i ub i i n batch eFAST Method The  eFAST  object has  num_harmonics  as the only field which is the number of harmonics to sum in the Fourier series decomposition this defaults to 4 eFAST Method Details eFAST offers a robust especially at low sample size and computationally efficient procedure to get the first and total order indices as discussed in Sobol It utilizes monodimensional Fourier decomposition along a curve exploring the parameter space The curve is defined by a set of parametric equations x_{i}(s  G_{i}(sin ω_{i}s ∀ i=1,2  n where s is a scalar variable varying over the range  ∞  s  ∞   G_{i  are transformation functions and  ω_{i ∀ i=1,2,...,n  is a set of different angular frequencies to be properly selected associated with each factor For more details on the transformation used and other implementation details you can go through   A Saltelli et al  API Example Below we show use of  eFAST  on the Ishigami function"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.BrownianBridge","title":"BrownianBridge","text":"t0 tend W0 Wend Z0 nothing Zend nothing kwargs t0 tend W0 Wend Z0 nothing Zend nothing kwargs A  BrownianBridge  process is a Wiener process with a pre-defined start and end value This process is distribution exact and back be back interpolated exactly as well The constructor is where  W(t0)=W₀   W(tend)=Wend  and likewise for the  Z  process if defined"},{"doctype":"documentation","id":"references/SciMLBase.AbstractSensitivityAlgorithm","title":"AbstractSensitivityAlgorithm","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/SciMLBase.AbstractDiffEqCompositeOperator","title":"AbstractDiffEqCompositeOperator","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/RecursiveArrayTools.AllObserved","title":"AllObserved","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs._edges","title":"_edges","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.uneven_invmap","title":"uneven_invmap","text":"returns an uneven inv map with length  n "},{"doctype":"documentation","id":"references/Catalyst.numreactions","title":"numreactions","text":"Return the total number of reactions within the given  ReactionSystem  and subsystems that are  ReactionSystem s"},{"doctype":"documentation","id":"references/SciMLOperators._vec","title":"_vec","text":""},{"doctype":"documentation","id":"references/SciMLBase.postamble!","title":"postamble!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.generate_gradient","title":"generate_gradient","text":"sys dvs sys ps sys expression Val kwargs Generates a function for the gradient of a system Extra arguments control the arguments to the internal  build_function  call"},{"doctype":"documentation","id":"references/DiffEqFlux.TriweightKernel","title":"TriweightKernel","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.RealWienerProcess!","title":"RealWienerProcess!","text":"t0 W0 Z0 nothing kwargs t0 W0 Z0 nothing kwargs The  RealWienerProcess  is a Brownian motion that is forced to be real-valued While the normal  WienerProcess  becomes complex valued if  W0  is complex this verion is real valued for when you want to for example solve an SDE defined by complex numbers where the noise is in the reals"},{"doctype":"documentation","id":"references/Integrals.transform_inf","title":"transform_inf","text":""},{"doctype":"documentation","id":"references/ParameterizedFunctions.@ode_def_bare","title":"@ode_def_bare","text":"Like  ode_def  but the  opts  options are set so that no symbolic functions are generated See the  ode_def  docstring for more details"},{"doctype":"documentation","id":"references/DiffEqSensitivity._pass","title":"_pass","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.ConnectionSet","title":"ConnectionSet","text":""},{"doctype":"documentation","id":"references/PolyChaos.computeTensorizedSP","title":"computeTensorizedSP","text":""},{"doctype":"documentation","id":"references/ParameterizedFunctions.modelingtoolkitize_expr","title":"modelingtoolkitize_expr","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractDiffEqOperator","title":"AbstractDiffEqOperator","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/ModelingToolkit.ExtraEquationsSystemException","title":"ExtraEquationsSystemException","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractDiscretization","title":"AbstractDiscretization","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.partial_state_selection_graph!","title":"partial_state_selection_graph!","text":""},{"doctype":"documentation","id":"references/NeuralOperators.ispermuted","title":"ispermuted","text":""},{"doctype":"document","id":"SymbolicNumericIntegration/symbolicnumericintegration.md","title":"API","text":"API"},{"doctype":"documentation","id":"references/SciMLBase.AbstractDAEIntegrator","title":"AbstractDAEIntegrator","text":"DocStringExtensions.TypeDefinition"},{"doctype":"document","id":"Surrogates/index.md","title":"Overview","text":"SurrogatesLogo Overview A surrogate model is an approximation method that mimics the behavior of a computationally expensive simulation In more mathematical terms suppose we are attempting to optimize a function   f(p  but each calculation of   f  is very expensive It may be the case that we need to solve a PDE for each point or use advanced numerical linear algebra machinery which is usually costly The idea is then to develop a surrogate model   g  which approximates   f  by training on previous data collected from evaluations of   f  The construction of a surrogate model can be seen as a three-step process Sample selection Construction of the surrogate model Surrogate optimization The sampling methods are super important for the behavior of the Surrogate At the moment they are Grid sample Uniform sample Sobol sample Latin Hypercube sample Low discrepancy sample The available surrogates are Linear Radial Basis Kriging Custom Kriging provided with Stheno Neural Network Support Vector Machine Random Forest Second Order Polynomial Inverse Distance After the surrogate is built we need to optimize it with respect to some objective function That is simultaneously looking for a minimum  and  sampling the most unknown region The available optimization methods are Stochastic RBF SRBF Lower confidence bound strategy LCBS Expected improvement EI Dynamic coordinate search DYCORS Multi-output Surrogates In certain situations the function being modeled may have a multi-dimensional output space In such a case the surrogate models can take advantage of correlations between the observed output variables to obtain more accurate predictions When constructing the original surrogate each element of the passed  y  vector should itself be a vector For example the following  y  are all valid Currently the following are implemented as multi-output surrogates Radial Basis Neural Network via Flux Second Order Polynomial Inverse Distance Custom Kriging via Stheno Gradients The surrogates implemented here are all automatically differentiable via Zygote Because of this property surrogates are useful models for processes which aren't explicitly differentiable and can be used as layers in for instance Flux models Installation Surrogates is registered in the Julia General Registry In the REPL You can obtain the current master with Quick example"},{"doctype":"documentation","id":"references/SciMLBase.isdiscrete","title":"isdiscrete","text":"isdiscrete(alg::DEAlgorithm Trait declaration for whether an algorithm allows for discrete state values such as integers Defaults to false"},{"doctype":"documentation","id":"references/DiffEqFlux.multiple_shoot","title":"multiple_shoot","text":"p ode_data tsteps prob loss_function solver group_size continuity_term kwargs p ode_data tsteps prob loss_function continuity_loss solver group_size continuity_term kwargs p ode_data_ensemble tsteps ensemble_prob ensemble_alg loss_function solver group_size continuity_term trajectories p ode_data_ensemble tsteps ensemble_prob ensemble_alg loss_function continuity_loss solver group_size continuity_term trajectories Returns a total loss after trying a Direct multiple shooting on ODE data and an array of predictions from each of the groups smaller intervals In Direct Multiple Shooting the Neural Network divides the interval into smaller intervals and solves for them separately The default continuity term is 100 implying any losses arising from the non-continuity of 2 different groups will be scaled by 100 Arguments p  The parameters of the Neural Network to be trained ode_data  Original Data to be modelled tsteps  Timesteps on which ode_data was calculated prob  ODE problem that the Neural Network attempts to solve loss_function  Any arbitrary function to calculate loss continuity_loss  Function that takes states  hat{u}_{end  of group  k  and  u_{0  of group  k+1  as input and calculates prediction continuity loss between them If no custom  continuity_loss  is specified  sum(abs û_end  u_0  is used solver  ODE Solver algorithm group_size  The group size achieved after splitting the ode_data into equal sizes continuity_term  Weight term to ensure continuity of predictions throughout different groups kwargs  Additional arguments splatted to the ODE solver Refer to the  Local Sensitivity Analysis  and  Common Solver Arguments  documentation for more details Note The parameter continuity_term should be a relatively big number to enforce a large penalty whenever the last point of any group doesn't coincide with the first point of next group Returns a total loss after trying a Direct multiple shooting on ODE data and an array of predictions from each of the groups smaller intervals In Direct Multiple Shooting the Neural Network divides the interval into smaller intervals and solves for them separately The default continuity term is 100 implying any losses arising from the non-continuity of 2 different groups will be scaled by 100 Arguments p  The parameters of the Neural Network to be trained ode_data_ensemble  Original Data to be modelled Batches or equivalently trajectories are located in the third dimension tsteps  Timesteps on which  ode_data_ensemble  was calculated ensemble_prob  Ensemble problem that the Neural Network attempts to solve ensemble_alg  Ensemble algorithm e.g  EnsembleThreads loss_function  Any arbitrary function to calculate loss continuity_loss  Function that takes states  hat{u}_{end  of group  k  and  u_{0  of group  k+1  as input and calculates prediction continuity loss between them If no custom  continuity_loss  is specified  sum(abs û_end  u_0  is used solver  ODE Solver algorithm group_size  The group size achieved after splitting the ode_data into equal sizes continuity_term  Weight term to ensure continuity of predictions throughout different groups trajectories  number of trajectories for  ensemble_prob  kwargs  Additional arguments splatted to the ODE solver Refer to the  Local Sensitivity Analysis  and  Common Solver Arguments  documentation for more details Note The parameter continuity_term should be a relatively big number to enforce a large penalty whenever the last point of any group doesn't coincide with the first point of next group"},{"doctype":"documentation","id":"references/SciMLBase.AbstractLinearSolution","title":"AbstractLinearSolution","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/SciMLBase.first_tstop","title":"first_tstop","text":"Gets the first stopping time of the integrator"},{"doctype":"documentation","id":"references/ModelingToolkit.round_trip_eq","title":"round_trip_eq","text":""},{"doctype":"documentation","id":"references/SciMLBase.DEAlgorithm","title":"DEAlgorithm","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/LinearSolve.RFWrapper","title":"RFWrapper","text":""},{"doctype":"document","id":"NeuralPDE/pinn/2D.md","title":"2-dimensional PDEs with GPU","text":"Flux GalacticOptimJL Quadrature Cuba CUDA Interval infimum supremum t x y u Dxx Differential x Dyy Differential y Dt Differential t t_min t_max x_min x_max y_min y_max eq Dt u t x y Dxx u t x y Dyy u t x y analytic_sol_func t x y exp x y cos x y t bcs u t_min x y analytic_sol_func t_min x y u t x_min y analytic_sol_func t x_min y u t x_max y analytic_sol_func t x_max y u t x y_min analytic_sol_func t x y_min u t x y_max analytic_sol_func t x y_max domains t Interval t_min t_max x Interval x_min x_max y Interval y_min y_max inner inner Flux σ inner inner Flux σ inner inner Flux σ inner inner Flux σ inner initθ CuArray Float64 strategy discretization strategy init_params initθ pde_system eq bcs domains t x y u t x y prob pde_system discretization symprob pde_system discretization callback p l println l res prob ADAM callback callback maxiters prob prob u0 res minimizer res prob ADAM callback callback maxiters discretization ts xs ys infimum d domain supremum d domain d domains u_real analytic_sol_func t x y t ts x xs y ys u_predict first Array t x y res minimizer t ts x xs y ys Plots Printf plot_ res anim i t enumerate t_max i u_real reshape analytic_sol_func t x y x xs y ys length xs length ys u_predict reshape Array t x y res minimizer x xs y ys length xs length ys u_error abs u_predict u_real title t p1 plot xs ys u_predict st surface label title title title p2 plot xs ys u_real st surface label title title title p3 plot xs ys u_error st contourf label title title plot p1 p2 p3 gif anim fps plot_ res 2-dimensional PDEs with GPU the 2-dimensional PDE ∂_t u(x y t  ∂^2_x u(x y t  ∂^2_y u(x y t   with the initial and boundary conditions begin{align*}\nu(x y 0  e^{x+y cos(x  y       \nu(0 y t  e^{y   cos(y  4t      \nu(2 y t  e^{2+y cos(2  y  4t  \nu(x 0 t  e^{x   cos(x  4t      \nu(x 2 t  e^{x+2 cos(x  2  4t  \n\\end{align on the space and time domain x in 0 2   y in 0 2    t in 0 2   with physics-informed neural networks The  remake  function allows to rebuild the PDE problem 3pde Performance benchmarks Here are some performance benchmarks for 2d-pde with various number of input points and the number of neurons in the hidden layer measuring the time for 100 iterations Сomparing runtime with GPU and CPU image"},{"doctype":"documentation","id":"references/PolyChaos.radau","title":"radau","text":"Gauss-Radau quadrature rule Given a weight function encoded by the recurrence coefficients  α,β for the associated orthogonal polynomials the function generates the nodes and weights  N+1 point Gauss-Radau quadrature rule for the weight function having a prescribed node  end0  typically at one of the end points of the support interval of w or outside thereof Note The function  radau  accepts at most  N  length(α  2  as an input hence providing at most an  length(α  1 point quadrature rule Note Reference OPQ A MATLAB SUITE OF PROGRAMS FOR GENERATING ORTHOGONAL POLYNOMIALS AND RELATED QUADRATURE RULES by Walter Gautschi"},{"doctype":"documentation","id":"references/NeuralPDE.AbstractPINN","title":"AbstractPINN","text":"Algorithm for solving Physics-Informed Neural Networks problems Arguments chain  a Flux.jl chain with a d-dimensional input and a 1-dimensional output strategy  determines which training strategy will be used init_params  the initial parameter of the neural network phi  a trial solution derivative  method that calculates the derivative"},{"doctype":"documentation","id":"references/PolyChaos.LogisticOrthoPoly","title":"LogisticOrthoPoly","text":""},{"doctype":"documentation","id":"references/Integrals","title":"Integrals","text":""},{"doctype":"documentation","id":"references/Catalyst.Subgraph","title":"Subgraph","text":""},{"doctype":"documentation","id":"references/Surrogates._calc_kriging_coeffs","title":"_calc_kriging_coeffs","text":""},{"doctype":"documentation","id":"references/MethodOfLines.wrapperiodic","title":"wrapperiodic","text":""},{"doctype":"documentation","id":"references/NeuralPDE.get_integration_variables","title":"get_integration_variables","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.FastDense","title":"FastDense","text":"FastDense(in,out,activation=identity bias  true precache  false initW  Flux.glorot_uniform initb  Flux.zeros32 A Dense layer  activation.(W*x  b  with input size  in  and output size  out  The  activation  function defaults to  identity  meaning the layer is an affine function Initial parameters are taken to match  Flux.Dense  bias represents b in the layer and it defaults to true.'precache is used to preallocate memory for the intermediate variables calculated during each pass This avoids heap allocations in each pass which would otherwise slow down the computation it defaults to false Note that this function has specializations on  tanh  for a slightly faster adjoint with Zygote"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.BoxWedgeTail","title":"BoxWedgeTail","text":"iip t0 W0 Z0 dist bridge rtol nr na nz box_grouping MinEntropy sqeezing save_everystep rng Xorshifts Xoroshiro128Plus rand UInt64 reset reseed iip The method for random generation of stochastic area integrals due to Gaines and Lyons The method is based on Marsaglia's rectangle-wedge-tail approach for two dimensions 3 different groupings for the boxes are implemented box_grouping  Columns full i.e as large as possible columns on a square spanned by dr and da box_grouping  none no grouping box_grouping  MinEntropy default grouping that achieves a smaller entropy than the column wise grouping and thus allows for slightly faster sampling  but has a slightly larger amount of groups The sampling is based on the Distributions.jl package i.e to sample from one of the many distributions a uni-/bi-variate distribution from Distributions.jl is constructed and then rand is used Constructor"},{"doctype":"documentation","id":"references/MethodOfLines.gridvals","title":"gridvals","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.VertType","title":"VertType","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.find_singlet","title":"find_singlet","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity._second_order_sensitivities","title":"_second_order_sensitivities","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timeseries_point_median","title":"timeseries_point_median","text":""},{"doctype":"documentation","id":"references/SciMLBase.DDEFunction","title":"DDEFunction","text":"iip recompile f mass_matrix I analytic nothing tgrad nothing jac nothing jvp nothing vjp nothing jac_prototype nothing sparsity jac_prototype paramjac nothing syms nothing indepsym nothing colorvec nothing DDEFunction  AbstractDDEFunction A representation of a DDE function  f  defined by M frac{du}{dt  f(u,h,p,t and all of its related functions such as the Jacobian of  f  its gradient with respect to time and more For all cases  u0  is the initial condition  p  are the parameters and  t  is the independent variable Constructor Note that only the function  f  itself is required This function should be given as  f!(du,u,h,p,t  or  du  f(u,h,p,t  See the section on  iip  for more details on in-place vs out-of-place handling The histroy function  h  acts as an interpolator over time i.e  h(t  with options matching the solution interface i.e  h(t save_idxs  2  All of the remaining functions are optional for improving or accelerating  the usage of  f  These include mass_matrix  the mass matrix  M  represented in the ODE function Can be used to determine that the equation is actually a differential-algebraic equation DAE if  M  is singular Note that in this case special solvers are required see the DAE solver page for more details https://diffeq.sciml.ai/stable/solvers/dae_solve Must be an AbstractArray or an AbstractSciMLOperator analytic(u0,p,t  used to pass an analytical solution function for the analytical  solution of the ODE Generally only used for testing and development of the solvers tgrad(dT,u,h,p,t  or dT=tgrad(u,p,t returns  frac{\\partial f(u,p,t)}{\\partial t jac(J,u,h,p,t  or  J=jac(u,p,t  returns  frac{df}{du jvp(Jv,v,h,u,p,t  or  Jv=jvp(v,u,p,t  returns the directional derivative frac{df}{du v vjp(Jv,v,h,u,p,t  or  Jv=vjp(v,u,p,t  returns the adjoint derivative frac{df}{du}^\\ast v jac_prototype  a prototype matrix matching the type that matches the Jacobian For example if the Jacobian is tridiagonal then an appropriately sized  Tridiagonal  matrix can be used as the prototype and integrators will specialize on this structure where possible Non-structured sparsity patterns should use a  SparseMatrixCSC  with a correct sparsity pattern for the Jacobian The default is  nothing  which means a dense Jacobian paramjac(pJ,h,u,p,t  returns the parameter Jacobian  frac{df}{dp  syms  the symbol names for the elements of the equation This should match  u0  in size For example if  u0  0.0,1.0  and  syms  x y  this will apply a canonical naming to the values allowing  sol[:x  in the solution and automatically naming values in plots indepsym  the canonical naming for the independent variable Defaults to nothing which internally uses  t  as the representation in any plots colorvec  a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the  jac_prototype  This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern Defaults to  nothing  which means a color vector will be internally computed on demand when required The cost of this operation is highly dependent on the sparsity pattern iip In-Place vs Out-Of-Place For more details on this argument see the ODEFunction documentation recompile Controlling Compilation and Specialization For more details on this argument see the ODEFunction documentation Fields The fields of the DDEFunction type directly match the names of the inputs"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.CompoundPoissonBridge!","title":"CompoundPoissonBridge!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.hist","title":"hist","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.GeneralBC","title":"GeneralBC","text":"q  GeneralBC(α_leftboundary α_rightboundary dx::T approximation_order Implements a generalization of the Robin boundary condition where α is a vector of coefficients Represents a condition of the form α[1  α[2]u[0  α[3]u'[0  α[4]u''[0  0 Implemented in a similar way to the RobinBC see above This time there are multiple stencils for multiple derivative orders  these can be written as a matrix S All components that multiply u(0 are factored out turns out to only involve the first column of S s̄0 The rest of S is denoted S  the coefficient of u(0 is s̄0⋅ᾱ[3:end  α[2 The remaining components turn out to be ᾱ[3:end]⋅(S ū  or equivalently transpose(ᾱ[3:end])*S ⋅ū  Rearranging a stencil q_a to be dotted with ū  upon extension can readily be found along with a constant component q_b"},{"doctype":"documentation","id":"references/SciMLBase.AbstractNonlinearAlgorithm","title":"AbstractNonlinearAlgorithm","text":"DocStringExtensions.TypeDefinition"},{"doctype":"document","id":"Optimization/tutorials/rosenbrock.md","title":"Rosenbrock function examples","text":"Optim ForwardDiff Zygote Test Random rosenbrock x p p x p x x x0 zeros _p f rosenbrock l1 rosenbrock x0 _p prob f x0 _p OptimizationOptimJL sol prob SimulatedAnnealing sol minimum l1 Random seed! prob f x0 _p lb ub sol prob SAMIN sol minimum l1 l1 rosenbrock x0 prob rosenbrock x0 sol prob NelderMead sol minimum l1 cons x p x x optprob rosenbrock cons cons prob optprob x0 sol prob ADAM maxiters sol minimum l1 sol prob BFGS sol minimum l1 sol prob Newton sol minimum l1 sol prob Optim KrylovTrustRegion sol minimum l1 prob optprob x0 lcons Inf ucons Inf sol prob IPNewton sol minimum l1 prob optprob x0 lcons ucons sol prob IPNewton sol minimum l1 prob optprob x0 lcons Inf ucons Inf lb ub sol prob IPNewton sol minimum l1 con2_c x p x x x sin x x optprob rosenbrock cons con2_c prob optprob x0 lcons Inf Inf ucons Inf Inf sol prob IPNewton sol minimum l1 cons_circ x p x x optprob rosenbrock cons cons_circ prob optprob x0 lcons Inf ucons sol prob IPNewton sqrt cons sol minimizer nothing rtol optprob rosenbrock prob optprob x0 sol prob ADAM maxiters progress sol minimum l1 prob optprob x0 lb ub sol prob Fminbox sol minimum l1 prob optprob x0 lb ub sol prob SAMIN sol minimum l1 OptimizationCMAEvolutionStrategy sol prob CMAEvolutionStrategyOpt sol minimum l1 rosenbrock x p nothing x x x OptimizationNLopt prob optprob x0 sol prob Opt LN_BOBYQA sol minimum l1 sol prob Opt LD_LBFGS sol minimum l1 prob optprob x0 lb ub sol prob Opt LD_LBFGS sol minimum l1 sol prob Opt G_MLSL_LDS nstart local_method Opt LD_LBFGS maxiters sol minimum l1 OptimizationEvolutionary sol prob CMAES μ λ abstol sol minimum l1 OptimizationBBO prob optprob x0 lb ub sol prob BBO sol minimum l1 Rosenbrock function examples Note This example uses many different solvers of Optimization.jl Each solver subpackage needs to be installed separate For example for the details on the installation and usage of OptimizationOptimJL.jl package see the Optim.jl page  optim"},{"doctype":"documentation","id":"references/ModelingToolkit.dae_order_lowering","title":"dae_order_lowering","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.Tail7","title":"Tail7","text":""},{"doctype":"documentation","id":"references/SciMLBase.getindepsym_defaultt","title":"getindepsym_defaultt","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.square_norm!","title":"square_norm!","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timestep_mean","title":"timestep_mean","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.has_ctrl_jac","title":"has_ctrl_jac","text":""},{"doctype":"documentation","id":"references/PoissonRandom.count_rand","title":"count_rand","text":""},{"doctype":"documentation","id":"references/NeuralPDE.transform_inf_integral","title":"transform_inf_integral","text":""},{"doctype":"document","id":"Surrogates/contributing.md","title":"Contributions","text":"Contributions Contributions are  very  welcome There are many ways do help Opening/solving issues Making the code more efficient Opening a new PR with a new Sampling technique Surrogate or optimization method Writing more tutorials with your own unique use case of the library Your own idea You can also contact me on the Julia slack channel at   List of contributors Ludovico Bessi   Chris Rackauckas   Rohit Singh Rathaur   Andrea Cognolato   Kanav Gupta  "},{"doctype":"documentation","id":"references/LabelledArrays.__getindex","title":"__getindex","text":""},{"doctype":"document","id":"DiffEqSensitivity/dae_fitting/physical_constraints.md","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"Lux OptimizationOptimJL DifferentialEquations Plots Random rng Random default_rng f! du u p t y₁ y₂ y₃ u k₁ k₂ k₃ p du k₁ y₁ k₃ y₂ y₃ du k₁ y₁ k₃ y₂ y₃ k₂ y₂ du y₁ y₂ y₃ nothing u₀ M tspan p stiff_func f! mass_matrix M prob_stiff stiff_func u₀ tspan p sol_stiff prob_stiff Rodas5 saveat nn_dudt2 Lux Lux Dense tanh Lux Dense pinit st Lux setup rng nn_dudt2 model_stiff_ndae nn_dudt2 u p t u u u tspan M Rodas5 autodiff saveat model_stiff_ndae u₀ Lux ComponentArray pinit st predict_stiff_ndae p model_stiff_ndae u₀ p st loss_stiff_ndae p pred predict_stiff_ndae p loss sum abs2 Array sol_stiff pred loss pred l1 first loss_stiff_ndae Lux ComponentArray pinit adtype optf x p loss_stiff_ndae x adtype optprob optf Lux ComponentArray pinit result_stiff optprob BFGS maxiters Lux OptimizationOptimJL DifferentialEquations Plots Random rng Random default_rng Enforcing Physical Constraints via Universal Differential-Algebraic Equations As shown in the  stiff ODE tutorial  differential-algebraic equations DAEs can be used to impose physical constraints One way to define a DAE is through an ODE with a singular mass matrix For example if we make  Mu  f(u  where the last row of  M  is all zeros then we have a constraint defined by the right hand side Using  NeuralODEMM  we can use this to define a neural ODE where the sum of all 3 terms must add to one An example of this is as follows Step-by-Step Description Load Packages Differential Equation First we define our differential equations as a highly stiff problem which makes the fitting difficult Parameters u₀   Initial Conditions M   Semi-explicit Mass Matrix last row is the constraint equation and are therefore all zeros tspan   Time span over which to evaluate p   parameters  k1   k2  and  k3  of the differential equation above ODE Function Problem and Solution We define and solve our ODE problem to generate the labeled data which will be used to train our Neural Network Because this is a DAE we need to make sure to use a  compatible solver   Rodas5  works well for this example Neural Network Layers Next we create our layers using  Lux.Chain  We use this instead of  Flux.Chain  because it is more suited to SciML applications similarly for  Lux.Dense  The input to our network will be the initial conditions fed in as  u₀  Because this is a stiff problem we have manually imposed that sum constraint via  u,p,t  u[1  u[2  u[3  1  making the fitting easier Prediction Function For simplicity we define a wrapper function that only takes in the model's parameters to make predictions Train Parameters Training our network requires a  loss function  an  optimizer  and a  callback function  to display the progress Loss We first make our predictions based on the current parameters then calculate the loss from these predictions In this case we use  least squares  as our loss Notice that we are feeding the  parameters  of  model_stiff_ndae  to the  loss_stiff_ndae  function  model_stiff_node.p  are the weights of our NN and is of size  386  4  64  65  2 including the biases Optimizer The optimizer is  BFGS see below Callback The callback function displays the loss during training Train Finally training with  Optimization.solve  by passing  loss function   model parameters   optimizer   callback  and  maximum iteration  Expected Output"},{"doctype":"documentation","id":"references/Catalyst.pprint","title":"pprint","text":"Pretty-print the Graphviz expression"},{"doctype":"documentation","id":"references/ModelingToolkit.namespace_defaults","title":"namespace_defaults","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.Divergence","title":"Divergence","text":""},{"doctype":"documentation","id":"references/RecursiveArrayTools.unpack","title":"unpack","text":""},{"doctype":"documentation","id":"references/Surrogates.SMB","title":"SMB","text":""},{"doctype":"document","id":"DiffEqFlux/examples/tensor_layer.md","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"OptimizationOptimJL DifferentialEquations LinearAlgebra k α β γ tspan dxdt_train du u p t du u du k u α u β u γ u u0 ts collect tspan prob_train dxdt_train u0 tspan p nothing data_train Array prob_train Tsit5 saveat ts A nn A f x min one x x dxdt_pred du u p t du u du p u p u f nn u p end α zeros prob_pred dxdt_pred u0 tspan p nothing predict_adjoint θ x Array prob_pred Tsit5 p θ saveat ts loss_adjoint θ x predict_adjoint θ loss sum norm x data_train loss cb θ l θ l adtype optf x p loss_adjoint x adtype optprob optf α res1 optprob ADAM cb cb maxiters optprob2 optf res1 u res2 optprob2 ADAM cb cb maxiters opt res2 u Plots data_pred predict_adjoint opt plot ts data_train label plot! ts data_train label plot! ts data_pred label plot! ts data_pred label Physics-Informed Machine Learning PIML with TensorLayer In this tutorial we show how to use the  DiffEqFlux  TensorLayer to solve problems in Physics Informed Machine Learning Let's consider the anharmonic oscillator described by the ODE ẍ   kx  αx³  βẋ γẋ³ To obtain the training data we solve the equation of motion using one of the solvers in  DifferentialEquations  Now we create a TensorLayer that will be able to perform 10th order expansions in a Legendre Basis and we also instantiate the model we are trying to learn informing the neural about the  ∝x  and  ∝v  dependencies in the equation of motion Note that we introduced a cap in the neural network term to avoid instabilities in the solution of the ODE We also initialized the vector of parameters to zero in order to obtain a faster convergence for this particular example Finally we introduce the corresponding loss function and we train the network using two rounds of  ADAM  We plot the results and we obtain a fairly accurate learned model plot_tutorial"},{"doctype":"documentation","id":"references/GlobalSensitivity._compute_first_order","title":"_compute_first_order","text":""},{"doctype":"document","id":"MethodOfLines/boundary_conditions.md","title":"[Boundary Conditions] ( bcs)","text":"Domainsets x y t u v Dt Differential t Dx Differential x Dy Differential y Dxx Differential x Dyy Differential y x_min y_min x_max y_max v t y u t y x_min y t v t x y_max sin x alpha f t x y x y t g x y z sin x y cos y IfElse ifelse z x u t x y_min f t x y_min alpha g x y_min alpha f t x y x y t g x y z sin x y cos y z x g x y u t x y_min f t x y_min alpha g x y_min v t x_min y Dx v t x_min y u t x_min y x_min Dy v t x_min y t v t x sin x Dyy v t x y_max Dt u t x_min y f u v u Dyy v Dy u v Dyy u t x y_min f u t x y_min v t x y_min u t x y_max Dy v t x y_max u t x_min y u t x_max y v t x y_max u t x_max y Boundary Conditions   bcs What follows is a set of allowable boundary conditions please note that this is not exhaustive  try your condition and see if it works the handling is quite general If it doesn't please post an issue and we'll try to support it At the moment boundary conditions have to be supplied at the edge of the domain but there are plans to support conditions embedded in the domain Definitions Dirichlet Time dependant Julia function User defined function Registered User Defined Function Neumann/Robin Time dependant Higher order Time derivative User defined function 0 lhs Periodic"},{"doctype":"documentation","id":"references/DiffEqFlux.NeuralDELayer","title":"NeuralDELayer","text":""},{"doctype":"document","id":"NeuralPDE/pinn/low_level.md","title":"1-D Burgers' Equation With Low-Level API","text":"Flux GalacticOptimJL Interval infimum supremum t x u Dt Differential t Dx Differential x Dxx Differential x eq Dt u t x u t x Dx u t x pi Dxx u t x bcs u x sin pi x u t u t u t u t domains t Interval x Interval dx Flux σ Flux σ initθ Float64 eltypeθ eltype initθ parameterless_type_θ DiffEqBase initθ strategy dx parameterless_type_θ derivative indvars t x depvars u _pde_loss_function eq indvars depvars derivative nothing initθ strategy bc_indvars bcs indvars depvars _bc_loss_functions bc indvars depvars derivative nothing initθ strategy bc_indvars bc_indvar bc bc_indvar zip bcs bc_indvars train_sets domains dx eq bcs eltypeθ indvars depvars train_domain_set train_bound_set train_sets pde_loss_function _pde_loss_function train_domain_set eltypeθ parameterless_type_θ strategy bc_loss_functions loss set eltypeθ parameterless_type_θ strategy loss set zip _bc_loss_functions train_bound_set loss_functions pde_loss_function bc_loss_functions loss_function__ θ sum map l l θ loss_functions loss_function_ θ p loss_function__ θ f loss_function_ prob f initθ cb_ p l println l map l l p loss_functions opt BFGS res prob opt callback cb_ maxiters Plots ts xs infimum d domain dx supremum d domain d domains u_predict_contourf reshape first t x res minimizer t ts x xs length xs length ts plot ts xs u_predict_contourf linetype contourf title u_predict first t x res minimizer x xs t ts p1 plot xs u_predict title p2 plot xs u_predict title p3 plot xs u_predict end title plot p1 p2 p3 1-D Burgers Equation With Low-Level API Let's consider the Burgers equation begin{gather*}\n∂_t u  u ∂_x u  0.01  pi ∂_x^2 u  0   quad x in 1 1 t in 0 1   \nu(0 x   sin(\\pi x   \nu(t 1  u(t 1  0  \n\\end{gather with Physics-Informed Neural Networks Here is an example of using the low-level API And some analysis burgers burgers2 See low-level API  Physics-Informed Neural Networks"},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.𝑠edges","title":"𝑠edges","text":""},{"doctype":"document","id":"ModelingToolkit/systems/JumpSystem.md","title":"JumpSystem","text":"JumpSystem System Constructors Composition and Accessor Functions get_eqs(sys  or  equations(sys  The equations that define the jump system get_states(sys  or  states(sys  The set of states in the jump system get_ps(sys  or  parameters(sys  The parameters of the jump system get_iv(sys  The independent variable of the jump system Transformations Analyses Problem Constructors"},{"doctype":"document","id":"Optimization/optimization_packages/flux.md","title":"Flux.jl","text":"Pkg Pkg add Flux.jl Installation OptimizationFlux.jl To use this package install the OptimizationFlux package Local Unconstrained Optimizers Flux.Optimise.Descent   Classic gradient descent optimizer with learning rate solve(problem Descent(η η  is the learning rate Defaults η  0.1 Flux.Optimise.Momentum   Classic gradient descent optimizer with learning rate and momentum solve(problem Momentum(η ρ η  is the learning rate ρ  is the momentum Defaults η  0.01 ρ  0.9 Flux.Optimise.Nesterov   Gradient descent optimizer with learning rate and Nesterov momentum solve(problem Nesterov(η ρ η  is the learning rate ρ  is the Nesterov momentum Defaults η  0.01 ρ  0.9 Flux.Optimise.RMSProp   RMSProp optimizer solve(problem RMSProp(η ρ η  is the learning rate ρ  is the momentum Defaults η  0.001 ρ  0.9 Flux.Optimise.ADAM   ADAM optimizer solve(problem ADAM(η β::Tuple η  is the learning rate β::Tuple  is the decay of momentums Defaults η  0.001 β::Tuple  0.9 0.999 Flux.Optimise.RADAM   Rectified ADAM optimizer solve(problem RADAM(η β::Tuple η  is the learning rate β::Tuple  is the decay of momentums Defaults η  0.001 β::Tuple  0.9 0.999 Flux.Optimise.AdaMax   AdaMax optimizer solve(problem AdaMax(η β::Tuple η  is the learning rate β::Tuple  is the decay of momentums Defaults η  0.001 β::Tuple  0.9 0.999 Flux.Optimise.ADAGRad   ADAGrad optimizer solve(problem ADAGrad(η η  is the learning rate Defaults η  0.1 Flux.Optimise.ADADelta   ADADelta optimizer solve(problem ADADelta(ρ ρ  is the gradient decay factor Defaults ρ  0.9 Flux.Optimise.AMSGrad   AMSGrad optimizer solve(problem AMSGrad(η β::Tuple η  is the learning rate β::Tuple  is the decay of momentums Defaults η  0.001 β::Tuple  0.9 0.999 Flux.Optimise.NADAM   Nesterov variant of the ADAM optimizer solve(problem NADAM(η β::Tuple η  is the learning rate β::Tuple  is the decay of momentums Defaults η  0.001 β::Tuple  0.9 0.999 Flux.Optimise.ADAMW   ADAMW optimizer solve(problem ADAMW(η β::Tuple η  is the learning rate β::Tuple  is the decay of momentums decay  is the decay to weights Defaults η  0.001 β::Tuple  0.9 0.999 decay  0"},{"doctype":"documentation","id":"references/DiffEqOperators.UpwindDifference","title":"UpwindDifference","text":"constructs a DerivativeOperator that automatically implements upwinding Inputs dx::T  or  dx::Vector{T  grid spacing coeff_func  function mapping index in the grid to coefficient at that grid location Examples julia drift  1 1 1 julia L1  UpwindDifference(1 1 1 3 i  drift[i julia L2  UpwindDifference(1 1 1 3 i  1 julia Q  Neumann0BC(1 1 julia Array(L1  Q)[1 3×3 Array{Float64,2 1.0   1.0   0.0 0.0  1.0   1.0 0.0   1.0  1.0 julia Array(L2  Q)[1 3×3 Array{Float64,2 1.0   1.0  0.0 0.0  1.0  1.0 0.0   0.0  0.0"},{"doctype":"documentation","id":"references/SciMLBase.SteadyStateSolution","title":"SteadyStateSolution","text":""},{"doctype":"documentation","id":"references/SciMLBase.@add_kwonly","title":"@add_kwonly","text":"Define keyword-only version of the  function_definition  expands to"},{"doctype":"documentation","id":"references/SciMLBase.has_tgrad","title":"has_tgrad","text":""},{"doctype":"documentation","id":"references/NeuralPDE.NNODE","title":"NNODE","text":"Algorithm for solving differential equation using a neural network Arguments chain  A Chain neural network opt  The optimizer to train the neural network Defaults to  BFGS  initθ  The initial parameter of the neural network autodiff  The switch between automatic and numerical differentiation for the PDE operators The reverse mode of the loss function is always AD"},{"doctype":"documentation","id":"references/NeuralPDE.AbstractAdaptiveLoss","title":"AbstractAdaptiveLoss","text":""},{"doctype":"documentation","id":"references/Optimization.DESolution","title":"DESolution","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.findcursor","title":"findcursor","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.sorted_incidence_matrix","title":"sorted_incidence_matrix","text":""},{"doctype":"documentation","id":"references/SciMLBase.NonlinearProblem","title":"NonlinearProblem","text":"f u0 p kwargs f u0 p kwargs Defines a nonlinear system problem Documentation Page https://nonlinearsolve.sciml.ai/dev/basics/NonlinearProblem Mathematical Specification of a Nonlinear Problem To define a Nonlinear Problem you simply need to give the function  f  which defines the nonlinear system f(u,p  0 and an initial guess  u₀  of where  f(u,p)=0   f  should be specified as  f(u,p  or in-place as  f(du,u,p  and  u₀  should be an AbstractArray or number whose geometry matches the desired geometry of  u  Note that we are not limited to numbers or vectors for  u₀  one is allowed to provide  u₀  as arbitrary matrices  higher-dimension tensors as well Problem Type Constructors isinplace  optionally sets whether the function is in-place or not This is determined automatically but not inferred Parameters are optional and if not given then a  NullParameters  singleton will be used which will throw nice errors if you try to index non-existent parameters Any extra keyword arguments are passed on to the solvers For example if you set a  callback  in the problem then that  callback  will be added in every solve call For specifying Jacobians and mass matrices see the NonlinearFunctions  nonlinearfunctions page Fields f  The function in the problem u0  The initial guess for the steady state p  The parameters for the problem Defaults to  NullParameters  kwargs  The keyword arguments passed on to the solvers DocStringExtensions.MethodSignatures Define a steady state problem using an instance of  AbstractNonlinearFunction   AbstractNonlinearFunction DocStringExtensions.MethodSignatures Define a steady state problem from a standard ODE problem"},{"doctype":"documentation","id":"references/NonlinearSolve.prevfloat_tdir","title":"prevfloat_tdir","text":"prevfloat_tdir(x x0 x1 Move  x  one floating point towards x0"},{"doctype":"documentation","id":"references/DiffEqSensitivity.ParamGradientWrapper","title":"ParamGradientWrapper","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.setup_next_step!","title":"setup_next_step!","text":""},{"doctype":"documentation","id":"references/PolyChaos._checkStandardDevation","title":"_checkStandardDevation","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.nilsas_min","title":"nilsas_min","text":""},{"doctype":"document","id":"LabelledArrays/Note_labelled_slices.md","title":"Note: Labelled slices","text":"Note Labelled slices This functionality has been removed from LabelledArrays.jl but can replicated with the same compile-time performance and indexing syntax using  DimensionalData.jl "},{"doctype":"documentation","id":"references/GlobalSensitivity.generate_ff_sample_matrix","title":"generate_ff_sample_matrix","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.generate_diffusion_function","title":"generate_diffusion_function","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.𝑑neighbors","title":"𝑑neighbors","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.kiops_update_solution!","title":"kiops_update_solution!","text":""},{"doctype":"documentation","id":"references/MethodOfLines.central_difference","title":"central_difference","text":"Performs a centered difference in  x  centered at index  II  of  u  ufunc is a function that returns the correct discretization indexed at Itap it is designed this way to allow for central differences of arbitrary expressions which may be needed in some schemes"},{"doctype":"documentation","id":"references/ExponentialUtilities.exp_gen!","title":"exp_gen!","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.dot_product!","title":"dot_product!","text":""},{"doctype":"document","id":"ModelingToolkit/systems/NonlinearSystem.md","title":"NonlinearSystem","text":"jacobian_sparsity NonlinearSystem System Constructors Composition and Accessor Functions get_eqs(sys  or  equations(sys  The equations that define the nonlinear system get_states(sys  or  states(sys  The set of states in the nonlinear system get_ps(sys  or  parameters(sys  The parameters of the nonlinear system Transformations Analyses Applicable Calculation and Generation Functions Problem Constructors Torn Problem Constructors"},{"doctype":"documentation","id":"references/ParameterizedFunctions.ode_findreplace","title":"ode_findreplace","text":""},{"doctype":"documentation","id":"references/Catalyst.isreversible","title":"isreversible","text":"sir SIR β S I I ν I R β ν rcs sir sir Given a reaction network returns if the network is reversible or not Notes Requires the  incidencemat  to already be cached in  rn  by a previous call to  reactioncomplexes  For example"},{"doctype":"documentation","id":"references/GlobalSensitivity.RegressionGSA","title":"RegressionGSA","text":"RegressionGSA methods for global sensitivity analysis Providing this to  gsa  results in a calculation of the following statistics provided as a  RegressionGSAResult  If the function f to be analyzed is of dimensionality f R^n  R^m then these coefficients are returned as a matrix with the corresponding statistic in the i j entry pearson  This is equivalent to the correlation coefficient matrix between input and output The rank version is known as the Spearman coefficient standard_regression  Standard regression coefficients also known as sigma-normalized derivatives partial_correlation  Partial correlation coefficients related to the precision matrix and a measure of the correlation of linear models of the Arguments rank::Bool  false  Flag determining whether to also run a rank regression analysis"},{"doctype":"documentation","id":"references/NonlinearSolve","title":"NonlinearSolve","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.inner_namespace","title":"inner_namespace","text":""},{"doctype":"documentation","id":"references/SciMLBase.IncrementingODEProblem","title":"IncrementingODEProblem","text":"DocStringExtensions.MethodSignatures Experimental"},{"doctype":"documentation","id":"references/LinearSolve.simplelu_factorize!","title":"simplelu_factorize!","text":""},{"doctype":"document","id":"LabelledArrays/SLArrays.md","title":"SLArrays","text":"SLArrays The  SLArray  and  SLVector  macros create static LabelledArrays First the user would create the array type then use that constructor to generate instances of the labelled array SLArray  and  SLVector  macros Macro constructors are convenient for building most  SLArray  objects An  SLArray  may be of arbitrary dimension while an  SLVector  is a one dimensional array SLArray  and  SLVector  constructors Alternatively users can construct a static labelled array using the  SLVector  and  SLArrays  constructors by writing out the entries as keyword arguments Manipulating  SLArrays  and  SLVectors Users may want a list of the labels or keys in an  SLArray  or  SLVector  The  symbols(::SLArray  function returns a tuple of array labels"},{"doctype":"documentation","id":"references/DiffEqSensitivity.h!","title":"h!","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.TensorProductBasis","title":"TensorProductBasis","text":""},{"doctype":"document","id":"Surrogates/wendland.md","title":"Wendland Surrogate","text":"Wendland Surrogate The Wendland surrogate is a compact surrogate it allocates much less memory then other surrogates The coefficients are found using an iterative solver f  x  exp(-x^2 We choose to sample f in 30 points between 5 to 25 using  sample  function The sampling points are chosen using a Sobol sequence this can be done by passing  SobolSample  to the  sample  function Building Surrogate The choice of the right parameter is especially important here a slight change in ϵ would produce a totally different fit Try it yourself with this function"},{"doctype":"documentation","id":"references/DiffEqFlux.TriangularKernel","title":"TriangularKernel","text":""},{"doctype":"documentation","id":"references/SciMLBase.ParamJacobianWrapper","title":"ParamJacobianWrapper","text":""},{"doctype":"document","id":"ModelingToolkit/systems/PDESystem.md","title":"PDESystem","text":"t t x UnitDisk v w x y z VectorUnitBall PDESystem PDESystem  is the common symbolic PDE specification for the SciML ecosystem It is currently being built as a component of the ModelingToolkit ecosystem Vision The vision for the common PDE interface is that a user should only have to specify their PDE once mathematically and have instant access to everything as simple as a finite difference method with constant grid spacing to something as complex as a distributed multi-GPU discontinuous Galerkin method The key to the common PDE interface is a separation of the symbolic handling from the numerical world All of the discretizers should not solve the PDE but instead be a conversion of the mathematical specification to a numerical problem Preferably the transformation should be to another ModelingToolkit.jl  AbstractSystem  but in some cases this cannot be done or will not be performant so a  SciMLProblem  is the other choice These elementary problems such as solving linear systems  Ax=b  solving nonlinear systems  f(x)=0  ODEs etc are all defined by SciMLBase.jl which then numerical solvers can all target these common forms Thus someone who works on linear solvers doesn't necessarily need to be working on a discontinuous Galerkin or finite element library but instead linear solvers that are good for matrices A with properties  which are then accessible by every other discretization method in the common PDE interface Similar to the rest of the  AbstractSystem  types transformation and analysis functions will allow for simplifying the PDE before solving it and constructing block symbolic functions like Jacobians Constructors Domains WIP Domains are specifying by saying  indepvar in domain  where  indepvar  is a single or a collection of independent variables and  domain  is the chosen domain type A 2-tuple can be used to indicate an  Interval  Thus forms for the  indepvar  can be like Domain Types WIP Interval(a,b  Defines the domain of an interval from  a  to  b  requires explicit import from  DomainSets.jl  but a 2-tuple can be used instead discretize  and  symbolic_discretize The only functions which act on a PDESystem are the following discretize(sys,discretizer  produces the outputted  AbstractSystem  or  SciMLProblem  symbolic_discretize(sys,discretizer  produces a debugging symbolic description of the discretized problem Boundary Conditions WIP Transformations Analyses Discretizer Ecosystem NeuralPDE.jl PhysicsInformedNN NeuralPDE.jl  defines the  PhysicsInformedNN  discretizer which uses a  DiffEqFlux.jl  neural network to solve the differential equation DiffEqOperators.jl MOLFiniteDifference WIP DiffEqOperators.jl  defines the  MOLFiniteDifference  discretizer which performs a finite difference discretization using the DiffEqOperators.jl stencils These stencils make use of NNLib.jl for fast operations on semi-linear domains"},{"doctype":"documentation","id":"references/RecursiveArrayTools.narrays","title":"narrays","text":"Retrieve number of arrays in the AbstractVectorOfArrays of a broadcast"},{"doctype":"documentation","id":"references/Surrogates.Hypervolume_Pareto_improving","title":"Hypervolume_Pareto_improving","text":""},{"doctype":"documentation","id":"references/LinearSolve.set_cacheval","title":"set_cacheval","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities._phiv_timestep_caches","title":"_phiv_timestep_caches","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.all_dimensionless","title":"all_dimensionless","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs._neighbors","title":"_neighbors","text":""},{"doctype":"documentation","id":"references/SciMLOperators.getops","title":"getops","text":""},{"doctype":"documentation","id":"references/QuasiMonteCarlo.fixed_dimensions","title":"fixed_dimensions","text":""},{"doctype":"documentation","id":"references/NeuralPDE.get_u","title":"get_u","text":""},{"doctype":"documentation","id":"references/PolyChaos.AbstractQuad","title":"AbstractQuad","text":""},{"doctype":"documentation","id":"references/MethodOfLines.remove","title":"remove","text":""},{"doctype":"document","id":"NonlinearSolve/tutorials/iterator_interface.md","title":"Nonlinear Solver Iterator Interface","text":"f u p u u u0 probB f u0 solver probB solver solver Nonlinear Solver Iterator Interface There is an iterator form of the nonlinear solver which mirrors the DiffEq integrator interface Note that the  solver  object is actually immutable since we want to make it live on the stack for the sake of performance"},{"doctype":"documentation","id":"references/PolyChaos.GaussOrthoPoly","title":"GaussOrthoPoly","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.NonlinearSystem","title":"NonlinearSystem","text":"x y z σ ρ β eqs σ y x x ρ z y x y β z ns eqs x y z σ ρ β DocStringExtensions.TypeDefinition A nonlinear system of equations Fields DocStringExtensions.TypeFields(false Examples"},{"doctype":"documentation","id":"references/PolyChaos.JacobiOrthoPoly","title":"JacobiOrthoPoly","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.runge_kutta_discretize","title":"runge_kutta_discretize","text":"sys dt tspan tab Transforms a nonlinear optimal control problem into a constrained  OptimizationProblem  according to a Runge-Kutta tableau that describes a collocation method Requires a fixed  dt  over a given  timespan  Defaults to using the 5th order RadauIIA tableau and altnerative tableaus can be specified using the SciML tableau style"},{"doctype":"documentation","id":"references/DiffEqOperators.checkbounds","title":"checkbounds","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.process_DEProblem","title":"process_DEProblem","text":""},{"doctype":"documentation","id":"references/SciMLBase.SciMLAlgorithm","title":"SciMLAlgorithm","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/SciMLBase.addsteps!","title":"addsteps!","text":""},{"doctype":"documentation","id":"references/SciMLOperators.cache_internals","title":"cache_internals","text":""},{"doctype":"documentation","id":"references/ParameterizedFunctions.flip_mult!","title":"flip_mult!","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.fibonacci_spiral","title":"fibonacci_spiral","text":""},{"doctype":"documentation","id":"references/RecursiveArrayTools.chain","title":"chain","text":"Iterate through any number of iterators in sequence"},{"doctype":"documentation","id":"references/NeuralPDE._transform_expression","title":"_transform_expression","text":"Transform the derivative expression to inner representation Examples First compute the derivative of function u(x,y with respect to x Take expressions in the form  derivative(u(x,y x  to  derivative(phi u x y εs order θ  where phi  trial solution u  function x,y  coordinates of point εs  epsilon mask order  order of derivative θ  weight in neural network"},{"doctype":"documentation","id":"references/SciMLBase.ContinuousCallback","title":"ContinuousCallback","text":"condition affect! affect_neg! initialize finalize idxs nothing rootfind save_positions interp_points abstol eps reltol repeat_nudge Contains a single callback whose  condition  is a continuous function The callback is triggered when this function evaluates to 0 Arguments condition  This is a function  condition(u,t,integrator  for declaring when the callback should be used A callback is initiated if the condition hits  0  within the time interval See the Integrator Interface  integrator documentation for information about  integrator  affect  This is the function  affect!(integrator  where one is allowed to modify the current state of the integrator If you do not pass an  affect_neg  function it is called when  condition  is found to be  0  at a root and the cross is either an upcrossing from negative to positive or a downcrossing from positive to negative You need to explicitly pass  nothing  as the  affect_neg  argument if it should only be called at upcrossings e.g  ContinuousCallback(condition affect nothing  For more information on what can be done see the Integrator Interface  integrator manual page Modifications to  u  are safe in this function affect_neg!=affect  This is the function  affect_neg!(integrator  where one is allowed to modify the current state of the integrator This is called when  condition  is found to be  0  at a root and the cross is an downcrossing from positive to negative For more information on what can be done see the Integrator Interface  integrator manual page Modifications to  u  are safe in this function rootfind=LeftRootFind  This is a flag to specify the type of rootfinding to do for finding event location If this is set to  LeftRootfind  the solution will be backtracked to the point where  condition==0  and if the solution isn't exact the left limit of root is used If set to  RightRootFind  the solution would be set to the right limit of the root Otherwise the systems and the  affect  will occur at  t+dt  Note that these enums are not exported and thus one needs to reference them as  SciMLBase.LeftRootFind   SciMLBase.RightRootFind  or  SciMLBase.NoRootFind  interp_points=10  The number of interpolated points to check the condition The condition is found by checking whether any interpolation point  endpoint has a different sign If  interp_points=0  then conditions will only be noticed if the sign of  condition  is different at  t  than at  t+dt  This behavior is not robust when the solution is oscillatory and thus it's recommended that one use some interpolation points they're cheap to compute  0  within the time interval save_positions=(true,true  Boolean tuple for whether to save before and after the  affect  This saving will occur just before and after the event only at event times and does not depend on options like  saveat   save_everystep  etc i.e if  saveat=[1.0,2.0,3.0  this can still add a save point at  2.1  if true For discontinuous changes like a modification to  u  to be handled correctly without error one should set  save_positions=(true,true  idxs=nothing  The components which will be interpolated into the condition Defaults to  nothing  which means  u  will be all components initialize  This is a function  c,u,t,integrator  which can be used to initialize the state of the callback  c  It should modify the argument  c  and the return is ignored finalize  This is a function  c,u,t,integrator  which can be used to finalize the state of the callback  c  It can modify the argument  c  and the return is ignored abstol=1e-14    reltol=0  These are used to specify a tolerance from zero for the rootfinder if the starting condition is less than the tolerance from zero then no root will be detected This is to stop repeat events happening just after a previously rootfound event repeat_nudge  1//100  This is used to set the next testing point after a previously found zero Defaults to 1//100 which means after a callback the next sign check will take place at t  dt*1//100 instead of at t to avoid repeats"},{"doctype":"documentation","id":"references/DiffEqSensitivity.AbstractShadowingSensitivityAlgorithm","title":"AbstractShadowingSensitivityAlgorithm","text":""},{"doctype":"documentation","id":"references/Optimization.__init__","title":"__init__","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.UNonDiagNoiseGradientWrapper","title":"UNonDiagNoiseGradientWrapper","text":""},{"doctype":"documentation","id":"references/SciMLBase.NullParameters","title":"NullParameters","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.CircleDomain","title":"CircleDomain","text":""},{"doctype":"document","id":"Optimization/optimization_packages/quaddirect.md","title":"QuadDIRECT.jl","text":"Pkg Pkg add rosenbrock x p p x p x x x0 zeros p f rosenbrock prob f x0 p lb ub prob QuadDirect splits QuadDIRECT.jl QuadDIRECT  is a Julia package implementing  QuadDIRECT algorithm inspired by DIRECT and MCS  The QuadDIRECT algorithm is called using  QuadDirect  Installation OptimizationQuadDIRECT.jl To use this package install the OptimizationQuadDIRECT package Also note that  QuadDIRECT  should for now be installed by doing  add https://github.com/timholy/QuadDIRECT.jl.git Global Optimizer Without Constraint Equations The algorithm in  QuadDIRECT  is performing global optimization on problems without constraint equations However lower and upper constraints set by  lb  and  ub  in the  OptimizationProblem  are required Furthermore  QuadDirect  requires  splits  which is a list of 3-vectors with initial locations at which to evaluate the function the values must be in strictly increasing order and lie within the specified bounds such that  solve(problem QuadDirect splits  Example The Rosenbrock function can optimized using the  QuadDirect  as follows"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.adaptive_alg","title":"adaptive_alg","text":""},{"doctype":"documentation","id":"references/MethodOfLines.generate_nonlinlap_rules","title":"generate_nonlinlap_rules","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractODESolution","title":"AbstractODESolution","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/MethodOfLines._wrapperiodic","title":"_wrapperiodic","text":""},{"doctype":"document","id":"Catalyst/tutorials/parameter_estimation.md","title":"Parameter Estimation","text":"OrdinaryDiffEq Flux brusselator A ∅ X X Y X B X Y X ∅ A B p_real u0 tspan sample_times range tspan stop tspan length prob brusselator u0 tspan p_real sol_real prob Rosenbrock23 tstops sample_times sample_vals sol_real u findfirst sol_real t ts rand ts sample_times Plots plot sol_real size label framestyle box lw color darkblue darkred plot! sample_times sample_vals seriestype scatter color blue red label optimise_p p_init tend loss p sol prob tspan tend p p Rosenbrock23 tstops sample_times vals hcat map ts sol u findfirst sol t ts sample_times findlast sample_times tend loss sum abs2 vals sample_vals size vals loss sol loss p_init ADAM maxiters p_estimate optimise_p minimizer sol_estimate prob tspan p p_estimate Rosenbrock23 plot sol_real size color blue red framestyle box lw label linealpha plot! sample_times sample_vals seriestype scatter color blue red label alpha plot! sol_estimate color darkblue darkred linestyle dash lw label xlimit tspan p_estimate optimise_p p_estimate minimizer sol_estimate prob tspan p p_estimate Rosenbrock23 plot sol_real size color blue red framestyle box lw label linealpha plot! sample_times sample_vals seriestype scatter color blue red label alpha plot! sol_estimate color darkblue darkred linestyle dash lw label xlimit tspan p_estimate optimise_p p_estimate minimizer sol_estimate prob tspan p p_estimate Rosenbrock23 plot sol_real size color blue red framestyle box lw label linealpha plot! sample_times sample_vals seriestype scatter color blue red label alpha plot! sol_estimate color darkblue darkred linestyle dash lw label xlimit tspan p_estimate optimise_p minimizer sol_estimate prob tspan p p_estimate Rosenbrock23 plot sol_real size color blue red framestyle box lw label linealpha plot! sample_times sample_vals seriestype scatter color blue red label alpha plot! sol_estimate color darkblue darkred linestyle dash lw label xlimit tspan Parameter Estimation The parameters of a model generated by Catalyst can be estimated using various packages available in the Julia ecosystem Refer  here  for more extensive information Below follows a quick tutorial of how  DiffEqFlux  can be used to fit a parameter set to data First we fetch the required packages Next we declare our model For our example we will use the Brusselator a simple oscillator We simulate our model and from the simulation generate sampled data points with added noise to which we will attempt to fit a parameter et We can plot the real solution as well as the noisy samples parameter_estimation_plot1 Next we create an optimisation function For a given initial estimate of the parameter values p this function will fit parameter values to our data samples However it will only do so on the interval 0,tend Next we will fit a parameter set to the data on the interval 0,10 We can compare this to the real solution as well as the sample data parameter_estimation_plot2 Next we use this parameter estimation as the input to the next iteration of our fitting process this time on the interval 0,20 parameter_estimation_plot3 Finally we use this estimate as the input to fit a parameter set on the full interval of sampled data parameter_estimation_plot4 The final parameter set becomes  0.9996559014056948 2.005632696191224  the real one was  1.0 2.0  Why we fit the parameters in iterations The reason we chose to fit the model on a smaller interval to begin with and then extend the interval is to avoid getting stuck in a local minimum Here specifically we chose our initial interval to be smaller than a full cycle of the oscillation If we had chosen to fit a parameter set on the full interval immediately we would have received an inferior solution parameter_estimation_plot5"},{"doctype":"documentation","id":"references/PolyChaos.evaluatePCE","title":"evaluatePCE","text":"Evaluation of polynomial chaos expansion mathsf{x  sum_{i=0}^{L x_i phi_i{\\xi_j where  L+1  length(x  and  x_j  is the  j th sample where  j=1,\\dots,m  with  m  length(ξ "},{"doctype":"documentation","id":"references/NeuralPDE.ParametersDomain","title":"ParametersDomain","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.generate_dae_jacobian","title":"generate_dae_jacobian","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.PolynomialBasis","title":"PolynomialBasis","text":"n Constructs a Polynomial basis of the form 1 x  x^(n-1 Arguments n  number of terms in the polynomial expansion"},{"doctype":"document","id":"Optimization/optimization_packages/cmaevolutionstrategy.md","title":"CMAEvolutionStrategy.jl","text":"Pkg Pkg add rosenbrock x p p x p x x x0 zeros p f rosenbrock prob f x0 p lb ub sol prob CMAEvolutionStrategyOpt CMAEvolutionStrategy.jl CMAEvolutionStrategy  is a Julia package implementing the  Covariance Matrix Adaptation Evolution Strategy algorithm  The CMAEvolutionStrategy algorithm is called by  CMAEvolutionStrategyOpt Installation OptimizationCMAEvolutionStrategy.jl To use this package install the OptimizationCMAEvolutionStrategy package Global Optimizer Without Constraint Equations The method in  CMAEvolutionStrategy  is performing global optimization on problems without constraint equations However lower and upper constraints set by  lb  and  ub  in the  OptimizationProblem  are required Example The Rosenbrock function can optimized using the  CMAEvolutionStrategyOpt  as follows"},{"doctype":"documentation","id":"references/DiffEqSensitivity.ODEInterpolatingAdjointSensitivityFunction","title":"ODEInterpolatingAdjointSensitivityFunction","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.UGradientWrapper","title":"UGradientWrapper","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.has_iv","title":"has_iv","text":""},{"doctype":"documentation","id":"references/Catalyst.hillr","title":"hillr","text":"A repressive Hill rate function"},{"doctype":"documentation","id":"references/ModelingToolkit.hasdefault","title":"hasdefault","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timeseries_point_meancov","title":"timeseries_point_meancov","text":""},{"doctype":"documentation","id":"references/SciMLBase.is_diagonal_noise","title":"is_diagonal_noise","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractNonlinearFunction","title":"AbstractNonlinearFunction","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/SciMLBase.has_expmv","title":"has_expmv","text":""},{"doctype":"documentation","id":"references/PolyChaos.coeffs","title":"coeffs","text":"returns recurrence coefficients of in matrix form"},{"doctype":"documentation","id":"references/NeuralPDE.get_argument","title":"get_argument","text":""},{"doctype":"documentation","id":"references/LinearSolve.AbstractKrylovSubspaceMethod","title":"AbstractKrylovSubspaceMethod","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractSDEAlgorithm","title":"AbstractSDEAlgorithm","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/SciMLBase.AbstractOptimizationFunction","title":"AbstractOptimizationFunction","text":"DocStringExtensions.TypeDefinition Base for types defining optimization functions"},{"doctype":"document","id":"DiffEqSensitivity/bayesian/BayesianNODE_NUTS.md","title":"Bayesian Neural ODEs: NUTS","text":"DifferentialEquations Plots AdvancedHMC MCMCChains JLD StatsPlots u0 datasize tspan tsteps range tspan tspan length datasize trueODEfunc du u p t true_A du u true_A prob_trueode trueODEfunc u0 tspan ode_data Array prob_trueode Tsit5 saveat tsteps dudt2 x p x tanh prob_neuralode dudt2 tspan Tsit5 saveat tsteps predict_neuralode p Array prob_neuralode u0 p loss_neuralode p pred predict_neuralode p loss sum abs2 ode_data pred loss pred l θ sum abs2 ode_data predict_neuralode θ sum θ θ dldθ θ x lambda Flux Zygote pullback l θ grad first lambda x grad metric DiagEuclideanMetric length prob_neuralode p h Hamiltonian metric l dldθ integrator Leapfrog find_good_stepsize h Float64 prob_neuralode p prop AdvancedHMC NUTS MultinomialTS GeneralisedNoUTurn integrator adaptor StanHMCAdaptor MassMatrixAdaptor metric StepSizeAdaptor integrator samples stats h prop Float64 prob_neuralode p adaptor progress losses map x x loss_neuralode samples i i length samples scatter losses ylabel yscale log label pl scatter tsteps ode_data color red label xlabel title scatter! tsteps ode_data color blue label k resol predict_neuralode samples end rand plot! tsteps resol alpha color red label plot! tsteps resol alpha color blue label findmin losses prediction predict_neuralode samples plot! tsteps prediction color black w label plot! tsteps prediction color black w label ylims pl scatter ode_data ode_data color red label xlabel ylabel title k resol predict_neuralode samples end rand plot! resol resol alpha color red label plot! prediction prediction color black w label ylims samples hcat samples samples_reduced samples samples_reshape reshape samples_reduced Chain_Spiral Chains samples_reshape plot Chain_Spiral autocorplot Chain_Spiral u0 datasize tspan tsteps range tspan tspan length datasize trueODEfunc du u p t true_A du u true_A prob_trueode trueODEfunc u0 tspan ode_data Array prob_trueode Tsit5 saveat tsteps dudt2 x p x tanh prob_neuralode dudt2 tspan Tsit5 saveat tsteps predict_neuralode p Array prob_neuralode u0 p loss_neuralode p pred predict_neuralode p loss sum abs2 ode_data pred loss pred l θ sum abs2 ode_data predict_neuralode θ sum θ θ dldθ θ x lambda Flux Zygote pullback l θ grad first lambda x grad metric DiagEuclideanMetric length prob_neuralode p h Hamiltonian metric l dldθ integrator Leapfrog find_good_stepsize h Float64 prob_neuralode p prop AdvancedHMC NUTS MultinomialTS GeneralisedNoUTurn integrator adaptor StanHMCAdaptor MassMatrixAdaptor metric StepSizeAdaptor integrator samples stats h prop Float64 prob_neuralode p adaptor progress samples hcat samples samples_reduced samples samples_reshape reshape samples_reduced Chain_Spiral Chains samples_reshape plot Chain_Spiral autocorplot Chain_Spiral pl scatter tsteps ode_data color red label xlabel title scatter! tsteps ode_data color blue label k resol predict_neuralode samples end rand plot! tsteps resol alpha color red label plot! tsteps resol alpha color blue label findmin losses prediction predict_neuralode samples plot! tsteps prediction color black w label plot! tsteps prediction color black w label ylims pl scatter ode_data ode_data color red label xlabel ylabel title k resol predict_neuralode samples end rand plot! resol resol alpha color red label plot! prediction prediction color black w label ylims Bayesian Neural ODEs NUTS In this tutorial we show how the DiffEqFlux.jl library in Julia can be seamlessly combined with Bayesian estimation libraries like AdvancedHMC.jl and Turing.jl This enables converting Neural ODEs to Bayesian Neural ODEs which enables us to estimate the error in the Neural ODE estimation and forecasting In this tutorial a working example of the Bayesian Neural ODE NUTS sampler is shown For more details please refer to  Bayesian Neural Ordinary Differential Equations  Copy-Pasteable Code Before getting to the explanation here's some code to start with We will follow wil a full explanation of the definition and training process Time Series Plots Contour Plots Chain Mixing Plot Auto-Correlation Plot Explanation Step 1 Get the data from the Spiral ODE example Step 2 Define the Neural ODE architecture Note that this step potentially offers a lot of flexibility in the number of layers number of units in each layer It may not necessarily be true that a 100 units architecture is better at prediction/forecasting than a 50 unit architecture On the other hand a complicated architecture can take a huge computational time without increasing performance Step 3 Define the loss function for the Neural ODE Step 4 Now we start integrating the Bayesian estimation workflow as prescribed by the AdvancedHMC interface with the NeuralODE defined above The Advanced HMC interface requires us to specify a the hamiltonian log density and its gradient  b the sampler and c the step size adaptor function For the hamiltonian log density we use the loss function The θ*θ term denotes the use of Gaussian priors The user can make several modifications to Step 4 The user can try different acceptance ratios warmup samples and posterior samples One can also use the Variational Inference ADVI framework which doesn't work quite as well as NUTS The SGLD Stochastic Langevin Gradient Descent sampler is seen to have a better performance than NUTS Have a look at https://sebastiancallh.github.io/post/langevin for a quick introduction to SGLD We use the NUTS sampler with a acceptance ratio of δ 0.45 in this example In addition we use Nesterov Dual Averaging for the Step Size adaptation We sample using 500 warmup samples and 500 posterior samples Step 5 Plot diagnostics A Plot chain object and auto-correlation plot of the first 5 parameters B Plot retrodicted data"},{"doctype":"documentation","id":"references/LinearSolve.IterativeSolversJL_CG","title":"IterativeSolversJL_CG","text":""},{"doctype":"documentation","id":"references/SciMLBase.u_modified!","title":"u_modified!","text":"Sets  bool  which states whether a change to  u  occurred allowing the solver to handle the discontinuity By default this is assumed to be true if a callback is used This will result in the re-calculation of the derivative at  t+dt  which is not necessary if the algorithm is FSAL and  u  does not experience a discontinuous change at the end of the interval Thus if  u  is unmodified in a callback a single call to the derivative calculation can be eliminated by  u_modified!(integrator,false "},{"doctype":"documentation","id":"references/GlobalSensitivity.RegressionGSAResult","title":"RegressionGSAResult","text":""},{"doctype":"documentation","id":"references/SciMLBase.expmv!","title":"expmv!","text":""},{"doctype":"document","id":"DiffEqSensitivity/optimal_control/optimal_control.md","title":"[Solving Optimal Control Problems with Universal Differential Equations]( optcontrol)","text":"Lux DifferentialEquations OptimizationOptimJL OptimizationFlux Plots Statistics Random rng Random default_rng tspan ann Lux Lux Dense tanh Lux Dense tanh Lux Dense θ st Lux setup rng ann dxdt_ dx x p t x1 x2 x dx x dx ann t p st x0 ts Float32 collect tspan prob dxdt_ x0 tspan θ prob Vern9 abstol reltol predict_adjoint θ Array prob Vern9 p θ saveat ts sensealg autojacvec loss_adjoint θ x predict_adjoint θ mean abs2 x mean abs2 x mean abs2 first ann t θ t ts l loss_adjoint θ cb θ l println l p plot prob p θ Tsit5 saveat ylim lw plot! p ts first ann t θ t ts label lw display p cb θ l loss1 loss_adjoint θ adtype optf x p loss_adjoint x adtype optprob optf θ res1 optprob ADAM cb cb maxiters optprob2 optf res1 u res2 optprob2 BFGS maxiters allow_f_increases loss_adjoint θ x predict_adjoint θ mean abs2 x mean abs2 x mean abs2 first ann t θ t ts optf3 x p loss_adjoint x adtype optprob3 optf3 res2 u res3 optprob3 BFGS maxiters allow_f_increases l loss_adjoint res3 u cb res3 u l p plot prob p res3 u Tsit5 saveat ylim lw plot! p ts first ann t res3 u t ts label lw savefig Solving Optimal Control Problems with Universal Differential Equations  optcontrol Here we will solve a classic optimal control problem with a universal differential equation Let x^{′′  u^3(t where we want to optimize our controller  u(t  such that the following is minimized L(\\theta  sum_i Vert 4  x(t_i Vert  2 Vert x^\\prime(t_i Vert  Vert u(t_i Vert where  i  is measured on 0,8 at 0.01 intervals To do this we rewrite the ODE in first order form begin{aligned}\nx^\\prime  v \nv^′  u^3(t \n\\end{aligned and thus L(\\theta  sum_i Vert 4  x(t_i Vert  2 Vert v(t_i Vert  Vert u(t_i Vert is our loss function on the first order system We thus choose a neural network form for  u  and optimize the equation with respect to this loss Note that we will first reduce control cost the last term by 10x in order to bump the network out of a local minimum This looks like Now that the system is in a better behaved part of parameter space we return to the original loss function to finish the optimization"},{"doctype":"documentation","id":"references/Catalyst._symbol_to_var","title":"_symbol_to_var","text":""},{"doctype":"documentation","id":"references/SciMLBase.DEElement","title":"DEElement","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/DiffEqSensitivity.compute_d!","title":"compute_d!","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.AdjointDiffCache","title":"AdjointDiffCache","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.modify_basis_matrix!","title":"modify_basis_matrix!","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.jacobian","title":"jacobian","text":""},{"doctype":"document","id":"NeuralOperators/apis.md","title":"APIs","text":"APIs Layers Operator convolutional layer F(s  mathcal{F  v(x  \nF'(s  g(F(s \nv'(x  mathcal{F}^{-1  F'(s  where  v(x  and  v'(x  denotes input and output function  mathcal{F  cdot    mathcal{F}^{-1  cdot   are Fourier transform inverse Fourier transform respectively Function  g  is a linear transform for lowering Fouier modes Reference  FNO2021 Operator kernel layer v_{t+1}(x  sigma(W v_t(x  mathcal{K  v_t(x   where  v_t(x  is the input function for  t th layer and  mathcal{K  cdot   denotes spectral convolutional layer Activation function  sigma  can be arbitrary non-linear function Reference  FNO2021 Graph kernel layer v_{t+1}(x_i  sigma(W v_t(x_i  frac{1}{|\\mathcal{N}(x_i sum_{x_j in mathcal{N}(x_i kappa  v_t(x_i v_t(x_j   where  v_t(x_i  is the input function for  t th layer  x_i  is the node feature for  i th node and  mathcal{N}(x_i  represents the neighbors for  x_i  Activation function  sigma  can be arbitrary non-linear function Reference  NO2020 Models Fourier neural operator Reference  FNO2021 Markov neural operator Reference  MNO2021"},{"doctype":"documentation","id":"references/DiffEqFlux.paramlength","title":"paramlength","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.VariableNoiseType","title":"VariableNoiseType","text":""},{"doctype":"documentation","id":"references/PolyChaos.w_meixner_pollaczek","title":"w_meixner_pollaczek","text":""},{"doctype":"documentation","id":"references/SciMLBase.initialize_dae!","title":"initialize_dae!","text":"initialize_dae!(integrator::DEIntegrator,initializealg  integrator.initializealg Runs the DAE initialization to find a consistent state vector The optional argument  initializealg  can be used to specify a different initialization algorithm to use"},{"doctype":"documentation","id":"references/ModelingToolkit.has_var_to_name","title":"has_var_to_name","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.augment","title":"augment","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.VariableUnit","title":"VariableUnit","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.backsolve_checkpoint_callbacks","title":"backsolve_checkpoint_callbacks","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.getvar","title":"getvar","text":""},{"doctype":"document","id":"Surrogates/LinearSurrogate.md","title":"LinearSurrogate","text":"Linear Surrogate Linear Surrogate is a linear approach to modeling the relationship between a scalar response or dependent variable and one or more explanatory variables We will use Linear Surrogate to optimize following function f(x  sin(x  log(x  First of all we have to import these two packages  Surrogates  and  Plots  Sampling We choose to sample f in 20 points between 0 and 10 using the  sample  function The sampling points are chosen using a Sobol sequence this can be done by passing  SobolSample  to the  sample  function Building a Surrogate With our sampled points we can build the  Linear Surrogate  using the  LinearSurrogate  function We can simply calculate  linear_surrogate  for any value Now we will simply plot  linear_surrogate  Optimizing Having built a surrogate we can now use it to search for minimas in our original function  f  To optimize using our surrogate we call  surrogate_optimize  method We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique Linear Surrogate tutorial ND First of all we will define the  Egg Holder  function we are going to build surrogate for Notice one how its argument is a vector of numbers one for each coordinate and its output is a scalar Sampling Let's define our bounds this time we are working in two dimensions In particular we want our first dimension  x  to have bounds  10 5  and  0 15  for the second dimension We are taking 50 samples of the space using Sobol Sequences We then evaluate our function on all of the sampling points Building a surrogate Using the sampled points we build the surrogate the steps are analogous to the 1-dimensional case Optimizing With our surrogate we can now search for the minimas of the function Notice how the new sampled points which were created during the optimization process are appended to the  xys  array This is why its size changes"},{"doctype":"document","id":"Surrogates/rosenbrock.md","title":"Rosenbrock function","text":"Rosenbrock function The Rosenbrock function is defined as  f(x  sum_{i=1}^{d-1 x_{i+1}-x_i)^2  x_i  1)^2 I will treat the 2D version which is commonly defined as  f(x,y  1-x)^2  100(y-x^2)^2  Let's import Surrogates and Plots Define the objective function Let's plot it Fitting different Surrogates Plotting"},{"doctype":"documentation","id":"references/Surrogates.Kriging","title":"Kriging","text":"Gives the current estimate for array val with respect to the Kriging object k Gives the current estimate for val with respect to the Kriging object k Constructor for type Kriging Arguments x,y sampled points p value between 0 and 2 modelling the smoothness of the function being approximated 0 rough  2 C^infinity Constructor for Kriging surrogate x,y sampled points p array of values 0<=p<2 modeling the smoothness of the function being approximated in the i-th variable low p  rough high p  smooth theta array of values  0 modeling how much the function is changing in the i-th variable"},{"doctype":"document","id":"DiffEqSensitivity/ad_examples/adjoint_continuous_functional.md","title":"[Adjoint Sensitivity Analysis of Continuous Functionals]( continuous_loss)","text":"f du u p t du dx p u p u u du dy p u u u p prob f p sol prob DP8 g u p t sum u dg out u p t out u u out u u res sol Vern9 g nothing dg abstol reltol iabstol ireltol QuadGK G p tmp_prob prob p p sol tmp_prob Vern9 abstol reltol res err quadgk t sum sol t atol rtol res res2 ForwardDiff gradient G res3 Calculus gradient G Adjoint Sensitivity Analysis of Continuous Functionals  continuous_loss The automatic differentiation tutorial  auto_diff demonstrated how to use AD packages like ForwardDiff.jl and Zygote.jl to compute derivatives of differential equation solutions with respect to initial conditions and parameters The subsequent direct sensitivity analysis tutorial  direct_sensitivity showed how to directly use the SciMLSensitivity.jl internals to define and solve the augmented differential equation systems which are used in the automatic differentiation process While these internal functions give more flexibility the previous demonstration focused on a case which was possible via automatic differentiation discrete cost functionals What is meant by discrete cost functionals is differentiation of a cost which uses a finite number of time points In the automatic differentiation case these finite time points are the points returned by  solve  i.e those chosen by the  saveat  option in the solve call In the direct adjoint sensitivity tooling these were the time points chosen by the  ts  vector However there is an expanded set of cost functionals supported by SciMLSensitivity.jl continuous cost functionals which are not possible through automatic differentiation interfaces In an abstract sense a continuous cost functional is a total cost  G  defined as the integral of the instantanious cost  g  at all time points In other words the total cost is defined as G(u,p)=G(u(\\cdot,p))=\\int_{t_{0}}^{T}g(u(t,p),p)dt Notice that this cost function cannot accurately be computed using only estimates of  u  at discrete time points The purpose of this tutorial is to demonstrate how such cost functionals can be easily evaluated using the direct sensitivity analysis interfaces Example Continuous Functionals with Forward Sensitivity Analysis via Interpolation Evaluating continuous cost functionals with forward sensitivity analysis is rather straightforward since one can simply use the fact that the solution from  ODEForwardSensitivityProblem  is continuous when  dense=true  For example gives a continuous solution  sol(t  with the derivative at each time point This can then be used to define a continuous cost function via  Quadrature.jl  though the derivative would need to be defined by hand using the extra sensitivity terms Example Continuous Adjoints on an Energy Functional Continuous adjoints on a continuous functional are more automatic than forward mode In this case we'd like to calculate the adjoint sensitivity of the scalar energy functional G(u,p)=\\int_{0}^{T}\\frac{\\sum_{i=1}^{n}u_{i}^{2}(t)}{2}dt which is Notice that the gradient of this function with respect to the state  u  is To get the adjoint sensitivities we call Notice that we can check this against autodifferentiation and numerical differentiation as follows"},{"doctype":"documentation","id":"references/GlobalSensitivity.generate_ff_design_matrix","title":"generate_ff_design_matrix","text":""},{"doctype":"documentation","id":"references/Catalyst.esc_dollars!","title":"esc_dollars!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.has_ps","title":"has_ps","text":""},{"doctype":"document","id":"DiffEqSensitivity/manual/differential_equation_sensitivities.md","title":"[Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)]( sensitivity_diffeq)","text":"OrdinaryDiffEq Zygote fiip du u p t du dx p u p u u du dy p u p u u p u0 prob fiip u0 p sol prob Tsit5 loss u0 p sum prob Tsit5 u0 u0 p p saveat du0 dp Zygote gradient loss u0 p loss u0 p sum prob Tsit5 u0 u0 p p saveat du0 dp Zygote gradient loss u0 p Sensitivity Algorithms for Differential Equations with Automatic Differentiation AD  sensitivity_diffeq DiffEqSensitivity.jl's high level interface allows for specifying a sensitivity algorithm  sensealg  to control the method by which  solve  is differentiated in an automatic differentiation AD context by a compatible AD library The underlying algorithms then use the direct interface methods like  ODEForwardSensitivityProblem  and  adjoint_sensitivities  to compute the derivatives without requiring the user to do any of the setup Current AD libraries whose calls are captured by the sensitivity system are Zygote.jl Diffractor.jl Using and Controlling Sensitivity Algorithms within AD Take for example this simple differential equation solve on Lotka-Volterra This will compute the gradient of the loss function sum of the values of the solution to the ODE at timepoints dt=0.1 using an adjoint method where  du0  is the derivative of the loss function with respect to the initial condition and  dp  is the derivative of the loss function with respect to the parameters Because the gradient is calculated by  Zygote.gradient  and Zygote.jl is one of the compatible AD libraries this derivative calculation will be captured by the  sensealg  system and one of DiffEqSensitivity.jl's adjoint overloads will be used to compute the derivative By default if the  sensealg  keyword argument is not defined then a smart polyalgorithm is used to automatically determine the most appropriate method for a given equation Likewise the  sensealg  argument can be given to directly control the method by which the derivative is computed For example Choosing a Sensitivity Algorithm There are two classes of algorithms the continuous sensitivity analysis methods and the discrete sensitivity analysis methods direct automatic differentiation Generally Continuous sensitivity analysis are more efficient while the discrete   sensitivity analysis is more stable  full discussion is in the appendix of that paper Continuous sensitivity analysis methods only support a subset of equations which currently includes ODEProblem with mass matrices for differential-algebraic equations DAEs SDEProblem SteadyStateProblem  NonlinearProblem Discrete sensitivity analysis methods only support a subset of algorithms namely the pure Julia solvers which are written generically For an analysis of which methods will be most efficient for computing the solution derivatives for a given problem consult our analysis  in this arxiv paper  A general rule of thumb is ForwardDiffSensitivity  is the fastest for differential equations with small numbers of parameters 100 and can be used on any differential equation solver that is native Julia If the chosen ODE solver is not compatible with direct automatic differentiation  ForwardSensitivty  may be used instead Adjoint senstivity analysis is the fastest when the number of parameters is sufficiently large There are three configurations of note Using  QuadratureAdjoint  is the fastest but uses the most memory  BacksolveAdjoint  uses the least memory but on very stiff problems it may be unstable and require a lot of checkpoints while  InterpolatingAdjoint  is in the middle allowing checkpointing to control total memory use The methods which use direct automatic differentiation  ReverseDiffAdjoint   TrackerAdjoint   ForwardDiffSensitivity  and  ZygoteAdjoint  support the full range of DifferentialEquations.jl features SDEs DDEs events etc but only work on native Julia solvers For non-ODEs with large numbers of parameters  TrackerAdjoint  in out-of-place form may be the best performer on GPUs and  ReverseDiffAdjoint TrackerAdjoint  is able to use a  TrackedArray  form with out-of-place functions  du  f(u,p,t  but requires an  Array{TrackedReal  form for  f(du,u,p,t  mutating  du  The latter has much more overhead and should be avoided if possible Thus if solving non-ODEs with lots of parameters using  TrackerAdjoint  with an out-of-place definition may be the current best option Note Compatibility with direct automatic differentiation algorithms  ForwardDiffSensitivity   ReverseDiffAdjoint  etc can be queried using the  SciMLBase.isautodifferentiable(::SciMLAlgorithm  trait function If the chosen algorithm is a continuous sensitivity analysis algorithm then an  autojacvec  argument can be given for choosing how the Jacobian-vector product  J*v  or vector-Jacobian product  J'*v  calculation is computed For the forward sensitivity methods  autojacvec=true  is the most efficient though  autojacvec=false  is slightly less accurate but very close in efficiency For adjoint methods it's more complicated and dependent on the way that the user's  f  function is implemented EnzymeVJP  is the most efficient if it's applicable on your equation If your function has no branching no if statements but uses mutation  ReverseDiffVJP(true  will be the most efficient after Enzyme Otherwise  ReverseDiffVJP  but you may wish to proceed with eliminating mutation as without compilation enabled this can be slow If your on the CPU or GPU and your function is very vectorized and has no mutation choose  ZygoteVJP  Else fallback to  TrackerVJP  if Zygote does not support the function Special Notes on Non-ODE Differential Equation Problems While all of the choices are compatible with ordinary differential equations specific notices apply to other forms Differential-Algebraic Equations We note that while all 3 are compatible with index-1 DAEs via the  derivation in the universal differential equations paper  note the reinitialization we do not recommend  BacksolveAdjoint  one DAEs because the stiffness inherent in these problems tends to cause major difficulties with the accuracy of the backwards solution due to reinitialization of the algebraic variables Stochastic Differential Equations We note that all of the adjoints except  QuadratureAdjoint  are applicable to stochastic differential equations Delay Differential Equations We note that only the discretize-then-optimize methods are applicable to delay differential equations Constant lag and variable lag delay differential equation parameters can be estimated but the lag times themselves are unable to be estimated through these automatic differentiation techniques Hybrid Equations Equations with events/callbacks and Jump Equations ForwardDiffSensitivity  can differentiate code with callbacks when  convert_tspan=true   ForwardSensitivity  is not compatible with hybrid equations The shadowing methods are not compatible with callbacks All methods based on discrete adjoint sensitivity analysis via automatic differentiation like  ReverseDiffAdjoint   TrackerAdjoint  or  QuadratureAdjoint  are fully compatible with events This applies to ODEs SDEs DAEs and DDEs The continuous adjoint sensitivities  BacksolveAdjoint   InterpolatingAdjoint  and  QuadratureAdjoint  are compatible with events for ODEs  BacksolveAdjoint  and  InterpolatingAdjoint  can also handle events for SDEs Use  BacksolveAdjoint  if the event terminates the time evolution and several states are saved Currently the continuous adjoint sensitivities do not support multiple events per time point Manual VJPs Note that when defining your differential equation the vjp can be manually overwritten by providing the  AbstractSciMLFunction  definition with  a  vjp(u,p,t  that returns a tuple  f(u,p,t),v->J*v  in the form of  ChainRules.jl  When this is done the choice of  ZygoteVJP  will utilize your VJP function during the internal steps of the adjoint This is useful for models where automatic differentiation may have trouble producing optimal code This can be paired with  ModelingToolkit.jl  for producing hyper-optimized sparse and parallel VJP functions utilizing the automated symbolic conversions Sensitivity Algorithms The following algorithm choices exist for  sensealg  See the sensitivity mathematics page  sensitivity_math for more details on the definition of the methods Vector-Jacobian Product VJP Choices Noise VJP Choices More Details on Sensitivity Algorithm Choices The following section describes a bit more details to consider when choosing a sensitivity algorithm Optimize-then-Discretize The original neural ODE paper  popularized optimize-then-discretize with O(1 adjoints via backsolve This is the methodology  BacksolveAdjoint  When training non-stiff neural ODEs  BacksolveAdjoint  with  ZygoteVJP  is generally the fastest method Additionally this method does not require storing the values of any intermediate points and is thus the most memory efficient However  BacksolveAdjoint  is prone to instabilities whenever the Lipschitz constant is sufficiently large like in stiff equations PDE discretizations and many other contexts so it is not used by default When training a neural ODE for machine learning applications the user should try  BacksolveAdjoint  and see if it is sufficiently accurate on their problem More details on this topic can be found in  Stiff Neural Ordinary Differential Equations Note that DiffEqFlux's implementation of  BacksolveAdjoint  includes an extra feature  BacksolveAdjoint(checkpointing=true  which mixes checkpointing with  BacksolveAdjoint  What this method does is that at  saveat  points values from the forward pass are saved Since the reverse solve should numerically be the same as the forward pass issues with divergence of the reverse pass are mitigated by restarting the reverse pass at the  saveat  value from the forward pass This reduces the divergence and can lead to better gradients at the cost of higher memory usage due to having to save some values of the forward pass This can stabilize the adjoint in some applications but for highly stiff applications the divergence can be too fast for this to work in practice To avoid the issues of backwards solving the ODE  InterpolatingAdjoint  and  QuadratureAdjoint  utilize information from the forward pass By default these methods utilize the  continuous solution  provided by DifferentialEquations.jl in the calculations of the adjoint pass  QuadratureAdjoint  uses this to build a continuous function for the solution of adjoint equation and then performs an adaptive quadrature via  Quadrature.jl  while  InterpolatingAdjoint  appends the integrand to the ODE so it's computed simultaneously to the Lagrange multiplier When memory is not an issue we find that the  QuadratureAdjoint  approach tends to be the most efficient as it has a significantly smaller adjoint differential equation and the quadrature converges very fast but this form requires holding the full continuous solution of the adjoint which can be a significant burden for large parameter problems The  InterpolatingAdjoint  is thus a compromise between memory efficiency and compute efficiency and is in the same spirit as  CVODES  However if the memory cost of the  InterpolatingAdjoint  is too high checkpointing can be used via  InterpolatingAdjoint(checkpointing=true  When this is used the checkpoints default to  sol.t  of the forward pass i.e the saved timepoints usually set by  saveat  Then in the adjoint intervals of  sol.t[i-1  to  sol.t[i  are re-solved in order to obtain a short interpolation which can be utilized in the adjoints This at most results in two full solves of the forward pass but dramatically reduces the computational cost while being a low-memory format This is the preferred method for highly stiff equations when memory is an issue i.e stiff PDEs or large neural DAEs For forward-mode the  ForwardSensitivty  is the version that performs the optimize-then-discretize approach In this case  autojacvec  corresponds to the method for computing  J*v  within the forward sensitivity equations which is either  true  or  false  for whether to use Jacobian-free forward-mode AD via ForwardDiff.jl or Jacobian-free numerical differentiation Discretize-then-Optimize In this approach the discretization is done first and then optimization is done on the discretized system While traditionally this can be done discrete sensitivity analysis this is can be equivalently done by automatic differentiation on the solver itself  ReverseDiffAdjoint  performs reverse-mode automatic differentiation on the solver via  ReverseDiff.jl   ZygoteAdjoint  performs reverse-mode automatic differentiation on the solver via  Zygote.jl  and  TrackerAdjoint  performs reverse-mode automatic differentiation on the solver via  Tracker.jl  In addition  ForwardDiffSensitivty  performs forward-mode automatic differentiation on the solver via  ForwardDiff.jl  We note that many studies have suggested that  this approach produces   more accurate gradients than the optimize-than-discretize approach"},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.NO_METADATA","title":"NO_METADATA","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractSteadyStateSolution","title":"AbstractSteadyStateSolution","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.pss_graph_modia!","title":"pss_graph_modia!","text":""},{"doctype":"documentation","id":"references/PolyChaos.JacobiMeasure","title":"JacobiMeasure","text":""},{"doctype":"documentation","id":"references/Optimization.SciMLSolution","title":"SciMLSolution","text":""},{"doctype":"documentation","id":"references/NeuralPDE.DESolution","title":"DESolution","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.calculate_factorized_W","title":"calculate_factorized_W","text":"sys Calculate the factorized W-matrix of a system Returns a matrix of  Num  instances The result from the first call will be cached in the system object"},{"doctype":"documentation","id":"references/ModelingToolkit.get_tgrad","title":"get_tgrad","text":""},{"doctype":"document","id":"ModelingToolkit/tutorials/acausal_components.md","title":"[Acausal Component-Based Modeling the RC Circuit]( acausal)","text":"Acausal Component-Based Modeling the RC Circuit  acausal In this tutorial we will build a hierarchical acausal component-based model of the RC circuit The RC circuit is a simple example where we connect a resistor and a capacitor  Kirchoff's laws  are then applied to state equalities between currents and voltages This specifies a differential-algebraic equation DAE system where the algebraic equations are given by the constraints and equalities between different component variables We then simplify this to an ODE by eliminating the equalities before solving Let's see this in action Note This tutorial teaches how to build the entire RC circuit from scratch However to simulate electrical components with more ease check out the  ModelingToolkitStandardLibrary.jl  which includes a  tutorial for simulating RC circuits with pre-built components Copy-Paste Example Explanation Building the Component Library For each of our components we use a Julia function which emits an  ODESystem  At the top we start with defining the fundamental qualities of an electrical circuit component At every input and output pin a circuit component has two values the current at the pin and the voltage Thus we define the  Pin  component connector to simply be the values there Whenever two  Pin s in a circuit are connected together the system satisfies Kirchoff's laws](https en.wikipedia.org/wiki/Kirchhoff%27s_circuit_laws i.e that currents sum to zero and voltages across the pins are equal  connect  Flow  informs MTK that currents ought to sum to zero and by default variables are equal in a connection Note that this is an incompletely specified ODESystem it cannot be simulated on its own because the equations for  v(t  and  i(t  are unknown Instead this just gives a common syntax for receiving this pair with some default values Notice that in a component we define the  name  as a keyword argument this is because later we will generate different  Pin  objects with different names to correspond to duplicates of this topology with unique variables One can then construct a  Pin  like or equivalently using the  named  helper macro Next we build our ground node A ground node is just a pin that is connected to a constant voltage reservoir typically taken to be  V=0  Thus to define this component we generate an  ODESystem  with a  Pin  subcomponent and specify that the voltage in such a  Pin  is equal to zero This gives Next we build a  OnePort  an abstraction for all simple electrical component with two pins The voltage difference between the positive pin and the negative pin is the voltage of the component the current between two pins must sum to zero and the current of the component equals to the current of the positive pin Next we build a resistor A resistor is an object that has two  Pin s the positive and the negative pins and follows Ohm's law  v  i*r  The voltage of the resistor is given as the voltage difference across the two pins while by conservation of charge we know that the current in must equal the current out which means no matter the direction of the current flow the sum of the currents must be zero This leads to our resistor equations Notice that we have created this system with a default parameter  R  for the resistor's resistance By doing so if the resistance of this resistor is not overridden by a higher level default or overridden at  ODEProblem  construction time this will be the value of the resistance Also note the use of  unpack  and  extend  For the  Resistor  we want to simply inherit  OnePort s equations and states and extend them with a new equation ModelingToolkit makes a new namespaced variable  oneport₊v(t  when using the syntax  oneport.v  and we can use  unpack  avoid the namespacing Using our knowledge of circuits we similarly construct the  Capacitor  Now we want to build a constant voltage electrical source term We can think of this as similarly being a two pin object where the object itself is kept at a constant voltage essentially generating the electrical current We would then model this as Connecting and Simulating Our Electrical Circuit Now we are ready to simulate our circuit Let's build our four components a  resistor   capacitor   source  and  ground  term For simplicity we will make all of our parameter values 1 This is done by Finally we will connect the pieces of our circuit together Let's connect the positive pin of the resistor to the source the negative pin of the resistor to the capacitor and the negative pin of the capacitor to a junction between the source and the ground This would mean our connection equations are Finally we build our four component model with these connection rules Note that we can also specify the subsystems in a vector This model is acasual because we have not specified anything about the causality of the model We have simply specified what is true about each of the variables This forms a system of differential-algebraic equations DAEs which define the evolution of each state of the system The equations are the states are and the parameters are Simplifying and Solving this System This system could be solved directly as a DAE using  one of the DAE solvers   from DifferentialEquations.jl  However let's take a second to symbolically simplify the system before doing the solve Although we can use ODE solvers that handles mass matrices to solve the above system directly we want to run the  structural_simplify  function first as it eliminates many unnecessary variables to build the leanest numerical representation of the system Let's see what it does here After structural simplification we are left with a system of only two equations with two state variables One of the equations is a differential equation while the other is an algebraic equation We can then give the values for the initial conditions of our states and solve the system by converting it to an ODEProblem in mass matrix form and solving it with an  ODEProblem mass matrix   DAE solver  This is done as follows Since we have run  structural_simplify  MTK can numerically solve all the unreduced algebraic equations numerically using the  ODAEProblem  note the letter  A  Notice that this solves the whole system by only solving for one variable However what if we wanted to plot the timeseries of a different variable Do not worry that information was not thrown away Instead transformations like  structural_simplify  simply change state variables into  observed  variables Let's see what our observed variables are These are explicit algebraic equations which can then be used to reconstruct the required variables on the fly This leads to dramatic computational savings because implicitly solving an ODE scales like O(n^3 so making there be as few states as possible is good The solution object can be accessed via its symbols For example let's retrieve the voltage of the resistor over time or we can plot the timeseries of the resistor's voltage"},{"doctype":"document","id":"NeuralPDE/solvers/pinns.md","title":"Physics-Informed Neural Networks","text":"pde_system eq bcs domains param discretization strategy init_params nothing nothing derivative nothing prob pde_system discretization Physics-Informed Neural Networks Using the PINNs solver we can solve general nonlinear PDEs generalPDE with suitable boundary conditions bcs where time t is a special component of x and Ω contains the temporal domain PDEs are defined using the ModelingToolkit.jl  PDESystem  Here  eq  is the equation  bcs  represents the boundary conditions  param  is the parameter of the equation like  x,y  and  var  represents variables like  u  The  PhysicsInformedNN  discretizer is defined as Keyword arguments chain  is a Flux.jl chain where the input of NN equals the number of dimensions and output equals the number of equations in the system strategy  determines which training strategy will be used init_params  is the initial parameter of the neural network If nothing then automatically generated from the neural network phi  is a trial solution derivative  is a method that calculates the derivative The method  discretize  interprets from the ModelingToolkit PDE form to the PINNs Problem which outputs an  OptimizationProblem  for  Optimization.jl  Training strategy List of training strategies that are available now GridTraining(dx  Initialize points on a lattice uniformly spaced via  dx  If  dx  is a scalar then  dx  corresponds to the spacing in each direction If  dx  is a vector then it should be sized to match the number of dimensions and corresponds to the spacing per direction StochasticTraining(points;bcs_points  points   points  is number of stochastically sampled points from the domain  bcs_points  is number of points for boundary conditions(by default it equals  points  In each optimization iteration we randomly select a new subset of points from a full training set QuasiRandomTraining(points;bcs_points  points sampling_alg  UniformSample resampling  true minibatch=500  The training set is generated on quasi-random low discrepency sequences  points  is the number of quasi-random points in every subset or set  bcs_points  is number of points for boundary conditions(by default it equals  points   sampling_alg  is the quasi-Monte Carlo sampling algorithm  if resampling  false  the full training set is generated in advance before training and at each iteration one subset is randomly selected out of the batch minibatch  is the number of subsets in full training set The number of the total points is  length(lb  points  minibatch  where  lb  is the lower bound and  length(lb  is the dimensionality  if resampling  true  the training set isn't generated beforehand and one set of quasi-random points is generated directly at each iteration in runtime In this case  minibatch  has no effect See the  QuasiMonteCarlo.jl  for the full set of quasi-random sampling algorithms which are available QuadratureTraining(;quadrature_alg=CubatureJLh(),reltol 1e-6,abstol 1e-3,maxiters=1e3,batch=100  The loss is computed as an approximation of the integral of the PDE loss at each iteration using  adaptive quadrature methods  via the differentiable  Quadrature.jl  quadrature_alg  is quadrature algorithm reltol  relative tolerance abstol  absolute tolerance maxiters  the maximum number of iterations in quadrature algorithm batch  the preferred number of points to batch If  batch   0 the number of points in the batch is determined adaptively by the algorithm See the  Quadrature.jl  documentation for the choices of quadrature methods Transfer Learning with neural_adapter Transfer learning is a machine learning technique where a model trained on one task is re-purposed on a second related task neural_adapter(loss_function,initθ,pde_system,strategy  the method that trains a neural network using the results from one already obtained prediction Keyword arguments loss_function the body of loss function initθ  the initial parameter of new neural networks pde_system  PDE are defined using the ModelingToolkit.jl  strategy  determines which training strategy will be used neural_adapter(loss_functions::Array,initθ,pde_systems::Array,strategy  the method that trains a neural network using the results from many already obtained predictions Keyword arguments loss_functions  the body of loss functions initθ  the initial parameter of the neural network pde_systems  PDEs are defined using the ModelingToolkit.jl strategy  determines which training strategy will be used Low-level API These additional methods exist to help with introspection symbolic_discretize(pde_system,discretization  This method is the same as  discretize  but instead returns the unevaluated Julia function to allow the user to see the generated training code build_symbolic_loss_function(eqs,indvars,depvars phi derivative initθ bc_indvars=nothing  return symbolic inner representation for the loss function Keyword arguments eqs  equation or equations indvars  independent variables the parameter of the equation depvars  dependent variables phi trial solution derivative  method that calculates the derivative initθ  the initial parameter of the neural network bc_indvars  independent variables for each boundary conditions build_symbolic_equation(eq,indvars,depvars  return symbolic inner representation for the equation build_loss_function(eqs indvars depvars phi derivative initθ bc_indvars=nothing  returns the body of loss function which is the executable Julia function for the main equation or boundary condition get_loss_function(loss_functions train_sets strategy::TrainingStrategies τ  nothing  return the executable loss function Keyword arguments loss_functions  the body of loss function which is created using   build_loss_function  train_sets  training sets strategy  training strategy τ  normalizing coefficient for loss function If  τ  is nothing then it is automatically set to  1/n  where  n  is the number of points checked in the loss function get_phi(chain parameterless_type_θ  return function for trial solution chain  neural network parameterless_type_θ  number format type(Float64/Float32 of weights of neural network get_numeric_derivative  return method that calculates the derivative generate_training_sets(domains,dx,bcs,_indvars::Array,_depvars::Array  return training sets for equations and boundary condition that is used for GridTraining strategy get_variables(eqs,_indvars::Array,_depvars::Array  returns all variables that are used in each equations or boundary condition get_argument(eqs,_indvars::Array,_depvars::Array  returns all arguments that are used in each equations or boundary condition get_bounds(domains,bcs,_indvars::Array,_depvars::Array  return pairs with lower and upper bounds for all domains It is used for all non-grid training strategy StochasticTraining QuasiRandomTraining QuadratureTraining See how this can be used in  Debugging  section or  2-D Burgers equation low-level API   examples"},{"doctype":"documentation","id":"references/NeuralOperators","title":"NeuralOperators","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.inputs_to_parameters!","title":"inputs_to_parameters!","text":""},{"doctype":"documentation","id":"references/PolyChaos.LegendreOrthoPoly","title":"LegendreOrthoPoly","text":""},{"doctype":"document","id":"Catalyst/tutorials/symbolic_stoich.md","title":"Parametric Stoichiometry","text":"Parametric Stoichiometry Catalyst supports stoichiometric coefficients that involve parameters species or even general expressions In this tutorial we show several examples of how to use parametric stoichiometry and discuss several caveats to be aware of Note this tutorial requires ModelingToolkit v8.5.4 or greater to work properly Using Symbolic Stoichiometry Let's first consider a simple reversible reaction where the number of reactants is a parameter and the number of products is the product of two parameters Note as always the  reaction_network  macro sets all symbols not declared to be parameters to be species so that in this example we have two species  A  and  B  and four parameters In addition the stoichiometry is applied to the right most symbol in a given term i.e in the first equation the substrate  A  has stoichiometry  m  and the product  B  has stoichiometry  m*n  For example in we see three species  A,B,C  however  A  is treated as the stoichiometric coefficient of  C  i.e We could have equivalently specified our systems directly via the Catalyst API For example for  revsys  we would could use which can be simplified using the  reaction  macro to Note the  reaction  macro assumes all symbols are parameters except the right most symbols in the reaction line i.e  A  and  B  For example in  reaction k F*A  2(H*G+B  D  the substrates are  A,G,B  with stoichiometries  F,2*H,2  Let's now convert  revsys  to ODEs and look at the resulting equations Notice as described in the  Reaction rate laws used in simulations  section the default rate laws involve factorials in the stoichiometric coefficients For this reason we must specify  m  and  n  as integers and hence  use a tuple for the parameter mapping We can now solve and plot the system If we had used a vector to store parameters  m  and  n  would be converted to floating point giving an error when solving the system An alternative approach to avoid the issues of using mixed floating point and integer variables is to disable the rescaling of rate laws as described in  Reaction rate laws used in simulations  section This requires passing the  combinatoric_ratelaws=false  keyword to  convert  or to  ODEProblem  if directly building the problem from a  ReactionSystem  instead of first converting to an  ODESystem  For the previous example this gives the following different system of ODEs Since we no longer have factorial functions appearing our example will now run even with floating point values for  m  and  n  Gene expression with randomly produced amounts of protein As a second example let's build the negative feedback model from  MomentClosure.jl  that involves a bursty reaction that produces a random amount of protein In our model  G₋  will denote the repressed state and  G₊  the active state where the gene can transcribe  P  will denote the protein product of the gene We will assume that proteins are produced in bursts that produce  m  proteins where  m  is a shifted geometric random variable with mean  b  To define  m  we must register the  Distributions.Geometric  distribution from Distributions.jl with Symbolics.jl after which we can use it in symbolic expressions Note as we require the shifted geometric distribution we add one to Distributions.jl's  Geometric  random variable which includes zero We can now define our model The parameter  b  does not need to be explicitly declared in the  reaction_network  macro as it is detected when the expression  rand(Geometric(1/b  1  is substituted for  m  We next convert our network to a jump process representation Notice the  equations  of  jsys  have three  MassActionJump s for the first three reactions and one  ConstantRateJump  for the last reaction If we examine the  ConstantRateJump  more closely we can see the generated  rate  and  affect  functions for the bursty reaction that makes protein Finally we can now simulate our jumpsystem To double check our results are consistent with MomentClosure.jl let's calculate and plot the average amount of protein which is also plotted in the MomentClosure.jl  tutorial  Comparing we see similar averages for  P(t "},{"doctype":"documentation","id":"references/ModelingToolkit.get_inequality_constraints","title":"get_inequality_constraints","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.mic_check","title":"mic_check","text":"mic_check(solver::AbstractImmutableNonlinearSolver mic_check!(solver::AbstractNonlinearSolver Checks before running main solving iterations"},{"doctype":"documentation","id":"references/SciMLBase.AbstractSDDEProblem","title":"AbstractSDDEProblem","text":"DocStringExtensions.TypeDefinition Base for types which define SDDE problems"},{"doctype":"documentation","id":"references/GlobalSensitivity.generate_design_matrix","title":"generate_design_matrix","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.is_linear_poly","title":"is_linear_poly","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.locally_structure_simplify!","title":"locally_structure_simplify!","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractEnsembleEstimator","title":"AbstractEnsembleEstimator","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/Surrogates.phi_nj1D","title":"phi_nj1D","text":""},{"doctype":"documentation","id":"references/ParameterizedFunctions.build_indvar_dict","title":"build_indvar_dict","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.OrnsteinUhlenbeck!","title":"OrnsteinUhlenbeck!","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.ComposedBoundaryPaddedMatrix","title":"ComposedBoundaryPaddedMatrix","text":""},{"doctype":"documentation","id":"references/GlobalSensitivity._calculate_correlation_matrix","title":"_calculate_correlation_matrix","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.BoxGeneration3","title":"BoxGeneration3","text":""},{"doctype":"documentation","id":"references/MethodOfLines.spherical_diffusion","title":"spherical_diffusion","text":"spherical_diffusion Based on https://web.mit.edu/braatzgroup/analysis_of_finite_difference_discretization_schemes_for_diffusion_in_spheres_with_variable_diffusivity.pdf See scheme 1 in appendix A The r  0 case is treated in a later appendix"},{"doctype":"documentation","id":"references/SciMLBase.SplitSDEFunction","title":"SplitSDEFunction","text":"iip recompile f1 f2 g mass_matrix I analytic nothing tgrad nothing jac nothing jvp nothing vjp nothing ggprime nothing jac_prototype nothing sparsity jac_prototype paramjac nothing syms nothing indepsym nothing colorvec nothing SplitSDEFunction  AbstractSDEFunction A representation of a split SDE function  f  defined by M frac{du}{dt  f_1(u,p,t  f_2(u,p,t  g(u,p,t dW and all of its related functions such as the Jacobian of  f  its gradient with respect to time and more For all cases  u0  is the initial condition  p  are the parameters and  t  is the independent variable Generally for SDE integrators the  f_1  portion should be considered the stiff portion of the model with larger time scale separation while the  f_2  portion should be considered the non-stiff portion This interpretation is directly used in integrators like IMEX implicit-explicit integrators and exponential integrators Constructor Note that only the function  f  itself is required All of the remaining functions are optional for improving or accelerating the usage of  f  These include mass_matrix  the mass matrix  M  represented in the SDE function Can be used to determine that the equation is actually a stochastic differential-algebraic equation SDAE if  M  is singular Note that in this case special solvers are required see the DAE solver page for more details https://diffeq.sciml.ai/stable/solvers/sdae_solve Must be an AbstractArray or an AbstractSciMLOperator analytic(u0,p,t  used to pass an analytical solution function for the analytical  solution of the ODE Generally only used for testing and development of the solvers tgrad(dT,u,p,t  or dT=tgrad(u,p,t returns  frac{\\partial f_1(u,p,t)}{\\partial t jac(J,u,p,t  or  J=jac(u,p,t  returns  frac{df_1}{du jvp(Jv,v,u,p,t  or  Jv=jvp(v,u,p,t  returns the directional derivative frac{df_1}{du v vjp(Jv,v,u,p,t  or  Jv=vjp(v,u,p,t  returns the adjoint derivative frac{df_1}{du}^\\ast v ggprime(J,u,p,t  or  J  ggprime(u,p,t  returns the Milstein derivative   frac{dg(u,p,t)}{du g(u,p,t jac_prototype  a prototype matrix matching the type that matches the Jacobian For example if the Jacobian is tridiagonal then an appropriately sized  Tridiagonal  matrix can be used as the prototype and integrators will specialize on this structure where possible Non-structured sparsity patterns should use a  SparseMatrixCSC  with a correct sparsity pattern for the Jacobian The default is  nothing  which means a dense Jacobian paramjac(pJ,u,p,t  returns the parameter Jacobian  frac{df_1}{dp  syms  the symbol names for the elements of the equation This should match  u0  in size For example if  u0  0.0,1.0  and  syms  x y  this will apply a canonical naming to the values allowing  sol[:x  in the solution and automatically naming values in plots indepsym  the canonical naming for the independent variable Defaults to nothing which internally uses  t  as the representation in any plots colorvec  a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the  jac_prototype  This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern Defaults to  nothing  which means a color vector will be internally computed on demand when required The cost of this operation is highly dependent on the sparsity pattern Note on the Derivative Definition The derivatives such as the Jacobian are only defined on the  f1  portion of the split ODE This is used to treat the  f1  implicit while keeping the  f2  portion explicit iip In-Place vs Out-Of-Place For more details on this argument see the ODEFunction documentation recompile Controlling Compilation and Specialization For more details on this argument see the ODEFunction documentation Fields The fields of the SplitSDEFunction type directly match the names of the inputs"},{"doctype":"documentation","id":"references/Catalyst","title":"Catalyst","text":"DocStringExtensions.Readme"},{"doctype":"documentation","id":"references/SciMLBase.DISCRETE_INPLACE_DEFAULT","title":"DISCRETE_INPLACE_DEFAULT","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.AbstractConnectorType","title":"AbstractConnectorType","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.expand_connections","title":"expand_connections","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.separate_nonunique","title":"separate_nonunique","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.CurlOperator","title":"CurlOperator","text":""},{"doctype":"documentation","id":"references/SciMLBase.has_vjp","title":"has_vjp","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timepoint_mean","title":"timepoint_mean","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.𝑠neighbors","title":"𝑠neighbors","text":""},{"doctype":"documentation","id":"references/PolyChaos.Beta01OrthoPoly","title":"Beta01OrthoPoly","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.applyA!","title":"applyA!","text":""},{"doctype":"documentation","id":"references/NeuralPDE.neural_adapter","title":"neural_adapter","text":"the method that trains a neural network using the results from one already obtained prediction Arguments loss  the body of loss function initθ  the initial parameter of the neural network pde_system  PDEs are defined using the ModelingToolkit.jl strategy  determines which training strategy will be used the method that trains a neural network using the results from many already obtained predictions Arguments loss  the body of loss functions initθ  the initial parameter of the neural network pde_system  PDEs are defined using the ModelingToolkit.jl strategy  determines which training strategy will be used"},{"doctype":"document","id":"DiffEqSensitivity/neural_ode/minibatch.md","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"DifferentialEquations Plots IterTools ncycle newtons_cooling du u p t temp u k temp_m p du dT k temp temp_m true_sol du u p t true_p log newtons_cooling du u true_p t ann tanh tanh θ ann dudt_ u p t ann u p u predict_adjoint time_batch _prob prob u0 u0 p θ Array _prob Tsit5 saveat time_batch loss_adjoint batch time_batch pred predict_adjoint time_batch sum abs2 batch pred u0 Float32 datasize tspan t range tspan tspan length datasize true_prob true_sol u0 tspan ode_data Array true_prob Tsit5 saveat t prob dudt_ u0 tspan θ k train_loader Flux Data DataLoader ode_data t batchsize k x y train_loader x y numEpochs losses cb l loss_adjoint ode_data t push! losses l l pred predict_adjoint t pl scatter t ode_data label color black ylim scatter! pl t pred label color darkgreen display plot pl opt ADAM Flux train! loss_adjoint Flux θ ncycle train_loader numEpochs opt cb Flux throttle cb starting_temp collect true_prob_func u0 true_sol u0 tspan color_cycle palette tab10 pl plot j temp enumerate starting_temp ode_test_sol true_sol temp Tsit5 saveat ode_nn_sol dudt_ temp θ scatter! pl ode_test_sol label color color_cycle j plot! pl ode_nn_sol label color color_cycle j lw display pl title! xlabel! ylabel! MLDataUtils train_loader _ _ kfolds ode_data t Flux train! loss_adjoint Flux θ ncycle eachbatch train_loader k numEpochs opt cb Flux throttle cb DifferentialEquations Plots IterTools ncycle newtons_cooling du u p t temp u k temp_m p du dT k temp temp_m true_sol du u p t true_p log newtons_cooling du u true_p t ann tanh tanh θ ann dudt_ u p t ann u p u predict_adjoint time_batch _prob prob u0 u0 p θ Array _prob Tsit5 saveat time_batch loss_adjoint batch time_batch pred predict_adjoint time_batch sum abs2 batch pred u0 Float32 datasize tspan t range tspan tspan length datasize true_prob true_sol u0 tspan ode_data Array true_prob Tsit5 saveat t prob dudt_ u0 tspan θ k train_loader Flux Data DataLoader ode_data t batchsize k x y train_loader x y numEpochs losses cb l loss_adjoint ode_data t push! losses l l pred predict_adjoint t pl scatter t ode_data label color black ylim scatter! pl t pred label color darkgreen display plot pl opt ADAM Flux train! loss_adjoint Flux θ ncycle train_loader numEpochs opt cb Flux throttle cb starting_temp collect true_prob_func u0 true_sol u0 tspan color_cycle palette tab10 pl plot j temp enumerate starting_temp ode_test_sol true_sol temp Tsit5 saveat ode_nn_sol dudt_ temp θ scatter! pl ode_test_sol label color color_cycle j plot! pl ode_nn_sol label color color_cycle j lw display pl title! xlabel! ylabel! MLDataUtils train_loader _ _ kfolds ode_data t Flux train! loss_adjoint Flux θ ncycle eachbatch train_loader k numEpochs opt cb Flux throttle cb Training a Neural Ordinary Differential Equation with Mini-Batching When training a neural network we need to find the gradient with respect to our data set There are three main ways to partition our data when using a training algorithm like gradient descent stochastic batching and mini-batching Stochastic gradient descent trains on a single random data point each epoch This allows for the neural network to better converge to the global minimum even on noisy data but is computationally inefficient Batch gradient descent trains on the whole data set each epoch and while computationally efficient is prone to converging to local minima Mini-batching combines both of these advantages and by training on a small random mini-batch of the data each epoch can converge to the global minimum while remaining more computationally efficient than stochastic descent Typically we do this by randomly selecting subsets of the data each epoch and use this subset to train on We can also pre-batch the data by creating an iterator holding these randomly selected batches before beginning to train The proper size for the batch can be determined experimentally Let us see how to do this with Julia For this example we will use a very simple ordinary differential equation newtons law of cooling We can represent this in Julia like so Now we define a neural-network using a linear approximation with 1 hidden layer of 8 neurons From here we build a loss function around it To add support for batches of size  k  we use  Flux.Data.DataLoader  To use this we pass in the  ode_data  and  t  as the x and y data to batch respectively The parameter  batchsize  controls the size of our batches We check our implementation by iterating over the batched data Now we train the neural network with a user defined call back function to display loss and the graphs with a maximum of 300 epochs Finally we can see how well our trained network will generalize to new initial conditions We can also minibatch using tools from  MLDataUtils  To do this we need to slightly change our implementation and is shown below again with a batch size of k and the same number of epochs"},{"doctype":"documentation","id":"references/SciMLOperators.isconstant","title":"isconstant","text":""},{"doctype":"documentation","id":"references/RecursiveArrayTools._npartitions","title":"_npartitions","text":""},{"doctype":"document","id":"ModelingToolkit/systems/OptimizationSystem.md","title":"OptimizationSystem","text":"hessian_sparsity OptimizationSystem System Constructors Composition and Accessor Functions get_eqs(sys  or  equations(sys  The equation to be minimized get_states(sys  or  states(sys  The set of states for the optimization get_ps(sys  or  parameters(sys  The parameters for the optimization Transformations Analyses Applicable Calculation and Generation Functions Problem Constructors"},{"doctype":"documentation","id":"references/DiffEqSensitivity.shadow_adjoint","title":"shadow_adjoint","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.count_nonzeros","title":"count_nonzeros","text":""},{"doctype":"documentation","id":"references/SciMLOperators.IdentityOperator","title":"IdentityOperator","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/Integrals.__solvebp","title":"__solvebp","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.expand_basis","title":"expand_basis","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_preface","title":"get_preface","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.TailApproxs","title":"TailApproxs","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractEnsembleSolution","title":"AbstractEnsembleSolution","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/Catalyst.drop_dynamics","title":"drop_dynamics","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractDynamicalODEProblem","title":"AbstractDynamicalODEProblem","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/Catalyst.empty_set","title":"empty_set","text":""},{"doctype":"documentation","id":"references/SciMLBase.BVProblem","title":"BVProblem","text":"f bc! u0 tspan p kwargs f bc! u0 tspan p kwargs bc! residual u p t Defines an BVP problem Documentation Page https://diffeq.sciml.ai/stable/types/bvp_types Mathematical Specification of a BVP Problem To define a BVP Problem you simply need to give the function  f  and the initial condition  u_0  which define an ODE frac{du}{dt  f(u,p,t along with an implicit function  bc  which defines the residual equation where bc(u,p,t  0 is the manifold on which the solution must live A common form for this is the two-point  BVProblem  where the manifold defines the solution at two points u(t_0  a\nu(t_f  b Problem Type Constructors For any BVP problem type  bc  is the inplace function where  residual  computed from the current  u   u  is an array of solution values where  u[i  is at time  t[i  while  p  are the parameters For a  TwoPointBVProblem   t  tspan  For the more general  BVProblem   u  can be all of the internal time points and for shooting type methods  u=sol  the ODE solution Note that all features of the  ODESolution  are present in this form In both cases the size of the residual matches the size of the initial condition Parameters are optional and if not given then a  NullParameters  singleton will be used which will throw nice errors if you try to index non-existent parameters Any extra keyword arguments are passed on to the solvers For example if you set a  callback  in the problem then that  callback  will be added in every solve call Fields f  The function for the ODE bc  The boundary condition function u0  The initial condition Either the initial condition for the ODE as an initial value problem or a  Vector  of values for  u(t_i  for collocation methods tspan  The timespan for the problem p  The parameters for the problem Defaults to  NullParameters kwargs  The keyword arguments passed onto the solves"},{"doctype":"documentation","id":"references/PolyChaos._checkBounds","title":"_checkBounds","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.namespace_assignment","title":"namespace_assignment","text":""},{"doctype":"documentation","id":"references/Catalyst.conservationlaw_constants","title":"conservationlaw_constants","text":"Calculate symbolic equations from conservation laws writing the conservation law constants in terms of the dependent and independent variables Notes Caches the resulting equations in  rn  so will be fast on subsequent calls Examples gives"},{"doctype":"documentation","id":"references/GlobalSensitivity.calculate_spread","title":"calculate_spread","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.BoxGeneration1","title":"BoxGeneration1","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.DEFAULT_NORM","title":"DEFAULT_NORM","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.𝑠vertices","title":"𝑠vertices","text":""},{"doctype":"documentation","id":"references/Surrogates.multivar_poly_basis","title":"multivar_poly_basis","text":"Evaluates in  x  the  ix th element of the multivariate polynomial basis of maximum degree  n  and  d  dimensions Time complexity  n+1)^d Example For n=2 d=2 the multivariate polynomial basis is Therefore the 3rd ix=3 element is  y   Therefore when x=(13,43 and ix=3 this function will return 43"},{"doctype":"documentation","id":"references/MethodOfLines.get_ranking!","title":"get_ranking!","text":"Creates a ranking of the variables in the term based on their derivative order The heuristic that should work is if there's a time derivative then use that variable otherwise use the highest derivative for that variable If there are two with the highest derivative pick first from the list that hasn't been chosen for another equation"},{"doctype":"documentation","id":"references/MethodOfLines.boundary_value_maps","title":"boundary_value_maps","text":""},{"doctype":"documentation","id":"references/PolyChaos.rm_compute","title":"rm_compute","text":"Given a positive  weight  function with domain  lb,ub  i.e a function  w lb ub  rightarrow mathbb{R}_{\\geq 0  this function creates  Npoly  recursion coefficients  α,β  The keyword  quadrature  specifies what quadrature rule is being used"},{"doctype":"document","id":"Surrogates/variablefidelity.md","title":"Variable fidelity Surrogates","text":"Variable fidelity Surrogates With the variable fidelity surrogate we can specify two different surrogates one for high fidelity data and one for low fidelity data By default the first half samples are considered high fidelity and the second half low fidelity"},{"doctype":"documentation","id":"references/PolyChaos.w_uniform_11","title":"w_uniform_11","text":""},{"doctype":"documentation","id":"references/PolyChaos.w_logistic","title":"w_logistic","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.exp_generic_core!","title":"exp_generic_core!","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration","title":"SymbolicNumericIntegration","text":""},{"doctype":"documentation","id":"references/SciMLBase.isautodifferentiable","title":"isautodifferentiable","text":"isautodifferentiable(alg::DEAlgorithm Trait declaration for whether an algorithm is compatible with direct automatic differentiation i.e can have algorithms like ForwardDiff or ReverseDiff attempt to differentiate directly through the solver Defaults to false as only pure-Julia algorithms can have this be true"},{"doctype":"documentation","id":"references/Optimization._check_and_convert_maxtime","title":"_check_and_convert_maxtime","text":""},{"doctype":"document","id":"NeuralOperators/references.md","title":"References","text":"References"},{"doctype":"documentation","id":"references/DiffEqFlux.LegendreBasis","title":"LegendreBasis","text":"n Constructs a Legendre basis of the form P_{0}(x P_{1}(x  P_(x where P_j is the j-th Legendre polynomial Arguments n  number of terms in the polynomial expansion"},{"doctype":"documentation","id":"references/SciMLBase.augment","title":"augment","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.NonZerosPairs","title":"NonZerosPairs","text":""},{"doctype":"document","id":"Catalyst/faqs.md","title":"FAQs","text":"osys convert rn rn k A B k k b t A t B t C t D t rx1 k B C B D rx2 k B D rx3 k B D mixedsys rx1 rx2 rx3 t A B C D k b osys convert mixedsys Plots OrdinaryDiffEq β ν t S t I t R t rx1 β S I I rx2 ν I R defs β ν S I R sir rx1 rx2 t defs oprob sir sol oprob Tsit5 plot sol β ν t S t I t R t rx1 β S I I rx2 ν I R sir rx1 rx2 t sir sir β S I I ν I R β ν sir β ν S I R rn α S I I β I R α β u0 S I R p α β op rn u0 p sol op Tsit5 α β t S t I t R t u0 S I R p α β op rn u0 p sol op Tsit5 dXdteq osys t osys dXdteq Equation dXdteq lhs dXdteq rhs sin t osys2 dXdteq t osys osys oprob osys2 u0map tspan pmap osol oprob Tsit5 dXdteq osys X t dXdteq Equation dXdteq lhs dXdteq rhs exp X osys2 dXdteq t osys osys oprob osys2 u0map tspan pmap osol oprob Tsit5 rn k X ∅ k rn k X ∅ k rn k X ∅ k myHill x x x rn myHill X ∅ X FAQs How to disable rescaling of reaction rates in rate laws As explained in the  Reaction rate laws used in simulations  section for a reaction such as  k 2X  0  the generated rate law will rescale the rate constant giving  k*X^2/2  instead of  k*X^2  for ODEs and  k*X*(X-1)/2  instead of  k*X*(X-1  for jumps This can be disabled when directly  convert ing a  ReactionSystem  If  rn  is a generated  ReactionSystem  we can do Disabling these rescalings should work for all conversions of  ReactionSystem s to other  ModelingToolkit.AbstractSystem s How to use non-integer stoichiometric coefficients or directly via Note when using  convert(ODESystem mixedsys combinatoric_ratelaws=false  the  combinatoric_ratelaws=false  parameter must be passed This is also true when calling  ODEProblem(mixedsys combinatoric_ratelaws=false  As described above this disables Catalyst's standard rescaling of reaction rates when generating reaction rate laws see also the  Reaction rate laws used in   simulations  section Leaving this keyword out for systems with floating point stoichiometry will give an error message How to set default values for initial conditions and parameters When directly constructing a  ReactionSystem  these can be passed to the constructor and allow solving the system without needing initial condition or parameter vectors in the generated problem For example alternatively we could also have said The  reaction_network  macro does not currently provide a way to specify default values however they can be added after creating the system via the  setdefaults  command like How to specify initial conditions and parameters values for  ODEProblem  and other problem types To explicitly pass initial conditions and parameters we can use mappings from Julia  Symbol s corresponding to each variable/parameter to values or from ModelingToolkit symbolic variables to each variable/parameter Using  Symbol s we have while using ModelingToolkit symbolic variables we have How to modify generated ODEs Conversion to other  ModelingToolkit.AbstractSystem s allows the possibility to modify the system with further terms that are difficult to encode as a chemical reaction For example suppose we wish to add a forcing term  10\\sin(10t  to the ODE for  dX/dt  above We can do so as We can add  e^{-X  to  dX/dt  as a forcing term by How to override mass action kinetics rate laws While generally one wants the reaction rate law to use the law of mass action so the reaction occurs at the ODE rate  d[X]/dt  k[X  it is possible to override this by using any of the following non-filled arrows when declaring the reaction  ⇐   ⟽   ⇒   ⟾   ⇔   ⟺  This means that the reaction will occur at rate  d[X]/dt  k  which might become a problem since  X  will be degraded at a constant rate even when very small or equal to 0 Note stoichiometric coefficients are still included i.e the reaction has rate  d[X]/dt  2 k  How to specify user defined functions as reaction rates  user_functions The reaction network DSL can see user defined functions that work with ModelingToolkit e.g this is should work In some cases it may be necessary or desirable to register functions with Symbolics.jl before their use in Catalyst see the discussion  here "},{"doctype":"documentation","id":"references/PolyChaos.MeixnerPollaczekMeasure","title":"MeixnerPollaczekMeasure","text":""},{"doctype":"documentation","id":"references/ModelingToolkit._readable_code","title":"_readable_code","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.componentwise_meancov","title":"componentwise_meancov","text":""},{"doctype":"documentation","id":"references/SciMLBase.get_tmp_cache","title":"get_tmp_cache","text":"Returns a tuple of internal cache vectors which are safe to use as temporary arrays This should be used for integrator interface and callbacks which need arrays to write into in order to be non-allocating The length of the tuple is dependent on the method"},{"doctype":"documentation","id":"references/DiffEqSensitivity.TrackerAdjoint","title":"TrackerAdjoint","text":"TrackerAdjoint  AbstractAdjointSensitivityAlgorithm An implementation of discrete adjoint sensitivity analysis using the Tracker.jl tracing-based AD Supports in-place functions through an Array of Structs formulation and supports out of place through struct of arrays Constructor SciMLProblem Support This  sensealg  supports any  DEProblem  if the algorithm is  SciMLBase.isautodifferentiable  Compatible with a limited subset of  AbstractArray  types for  u0  including  CuArrays "},{"doctype":"documentation","id":"references/ModelingToolkit.VariableDistribution","title":"VariableDistribution","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.@parameters","title":"@parameters","text":"DocStringExtensions.MethodSignatures Define one or more known variables"},{"doctype":"documentation","id":"references/PolyChaos.fejer2","title":"fejer2","text":"Fejer's second quadrature rule according to  Waldvogel J Bit Numer Math 2006 46 195 "},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.split_VBT_seed","title":"split_VBT_seed","text":""},{"doctype":"documentation","id":"references/MethodOfLines.clip_interior!!","title":"clip_interior!!","text":""},{"doctype":"documentation","id":"references/MethodOfLines.interpolate_discrete_param","title":"interpolate_discrete_param","text":"interpolate_discrete_param Interpolate gridpoints by taking the average of the values of the discrete points or if the offset is outside the grid extrapolate the value with dx"},{"doctype":"document","id":"MethodOfLines/nonuniform.md","title":"Non-Uniform Rectilinear Grids","text":"Non-Uniform Rectilinear Grids For more information on how to use a non-uniform rectilinear grid see the docs for MOLFiniteDifference  molfd"},{"doctype":"documentation","id":"references/SciMLBase.wrapfun_oop","title":"wrapfun_oop","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.TrackerVJP","title":"TrackerVJP","text":"compile TrackerVJP  VJPChoice Uses Tracker.jl to compute the vector-Jacobian products If  f  is in-place then it uses a array of structs formulation to do scalarized reverse mode while if  f  is out-of-place then it uses an array-based reverse mode Not as efficient as  ReverseDiffVJP  but supports GPUs when doing array-based reverse mode Constructor"},{"doctype":"documentation","id":"references/MethodOfLines.generate_winding_rules","title":"generate_winding_rules","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.hasbounds","title":"hasbounds","text":"Determine whether or not symbolic variable  x  has bounds associated with it See also  getbounds "},{"doctype":"documentation","id":"references/NeuralPDE.generate_quasi_random_points_batch","title":"generate_quasi_random_points_batch","text":""},{"doctype":"document","id":"ParameterizedFunctions/ode_def.md","title":"The ode_def macro","text":"The ode_def macro Internal API"},{"doctype":"documentation","id":"references/ModelingToolkit.get_ctrls","title":"get_ctrls","text":""},{"doctype":"documentation","id":"references/SciMLOperators._reshape","title":"_reshape","text":"use Base.ReshapedArray"},{"doctype":"documentation","id":"references/ModelingToolkit.get_Wfact_t","title":"get_Wfact_t","text":""},{"doctype":"documentation","id":"references/RecursiveArrayTools.getblock","title":"getblock","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.Transform","title":"Transform","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.AbstractBracketingAlgorithm","title":"AbstractBracketingAlgorithm","text":""},{"doctype":"documentation","id":"references/Surrogates.LCBS","title":"LCBS","text":""},{"doctype":"documentation","id":"references/SciMLBase.DEFAULT_OBSERVED","title":"DEFAULT_OBSERVED","text":""},{"doctype":"documentation","id":"references/MethodOfLines.clip","title":"clip","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.complete","title":"complete","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.try_integrate","title":"try_integrate","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.solve_newton","title":"solve_newton","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.gettype","title":"gettype","text":""},{"doctype":"document","id":"NeuralPDE/pinn/debugging.md","title":"Debugging PINN Solutions","text":"Debugging PINN Solutions Let's walk through debugging functions for the physics-informed neural network PDE solvers"},{"doctype":"documentation","id":"references/SciMLBase.set_proposed_dt!","title":"set_proposed_dt!","text":"Sets the proposed  dt  for the next timestep If second argument isa  DEIntegrator  then it sets the timestepping of first argument to match that of second one Note that due to PI control and step acceleration this is more than matching the factors in most cases"},{"doctype":"documentation","id":"references/ExponentialUtilities._exp!","title":"_exp!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.safe_get_unit","title":"safe_get_unit","text":"Get unit of term returning nothing  showing warning instead of throwing errors"},{"doctype":"documentation","id":"references/MethodOfLines.generate_bc_eqs","title":"generate_bc_eqs","text":""},{"doctype":"documentation","id":"references/LinearSolve.KrylovJL_GMRES","title":"KrylovJL_GMRES","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.Tail4","title":"Tail4","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.BoxGeneration2","title":"BoxGeneration2","text":""},{"doctype":"documentation","id":"references/PolyChaos.ProductMeasure","title":"ProductMeasure","text":""},{"doctype":"document","id":"ModelingToolkit/tutorials/tearing_parallelism.md","title":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","text":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems Sometimes it can be very non-trivial to parallelize a system In this tutorial we will demonstrate how to make use of  structural_simplify  to expose more parallelism in the solution process and parallelize the resulting simulation The Component Library The following tutorial will use the following set of components describing electrical circuits The Model Assuming that the components are defined our model is 50 resistors and capacitors connected in parallel Thus following the acausal components tutorial  acausal we can connect a bunch of RC components as follows Now let's say we want to expose a bit more parallelism via running tearing How do we do that Done that's it There's no more to it What Happened Yes that's a good question Let's investigate a little bit more what had happened If you look at the system we defined You see it started as a massive 1051 set of equations However after eliminating redundancies we arrive at 151 equations That's not all though In addition the tearing process has turned the sets of nonlinear equations into separate blocks and constructed a DAG for the dependencies between the blocks We can use the bipartite graph functionality to dig in and investigate what this means The figure on the left is the original incidence matrix of the algebraic equations Notice that the original formulation of the model has dependencies between different equations and so the full set of equations must be solved together That exposes no parallelism However the Block Lower Triangular BLT transformation exposes independent blocks This is then further improved by the tearing process which removes 90 of the equations and transforms the nonlinear equations into 50 independent blocks  which can now all be solved in parallel  The conclusion is that your attempts to parallelize are neigh performing parallelism after structural simplification greatly improves the problem that can be parallelized so this is better than trying to do it by hand After performing this you can construct the  ODEProblem  ODAEProblem  and set  parallel_form  to use the exposed parallelism in multithreaded function constructions but this showcases why  structural_simplify  is so important to that process"},{"doctype":"documentation","id":"references/ExponentialUtilities.phiv_dense","title":"phiv_dense","text":"Compute the matrix-phi-vector products for small dense  A  k  1 The phi functions are defined as varphi_0(z  exp(z),\\quad varphi_{k+1}(z  frac{\\varphi_k(z  1}{z Instead of using the recurrence relation which is numerically unstable a formula given by Sidje is used Sidje R B 1998 Expokit a software package for computing matrix exponentials ACM Transactions on Mathematical Software TOMS 24(1 130-156 Theorem 1"},{"doctype":"documentation","id":"references/SciMLBase.AbstractODEIntegrator","title":"AbstractODEIntegrator","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/ModelingToolkit.check_name","title":"check_name","text":""},{"doctype":"document","id":"ExponentialUtilities/index.md","title":"ExponentialUtilities.jl: High-Performance Matrix Exponentiation and Products","text":"Pkg Pkg add A rand A v rand t rand t A v ExponentialUtilities.jl High-Performance Matrix Exponentiation and Products ExponentialUtilities is a package of utility functions for matrix functions of exponential type including functionality for the matrix exponential and phi-functions The tools are used by the exponential integrators in OrdinaryDiffEq Installation To install ExponentialUtilities.jl use the Julia package manager Example Matrix-phi-vector product The main functionality of ExponentialUtilities is the computation of matrix-phi-vector products The phi functions are defined as In exponential algorithms products in the form of  ϕ_m(tA)b  is frequently encountered Instead of computing the matrix function first and then computing the matrix-vector product the common alternative is to construct a  Krylov subspace   K_m(A,b  and then approximate the matrix-phi-vector product expv  and  phiv For  phiv   all   ϕ_m(tA)b  products up to order  k  is returned as a matrix This is because it's more economical to produce all the results at once than individually A second output is returned if  errest=true  in  kwargs  The error estimate is given for the second-to-last product using the last product as an estimator If  correct=true  then  ϕ_0  through  ϕ_(k-1  are updated using the last Arnoldi vector The correction algorithm is described in 1 You can adjust how the Krylov subspace is constructed by setting various keyword arguments See the Arnoldi iteration section for more details Contributing Please refer to the  SciML ColPrac Contributor's Guide on Collaborative Practices for Community Packages  for guidance on PRs issues and other matters relating to contributing to ModelingToolkit There are a few community forums the diffeq-bridged channel in the  Julia Slack JuliaDiffEq  on Gitter on the  Julia Discourse forums see also  SciML Community page"},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.invview","title":"invview","text":""},{"doctype":"documentation","id":"references/GlobalSensitivity._calc_delta","title":"_calc_delta","text":""},{"doctype":"documentation","id":"references/SciMLBase.OptimizationFunction","title":"OptimizationFunction","text":"f OptimizationFunction  AbstractOptimizationFunction A representation of an optimization of an objective function  f  defined by min_{u f(u,p and all of its related functions such as the gradient of  f  its Hessian and more For all cases  u  is the state and  p  are the parameters Constructor OptimizationFunction(f,adtype::AbstractADType=NoAD grad=nothing,hess=nothing,hv=nothing cons=nothing cons_j=nothing,cons_h=nothing hess_prototype=nothing,cons_jac_prototype=nothing cons_hess_prototype  nothing syms  nothing hess_colorvec  nothing cons_jac_colorvec  nothing cons_hess_colorvec  nothing adtype  see the section Defining Optimization Functions via AD grad(G,u,p  or  G=grad(u,p  the gradient of  f  with respect to  u hess(H,u,p  or  H=hess(u,p  the Hessian of  f  with respect to  u hv(Hv,u,v,p  or  Hv=hv(u,v,p  the Hessian-vector product  rac{d^2 f}{du^2 v  cons(res,x,p  or  res=cons(x,p  the equality constraints vector where the constraints are satisfied when  res  0  cons_j(res,x,p  or  res=cons_j(x,p  the Jacobian of the equality constraints cons_h(res,x,p  or  res=cons_h(x,p  the Hessian of the equality constratins provided as and array of Hessians with  res[i  being the Hessian with respect to the  i th output on  cons  paramjac(pJ,u,p  returns the parameter Jacobian  rac{df}{dp  hess_prototype  a prototype matrix matching the type that matches the Hessian For example if the Hessian is tridiagonal then an appropriately sized  Hessian  matrix can be used as the prototype and integrators will specialize on this structure where possible Non-structured sparsity patterns should use a  SparseMatrixCSC  with a correct sparsity pattern for the Hessian The default is  nothing  which means a dense Hessian cons_jac_prototype  a prototype matrix matching the type that matches the constraint Jacobian The default is  nothing  which means a dense constraint Jacobian cons_hess_prototype  a prototype matrix matching the type that matches the constraint Hessian This is defined as an array of matrices where  hess[i  is the Hessian w.r.t the  i th output For example if the Hessian is sparse then  hess  is a  Vector{SparseMatrixCSC  The default is  nothing  which means a dense constraint Hessian syms  the symbol names for the elements of the equation This should match  u0  in size For example if  u  0.0,1.0  and  syms  x y  this will apply a canonical naming to the values allowing  sol[:x  in the solution and automatically naming values in plots hess_colorvec  a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the  hess_prototype  This specializes the Hessian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern Defaults to  nothing  which means a color vector will be internally computed on demand when required The cost of this operation is highly dependent on the sparsity pattern cons_jac_colorvec  a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the  cons_jac_prototype  cons_hess_colorvec  an array of color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the  cons_hess_prototype  Defining Optimization Functions Via AD While using the keyword arguments gives the user control over defining all of the possible functions the simplest way to handle the generation of an  OptimizationFunction  is by specifying an AD type By doing so this will automatically fill in all of the extra functions For example will use  Zygote.jl  to define all of the necessary functions Note that if any functions are defined directly the auto-AD definition does not overwrite the user's choice Each of the AD-based constructors are documented separately via their own dispatches iip In-Place vs Out-Of-Place For more details on this argument see the ODEFunction documentation recompile Controlling Compilation and Specialization For more details on this argument see the ODEFunction documentation Fields The fields of the OptimizationFunction type directly match the names of the inputs"},{"doctype":"documentation","id":"references/PolyChaos.genHermiteMeasure","title":"genHermiteMeasure","text":""},{"doctype":"documentation","id":"references/Optimization","title":"Optimization","text":"DocStringExtensions.Readme"},{"doctype":"documentation","id":"references/NeuralOperators.GraphKernel","title":"GraphKernel","text":"Graph kernel layer Arguments κ  A neural network layer for approximation e.g a  Dense  layer or a MLP ch  Channel size for linear transform e.g  32  σ  Activation function"},{"doctype":"documentation","id":"references/MethodOfLines._upwind_difference","title":"_upwind_difference","text":""},{"doctype":"document","id":"LinearSolve/basics/CachingAPI.md","title":"Caching Interface API Functions","text":"Caching Interface API Functions"},{"doctype":"documentation","id":"references/DiffEqOperators.AbstractMatrixFreeOperator","title":"AbstractMatrixFreeOperator","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.square_norm","title":"square_norm","text":""},{"doctype":"document","id":"DiffEqFlux/examples/augmented_neural_ode.md","title":"Augmented Neural Ordinary Differential Equations","text":"DifferentialEquations Statistics LinearAlgebra Plots Flux Data DataLoader random_point_in_sphere min_radius max_radius distance max_radius min_radius rand min_radius direction randn unit_direction direction norm direction distance unit_direction concentric_sphere inner_radius_range outer_radius_range num_samples_inner num_samples_outer batch_size data labels _ num_samples_inner push! data reshape random_point_in_sphere inner_radius_range push! labels ones _ num_samples_outer push! data reshape random_point_in_sphere outer_radius_range push! labels ones data cat data dims labels cat labels dims DataLoader data gpu labels gpu batchsize batch_size shuffle partial diffeqarray_to_array x reshape gpu x size x construct_model out_dim input_dim hidden_dim augment_dim input_dim input_dim augment_dim node Dense input_dim hidden_dim relu Dense hidden_dim hidden_dim relu Dense hidden_dim input_dim gpu Tsit5 save_everystep reltol abstol save_start gpu node augment_dim node node augment_dim x p node p node x p diffeqarray_to_array Dense input_dim out_dim gpu node p gpu plot_contour model npoints grid_points zeros npoints x range length npoints y range length npoints x1 x x2 y grid_points x1 x2 sol reshape model grid_points gpu npoints npoints cpu contour x y sol fill linewidth loss_node x y mean model x y println dataloader concentric_sphere batch_size cb iter iter println iter loss_node dataloader data dataloader data model construct_model opt ADAM iter println _ Flux train! loss_node Flux model dataloader opt cb cb plt_node plot_contour model model construct_model opt ADAM iter println println _ Flux train! loss_node Flux model dataloader opt cb cb plt_anode plot_contour model DifferentialEquations Statistics LinearAlgebra Plots Flux Data DataLoader random_point_in_sphere min_radius max_radius distance max_radius min_radius rand min_radius direction randn unit_direction direction norm direction distance unit_direction concentric_sphere inner_radius_range outer_radius_range num_samples_inner num_samples_outer batch_size data labels _ num_samples_inner push! data reshape random_point_in_sphere inner_radius_range push! labels ones _ num_samples_outer push! data reshape random_point_in_sphere outer_radius_range push! labels ones data cat data dims labels cat labels dims DataLoader data gpu labels gpu batchsize batch_size shuffle partial diffeqarray_to_array x reshape gpu x size x construct_model out_dim input_dim hidden_dim augment_dim input_dim input_dim augment_dim node Dense input_dim hidden_dim relu Dense hidden_dim hidden_dim relu Dense hidden_dim input_dim gpu Tsit5 save_everystep reltol abstol save_start gpu node augment_dim node node augment_dim gpu x p node p node x p diffeqarray_to_array Dense input_dim out_dim gpu node p gpu plot_contour model npoints grid_points zeros npoints x range length npoints y range length npoints x1 x x2 y grid_points x1 x2 sol reshape model grid_points gpu npoints npoints cpu contour x y sol fill linewidth loss_node x y mean model x y dataloader concentric_sphere batch_size cb iter iter println iter loss_node dataloader data dataloader data opt ADAM model construct_model _ Flux train! loss_node Flux model dataloader opt cb cb model construct_model _ Flux train! loss_node Flux model dataloader opt cb cb Augmented Neural Ordinary Differential Equations Copy-Pasteable Code Step-by-Step Explanation Loading required packages Generating a toy dataset In this example we will be using data sampled uniformly in two concentric circles and then train our Neural ODEs to do regression on that values We assign  1  to any point which lies inside the inner circle and  1  to any point which lies between the inner and outer circle Our first function  random_point_in_sphere  samples points uniformly between 2 concentric circles/spheres of radii  min_radius  and  max_radius  respectively Next we will construct a dataset of these points and use Flux's DataLoader to automatically minibatch and shuffle the data Models We consider 2 models in this tutorial The first is a simple Neural ODE which is described in detail in  this tutorial  The other one is an Augmented Neural ODE  1 The idea behind this layer is very simple It augments the input to the Neural DE Layer by appending zeros So in order to use any arbitrary DE Layer in combination with this layer simply assume that the input to the DE Layer is of size  size(x 1  augment_dim  instead of  size(x 1  and construct that layer accordingly In order to run the models on GPU we need to manually transfer the models to GPU First one is the network predicting the derivatives inside the Neural ODE and the other one is the last layer in the Chain Plotting the Results Here we define an utility to plot our model regression results as a heatmap Training Parameters Loss Functions We use the L2 distance between the model prediction  model(x  and the actual prediction  y  as the optimization objective Dataset Next we generate the dataset We restrict ourselves to 2 dimensions as it is easy to visualize We sample a total of  4000  data points Callback Function Additionally we define a callback function which displays the total loss at specific intervals Optimizer We use ADAM as the optimizer with a learning rate of 0.005 Training the Neural ODE To train our neural ode model we need to pass the appropriate learnable parameters  parameters  which is returned by the  construct_models  function It is simply the  node.p  vector We then train our model for  20  epochs Here is what the contour plot should look for Neural ODE Notice that the regression is not perfect due to the thin artifact which connects the circles node Training the Augmented Neural ODE Our training configuration will be same as that of Neural ODE Only in this case we have augmented the input with a single zero This makes the problem 3 dimensional and as such it is possible to find a function which can be expressed by the neural ode For more details and proofs please refer to 1 For the augmented Neural ODE we notice that the artifact is gone anode Expected Output References 1 Dupont Emilien Arnaud Doucet and Yee Whye Teh Augmented neural ODEs In Proceedings of the 33rd International Conference on Neural Information Processing Systems pp 3140-3150 2019"},{"doctype":"document","id":"MethodOfLines/get_grid.md","title":"get_discrete ( get_grid)","text":"grid pdesys discretization discrete_x grid x u_sol map d sol d i grid u t x i length sol t get_discrete   get_grid MethodOfLines.jl  exports a helper function  get_discrete  which returns a  Dict  with the keys being the independent and dependent variables and the values their corresponding discrete grid and discretized variables used in the discretization It is used as following"},{"doctype":"documentation","id":"references/ExponentialUtilities.Stegr.StegrWork","title":"StegrWork","text":"Allocate work arrays for diagonalization of real-symmetric tridiagonal matrices of sizes up to  n × n "},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timestep_meanvar","title":"timestep_meanvar","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.SparseMatrixCLIL","title":"SparseMatrixCLIL","text":"The SparseMatrixCLIL represents a sparse matrix in two distinct ways As a sparse in both row and column n x m matrix As a row-dense column-sparse k x m matrix The data structure keeps a permutation between the row order of the two representations Swapping the rows in one does not affect the other On construction the second representation is equivalent to the first with fully-sparse rows removed though this may cease being true as row permutations are being applied to the matrix The default structure of the  SparseMatrixCLIL  type is the second structure while the first is available via the thin  AsSubMatrix  wrapper"},{"doctype":"documentation","id":"references/DiffEqSensitivity.get_indx","title":"get_indx","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.equivalent","title":"equivalent","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.push_defaults!","title":"push_defaults!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.getdist","title":"getdist","text":"Distributions d Normal u dist d u u Get the probability distribution associated with symbolc variable  x  If no distribution is associated with  x   nothing  is returned Create parameters with associated distributions like this"},{"doctype":"documentation","id":"references/SciMLBase.AnalyticalProblem","title":"AnalyticalProblem","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/Catalyst.recursive_clean!","title":"recursive_clean!","text":""},{"doctype":"documentation","id":"references/Surrogates.SecondOrderPolynomialStructure","title":"SecondOrderPolynomialStructure","text":""},{"doctype":"documentation","id":"references/MethodOfLines.Idx","title":"Idx","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.try_assign_eq!","title":"try_assign_eq!","text":""},{"doctype":"document","id":"ModelingToolkit/tutorials/higher_order.md","title":"Automatic Transformation of Nth Order ODEs to 1st Order ODEs","text":"Automatic Transformation of Nth Order ODEs to 1st Order ODEs ModelingToolkit has a system for transformations of mathematical systems These transformations allow for symbolically changing the representation of the model to problems that are easier to numerically solve One simple to demonstrate transformation is the  ode_order_lowering  transformation that sends an Nth order ODE to a 1st order ODE To see this let's define a second order riff on the Lorenz equations We utilize the derivative operator twice here to define the second order Note that we could've used an alternative syntax for 2nd order i.e  D  Differential(t)^2  and then  E(x  would be the second derivative and this syntax extends to  N th order Also we can use    or  ∘  to compose  Differential s like  Differential(t  Differential(x  Now let's transform this into the  ODESystem  of first order components We do this by simply calling  ode_order_lowering  Now we can directly numerically solve the lowered system Note that following the original problem the solution requires knowing the initial condition for  x  and thus we include that in our input specification"},{"doctype":"documentation","id":"references/SciMLBase.DynamicalODEFunction","title":"DynamicalODEFunction","text":"iip recompile f1 f2 mass_matrix I analytic nothing tgrad nothing jac nothing jvp nothing vjp nothing jac_prototype nothing sparsity jac_prototype paramjac nothing syms nothing indepsym nothing colorvec nothing DynamicalODEFunction  AbstractODEFunction A representation of an ODE function  f  defined by M frac{du}{dt  f(u,p,t as a partitioned ODE M_1 frac{du}{dt  f_1(u,p,t)\nM_2 frac{du}{dt  f_2(u,p,t and all of its related functions such as the Jacobian of  f  its gradient with respect to time and more For all cases  u0  is the initial condition  p  are the parameters and  t  is the independent variable Constructor Note that only the functions  f_i  themselves are required These functions should be given as  f_i!(du,u,p,t  or  du  f_i(u,p,t  See the section on  iip  for more details on in-place vs out-of-place handling All of the remaining functions are optional for improving or accelerating  the usage of  f  These include mass_matrix  the mass matrix  M_i  represented in the ODE function Can be used to determine that the equation is actually a differential-algebraic equation DAE if  M  is singular Note that in this case special solvers are required see the DAE solver page for more details https://diffeq.sciml.ai/stable/solvers/dae*solve Must be an AbstractArray or an AbstractSciMLOperator Should be given as a tuple of mass matrices i.e  M*1 M_2  for the mass matrices of equations 1 and 2 respectively analytic(u0,p,t  used to pass an analytical solution function for the analytical  solution of the ODE Generally only used for testing and development of the solvers tgrad(dT,u,p,t  or dT=tgrad(u,p,t returns  frac{\\partial f(u,p,t)}{\\partial t jac(J,u,p,t  or  J=jac(u,p,t  returns  frac{df}{du jvp(Jv,v,u,p,t  or  Jv=jvp(v,u,p,t  returns the directional derivative frac{df}{du v vjp(Jv,v,u,p,t  or  Jv=vjp(v,u,p,t  returns the adjoint derivative frac{df}{du}^\\ast v jac_prototype  a prototype matrix matching the type that matches the Jacobian For example if the Jacobian is tridiagonal then an appropriately sized  Tridiagonal  matrix can be used as the prototype and integrators will specialize on this structure where possible Non-structured sparsity patterns should use a  SparseMatrixCSC  with a correct sparsity pattern for the Jacobian The default is  nothing  which means a dense Jacobian paramjac(pJ,u,p,t  returns the parameter Jacobian  frac{df}{dp  syms  the symbol names for the elements of the equation This should match  u0  in size For example if  u0  0.0,1.0  and  syms  x y  this will apply a canonical naming to the values allowing  sol[:x  in the solution and automatically naming values in plots indepsym  the canonical naming for the independent variable Defaults to nothing which internally uses  t  as the representation in any plots colorvec  a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the  jac_prototype  This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern Defaults to  nothing  which means a color vector will be internally computed on demand when required The cost of this operation is highly dependent on the sparsity pattern iip In-Place vs Out-Of-Place For more details on this argument see the ODEFunction documentation recompile Controlling Compilation and Specialization For more details on this argument see the ODEFunction documentation Fields The fields of the DynamicalODEFunction type directly match the names of the inputs"},{"doctype":"documentation","id":"references/MethodOfLines.get_depvars","title":"get_depvars","text":"find all the dependent variables given by depvar_ops in an expression"},{"doctype":"documentation","id":"references/Catalyst.dependents","title":"dependents","text":"Given a  Reaction  and a  ReactionSystem  return a vector of  ModelingToolkit.Num s corresponding to species the  reaction rate   law  depends on E.g for k*W 2X  3Y  5Z  W the returned vector would be  W(t),X(t),Y(t  Notes Allocates Does not check for dependents within any subsystems"},{"doctype":"documentation","id":"references/RecursiveArrayTools.observed","title":"observed","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.GradientOperator","title":"GradientOperator","text":""},{"doctype":"documentation","id":"references/SciMLOperators.DEFAULT_UPDATE_FUNC","title":"DEFAULT_UPDATE_FUNC","text":""},{"doctype":"documentation","id":"references/NeuralPDE.NNPDENS","title":"NNPDENS","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.find_var_sccs","title":"find_var_sccs","text":"Find strongly connected components of the variables defined by  g   assign  gives the undirected bipartite graph a direction When  assign  nothing  we assume that the  i th variable is assigned to the  i th equation"},{"doctype":"documentation","id":"references/SciMLBase.EnsembleSummary","title":"EnsembleSummary","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/Optimization.NullData","title":"NullData","text":""},{"doctype":"document","id":"ExponentialUtilities/arnoldi.md","title":"Arnoldi Iteration","text":"Arnoldi Iteration API"},{"doctype":"documentation","id":"references/Surrogates.SOP","title":"SOP","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.has_substitutions","title":"has_substitutions","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.compute_xi","title":"compute_xi","text":""},{"doctype":"documentation","id":"references/RecursiveArrayTools._narrays","title":"_narrays","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.ConditionTimeWrapper","title":"ConditionTimeWrapper","text":""},{"doctype":"document","id":"Surrogates/welded_beam.md","title":"welded_beam","text":"Welded beam function The welded beam function is defined as  f(h,l,t  sqrt{\\frac{a^2  b^2  abl}{\\sqrt{0.25(l^2+(h+t)^2  With  a  frac{6000}{\\sqrt{2}hl   b  frac{6000(14  0.5l)*\\sqrt{0.25(l^2+(h+t)^2)}}{2*[0.707hl(\\frac{l^2}{12}+0.25*(h+t)^2 It has 3 dimension Define the objective function"},{"doctype":"documentation","id":"references/DiffEqSensitivity.fix_endpoints","title":"fix_endpoints","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_noiseeqs","title":"get_noiseeqs","text":""},{"doctype":"documentation","id":"references/SciMLBase.has_exp","title":"has_exp","text":""},{"doctype":"documentation","id":"references/SciMLBase.solution_slice","title":"solution_slice","text":""},{"doctype":"documentation","id":"references/SciMLBase.interp_summary","title":"interp_summary","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.reject_step!","title":"reject_step!","text":""},{"doctype":"documentation","id":"references/LinearSolve.LinearCache","title":"LinearCache","text":""},{"doctype":"document","id":"NeuralOperators/introduction.md","title":"Introduction","text":"Introduction Neural operator is a novel deep learning architecture It learns a operator which is a mapping between infinite-dimensional function spaces It can be used to resolve  partial differential equations PDE  Instead of solving by time-consuming finite element method a PDE problem can be resolved by training a neural network to learn an operator mapping from infinite-dimensional space  u t  to infinite-dimensional space  f(u t  Neural operator learns a continuous function between two continuous function spaces The kernel can be trained on different geometry including regular Euclidean space or a graph topology Fourier Neural Operators Fourier neural operator FNO learns a neural operator with Dirichlet kernel to form a Fourier transformation It performs Fourier transformation across infinite-dimensional function spaces and learns better than neural operator Markov Neural Operators Markov neural operator MNO learns a neural operator with Fourier operators With only one time step information of learning it can predict the following few steps with low loss by linking the operators into a Markov chain Deep Operator Network Deep operator network DeepONet learns a neural operator with the help of two sub-neural network structures described as the branch and the trunk network The branch network is fed the initial conditions data whereas the trunk is fed with the locations where the target(output is evaluated from the corresponding initial conditions It is important that the output size of the branch and trunk subnets is same so that a dot product can be performed between them"},{"doctype":"documentation","id":"references/RecursiveArrayTools.recursive_unitless_bottom_eltype","title":"recursive_unitless_bottom_eltype","text":"a Grabs the unitless element type at the bottom of the chain For example if ones has a  Array{Array{Float64,N},N  this will return  Float64 "},{"doctype":"documentation","id":"references/Catalyst.pprint_attrs","title":"pprint_attrs","text":""},{"doctype":"documentation","id":"references/Optimization.ObjSense","title":"ObjSense","text":""},{"doctype":"documentation","id":"references/Catalyst.make_empty_network","title":"make_empty_network","text":"Construct an empty  ReactionSystem   iv  is the independent variable usually time and  name  is the name to give the  ReactionSystem "},{"doctype":"documentation","id":"references/SciMLBase.AbstractODEFunction","title":"AbstractODEFunction","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/DiffEqOperators.compute_coeffs!","title":"compute_coeffs!","text":"Calculates the coefficients for the stencil of UpwindDifference operators"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.BoxWedgeTail!","title":"BoxWedgeTail!","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.ZygoteVJP","title":"ZygoteVJP","text":"compile ZygoteVJP  VJPChoice Uses Zygote.jl to compute vector-Jacobian products Tends to be the fastest VJP method if the ODE/DAE/SDE/DDE is written with mostly vectorized  functions like neural networks and other layers from Flux.jl and the  f  functions is given out-of-place If the  f  function is in-place then  Zygote.Buffer  arrays are used internally which can greatly reduce the performance of the VJP method Constructor"},{"doctype":"documentation","id":"references/PolyChaos.stieltjes","title":"stieltjes","text":"Stieltjes procedure---Given the nodes and weights the function generates the first N  recurrence coefficients of the corresponding discrete orthogonal polynomials Set the Boolean  removezeroweights  to  true  if zero weights should be removed"},{"doctype":"documentation","id":"references/Catalyst.ReactionSystem","title":"ReactionSystem","text":"rs rxs t rs rxs t A B C D k DocStringExtensions.TypeDefinition A system of chemical reactions Fields DocStringExtensions.TypeFields(false Example Continuing from the example in the  Reaction  definition Keyword Arguments observed::Vector{Equation  equations specifying observed variables systems::Vector{AbstractSystems  vector of sub-systems Can be  ReactionSystem s  ODESystem s or  NonlinearSystem s name::Symbol  the name of the system must be provided or  named  must be used defaults::Dict  a dictionary mapping parameters to their default values and species to their default initial values checks  true  boolean for whether to check units constraints  nothing  a  NonlinearSystem  or  ODESystem  of coupled constraint equations networkproperties  NetworkProperties  cache for network properties calculated via API functions combinatoric_ratelaws  true  sets the default value of  combinatoric_ratelaws  used in calls to  convert  or calling various problem types with the  ReactionSystem  Notes ReactionSystems currently do rudimentary unit checking requiring that all species have the same units and all reactions have rate laws with units of species units  time units Unit checking can be disabled by passing the keyword argument  checks=false "},{"doctype":"documentation","id":"references/DiffEqSensitivity.forward_sense","title":"forward_sense","text":""},{"doctype":"documentation","id":"references/SciMLBase.DDEProblem","title":"DDEProblem","text":"DiffEqProblemLibrary ODEProblemLibrary ODEProblemLibrary importodeproblems prob ODEProblemLibrary prob_ode_linear sol prob Defines a delay differential equation DDE problem Documentation Page https://diffeq.sciml.ai/stable/types/dde_types Mathematical Specification of a DDE Problem To define a DDE Problem you simply need to give the function  f  the initial condition  u_0  at time point  t_0  and the history function  h  which together define a DDE frac{du}{dt  f(u,h,p,t qquad t geq t_0 u(t_0  u_0 u(t  h(t qquad t  t_0 f  should be specified as  f(u h p t  or in-place as  f(du u h p t   u_0  should be an AbstractArray or number whose geometry matches the desired geometry of  u  and  h  should be specified as described below The history function  h  is accessed for all delayed values Note that we are not limited to numbers or vectors for  u_0  one is allowed to provide  u_0  as arbitrary matrices  higher dimension tensors as well Functional Forms of the History Function The history function  h  can be called in the following ways h(p t  out-of-place calculation h(out p t  in-place calculation h(p t deriv::Type{Val{i  out-of-place calculation of the  i th derivative h(out p t deriv::Type{Val{i  in-place calculation of the  i th derivative h(args idxs  calculation of  h(args  for indices  idxs Note that a dispatch for the supplied history function of matching form is required for whichever function forms are used in the user derivative function  f  Declaring Lags Lags are declared separately from their use One can use any lag by simply using the interpolant of  h  at that point However one should use caution in order to achieve the best accuracy When lags are declared the solvers can more efficiently be more accurate and thus this is recommended Neutral and Retarded Delay Differential Equations Note that the history function specification can be used to specify general retarded arguments i.e  h(p,α(u,t  Neutral delay differential equations can be specified by using the  deriv  value in the history interpolation For example  h(p,t-τ Val{1  returns the first derivative of the history values at time  t-τ  Note that algebraic equations can be specified by using a singular mass matrix Problem Type Constructors Parameter  isinplace  optionally sets whether the function is inplace or not This is determined automatically but not inferred Parameters are optional and if not given then a  NullParameters  singleton will be used which will throw nice errors if you try to index non-existent parameters Any extra keyword arguments are passed on to the solvers For example if you set a  callback  in the problem then that  callback  will be added in every solve call For specifying Jacobians and mass matrices see the DiffEqFunctions  performance_overloads page Arguments f  The function in the DDE u0  The initial condition Defaults to the value  h(p first(tspan  of the history function evaluated at the initial time point h  The history function for the DDE before  t0  tspan  The timespan for the problem p  The parameters with which function  f  is called Defaults to  NullParameters  constant_lags  A collection of constant lags used by the history function  h  Defaults to    dependent_lags  A tuple of functions  u p t  lag  for the state-dependent lags used by the history function  h  Defaults to    neutral  If the DDE is neutral i.e if delays appear in derivative terms order_discontinuity_t0  The order of the discontinuity at the initial time point Defaults to  0  if an initial condition  u0  is provided Otherwise it is forced to be greater or equal than  1  kwargs  The keyword arguments passed onto the solves Dynamical Delay Differential Equations Much like Dynamical ODEs  dynamical_prob a Dynamical DDE is a Partitioned DDE of the form frac{dv}{dt  f_1(u,t,h \n\\frac{du}{dt  f_2(v,h  Constructors Parameter  isinplace  optionally sets whether the function is inplace or not This is determined automatically but not inferred Arguments f  The function in the DDE v0  and  u0  The initial condition Defaults to the values  h(p first(tspan  of the history function evaluated at the initial time point h  The history function for the DDE before  t0  Must return an object with the indices 1 and 2 with the values of  v  and  u  respectively tspan  The timespan for the problem p  The parameters with which function  f  is called Defaults to  NullParameters  constant_lags  A collection of constant lags used by the history function  h  Defaults to    dependent_lags  A tuple of functions  v u p t  lag  for the state-dependent lags used by the history function  h  Defaults to    neutral  If the DDE is neutral i.e if delays appear in derivative terms order_discontinuity_t0  The order of the discontinuity at the initial time point Defaults to  0  if an initial condition  u0  is provided Otherwise it is forced to be greater or equal than  1  kwargs  The keyword arguments passed onto the solves The for dynamical and second order DDEs the history function will return an object with the indicies 1 and 2 defined where  h(p t_prev)[1  is the value of  f_2(v u h p t_{\\mathrm{prev  and  h(p t_prev)[2  is the value of  f_1(v u h p t_{\\mathrm{prev  this is for consistency with the ordering of the intitial conditions in the constructor The supplied history function must also return such a 2-index object which can be accomplished with a tuple  v,u  or vector  v,u  2nd Order Delay Differential Equations To define a 2nd Order DDE Problem you simply need to give the function  f  and the initial condition  u_0  which define an DDE u  f(u',u,h,p,t f  should be specified as  f(du,u,p,t  or in-place as  f(ddu,du,u,p,t  and  u₀  should be an AbstractArray or number whose geometry matches the desired geometry of  u  Note that we are not limited to numbers or vectors for  u₀  one is allowed to provide  u₀  as arbitrary matrices  higher dimension tensors as well From this form a dynamical ODE v  f(v,u,h,p,t \nu  v  Constructors Parameter  isinplace  optionally sets whether the function is inplace or not This is determined automatically but not inferred Arguments f  The function in the DDE du0  and  u0  The initial condition Defaults to the values  h(p first(tspan  of the history function evaluated at the initial time point h  The history function for the DDE before  t0  Must return an object with the indices 1 and 2 with the values of  v  and  u  respectively tspan  The timespan for the problem p  The parameters with which function  f  is called Defaults to  NullParameters  constant_lags  A collection of constant lags used by the history function  h  Defaults to    dependent_lags  A tuple of functions  v u p t  lag  for the state-dependent lags used by the history function  h  Defaults to    neutral  If the DDE is neutral i.e if delays appear in derivative terms order_discontinuity_t0  The order of the discontinuity at the initial time point Defaults to  0  if an initial condition  u0  is provided Otherwise it is forced to be greater or equal than  1  kwargs  The keyword arguments passed onto the solves As above the history function will return an object with indices 1 and 2 with the values of  du  and  u  respectively The supplied history function must also match this return type e.g by returning a 2-element tuple or vector Example Problems Example problems can be found in  DiffEqProblemLibrary.jl  To use a sample problem such as  prob_ode_linear  you can do something like"},{"doctype":"document","id":"DiffEqNoiseProcess/abstract_noise_processes.md","title":"Abstract Noise Processes","text":"Abstract Noise Processes In addition to the  NoiseProcess  type more general  AbstractNoiseProcess es are defined The  NoiseGrid  allows you to define a noise process from a set of pre-calculated points the normal way The  NoiseApproximation  allows you to define a new noise process as the solution to some stochastic differential equation While these methods are only approximate they are more general and allow the user to easily define their own colored noise to use in simulations The  NoiseWrapper  allows one to wrap a  NoiseProcess  from a previous simulation to re-use it in a new simulation in a way that follows the same stochastic trajectory even if different points are hit for example solving with a smaller  dt  in a distributionally-exact manner It is demonstrated how the  NoiseWrapper  can be used to wrap the  NoiseProcess  of one SDE/RODE solution in order to re-use the same noise process in another simulation The  VirtualBrownianTree  allows one to trade speed for O(1 memory usage Instead of storing Brownian motion increments the  VirtualBrownianTree  samples recursively from the midpoint  tmid  of Brownian bridges using a splittable PRNG The recursion terminates when the query time agrees within some tolerance with  tmid  or when the maximum depth of the tree is reached Lastly the  NoiseFunction  allows you to use any function of time as the noise process Together this functionality allows you to define any colored noise process and use this efficiently and accurately in your simulations The Standard  AbstractNoiseProcess Alternative  AbstractNoiseProcess  Types In addition to the mathematically-defined noise processes above there exists more generic functionality for building noise processes from other noise processes from arbitrary functions from arrays and from approximations of stochastic differential equations"},{"doctype":"document","id":"ModelingToolkit/internals.md","title":"Internal Details","text":"Internal Details This is a page for detailing some of the inner workings to help future contributors to the library Observables and Variable Elimination In the variable elimination algorithms what is actually done is that variables are removed from being states and equations are moved into the  observed  category of the system The  observed  equations are explicit algebraic equations which are then substituted out to completely eliminate these variables from the other equations allowing the system to act as though these variables no longer exist However as a user may have wanted to interact with such variables for example plotting their output these relationships are stored and are then used to generate the  observed  equation found in the  SciMLFunction  interface so that  sol[x  lazily reconstructs the observed variable when necessary In this sense there is an equivalence between observables and the variable elimination system The procedure for variable elimination inside  structural_simplify  is ModelingToolkit.initialize_system_structure  ModelingToolkit.alias_elimination  This step moves equations into  observed(sys  ModelingToolkit.dae_index_lowering  by means of  pantelides  if the system is an  ODESystem  ModelingToolkit.tearing  Preparing a system for simulation Before a simulation or optimization can be performed the symbolic equations stored in an  AbstractSystem  must be converted into executable code This step is typically occurs after the simplification explained above and is performed when an instance of a  AbsSciMLBase.SciMLProblem  such as a  ODEProblem  is constructed The call chain typically looks like this with the function names in the case of an  ODESystem  indicated in parenthesis Problem constructor  ODEProblem  Build an  DEFunction   process_DEProblem    ODEFunction Write actual executable code  generate_function  Apart from  generate_function  which generates the dynamics function  ODEFunction  also builds functions for observed equations  build_explicit_observed_function  and jacobians  generate_jacobian  etc These are all stored in the  ODEFunction "},{"doctype":"documentation","id":"references/NeuralPDE.get_phi","title":"get_phi","text":""},{"doctype":"document","id":"RecursiveArrayTools/index.md","title":"RecursiveArrayTools.jl: Arrays of Arrays and Even Deeper","text":"Pkg Pkg add RecursiveArrayTools.jl Arrays of Arrays and Even Deeper RecursiveArrayTools.jl is a set of tools for dealing with recursive arrays like arrays of arrays It contains type wrappers for making recursive arrays act more like normal arrays for example automating the recursion of broadcast maps iteration and more and utility functions which make it easier to work with recursive arrays Installation To install RecursiveArrayTools.jl use the Julia package manager Contributing Please refer to the  SciML ColPrac Contributor's Guide on Collaborative Practices for Community Packages  for guidance on PRs issues and other matters relating to contributing to SciML There are a few community forums the diffeq-bridged channel in the  Julia Slack JuliaDiffEq  on Gitter on the  Julia Discourse forums see also  SciML Community page"},{"doctype":"documentation","id":"references/ExponentialUtilities.exponential!","title":"exponential!","text":"Computes the matrix exponential with method specified in  method  The contents of  A  is modified allowing for less allocations The  method  parameter specifies the implementation and implementation parameters e.g  ExpMethodNative   ExpMethodDiagonalization   ExpMethodGeneric   ExpMethodHigham2005  Memory needed can be preallocated and provided in parameter  cache  such that the memory can recycled when calling  exponential  several times The preallocation is done with the command  alloc_mem   cache=alloc_mem(A,method  Example"},{"doctype":"documentation","id":"references/LabelledArrays.@LArray","title":"@LArray","text":"Eltype Size Names Values Names A a b c A a A Float64 a b c d W rand A W A d W The  LArray  macro creates an  LArray  with names determined from the  Names  vector and values determined from the  Values  vector Otherwise the eltype and size are used to make an  LArray  with undefined values Users can also generate a labelled array with undefined values by instead giving the dimensions This approach is useful if the user intends to pre-allocate an array for some later input Users may also use an alternative constructor to set the Names and Values and ranges at the same time The labels of LArray and SLArray can be accessed by function  symbols  which returns a tuple of symbols"},{"doctype":"documentation","id":"references/RecursiveArrayTools","title":"RecursiveArrayTools","text":"DocStringExtensions.Readme"},{"doctype":"documentation","id":"references/Catalyst.make_ReactionSystem_internal","title":"make_ReactionSystem_internal","text":""},{"doctype":"documentation","id":"references/Catalyst.Digraph","title":"Digraph","text":""},{"doctype":"document","id":"DiffEqSensitivity/sensitivity_math.md","title":"[Mathematics of Sensitivity Analysis]( sensitivity_math)","text":"Mathematics of Sensitivity Analysis  sensitivity_math Forward Sensitivity Analysis The local sensitivity is computed using the sensitivity ODE frac{d}{dt}\\frac{\\partial u}{\\partial p_{j}}=\\frac{\\partial f}{\\partial u}\\frac{\\partial u}{\\partial p_{j}}+\\frac{\\partial f}{\\partial p_{j}}=J\\cdot S_{j}+F_{j where J=\\left(\\begin{array}{cccc}\n\\frac{\\partial f_{1}}{\\partial u_{1  frac{\\partial f_{1}}{\\partial u_{2  cdots  frac{\\partial f_{1}}{\\partial u_{k}}\\\\\n\\frac{\\partial f_{2}}{\\partial u_{1  frac{\\partial f_{2}}{\\partial u_{2  cdots  frac{\\partial f_{2}}{\\partial u_{k}}\\\\\n\\cdots  cdots  cdots  cdots\\\\\n\\frac{\\partial f_{k}}{\\partial u_{1  frac{\\partial f_{k}}{\\partial u_{2  cdots  frac{\\partial f_{k}}{\\partial u_{k}}\n\\end{array}\\right is the Jacobian of the system F_{j}=\\left(\\begin{array}{c}\n\\frac{\\partial f_{1}}{\\partial p_{j}}\\\\\n\\frac{\\partial f_{2}}{\\partial p_{j}}\\\\\n\\vdots\\\\\n\\frac{\\partial f_{k}}{\\partial p_{j}}\n\\end{array}\\right are the parameter derivatives and S_{j}=\\left(\\begin{array}{c}\n\\frac{\\partial u_{1}}{\\partial p_{j}}\\\\\n\\frac{\\partial u_{2}}{\\partial p_{j}}\\\\\n\\vdots\\\\\n\\frac{\\partial u_{k}}{\\partial p_{j}}\n\\end{array}\\right is the vector of sensitivities Since this ODE is dependent on the values of the independent variables themselves this ODE is computed simultaneously with the actual ODE system Note that the Jacobian-vector product frac{\\partial f}{\\partial u}\\frac{\\partial u}{\\partial p_{j can be computed without forming the Jacobian With finite differences this through using the following formula for the directional derivative Jv approx frac{f(x+v epsilon  f(x)}{\\epsilon or alternatively and without truncation error by using a dual number with a single partial dimension  d  x  v epsilon  we get that f(d  f(x  Jv epsilon as a fast way to calcuate  Jv  Thus except when a sufficiently good function for  J  is given by the user the Jacobian is never formed For more details consult the  MIT 18.337 lecture notes on forward mode AD  Adjoint Sensitivity Analysis This adjoint requires the definition of some scalar functional  g(u,p  where  u(t,p  is the numerical solution to the differential equation  d/dt u(t,p)=f(t,u,p  with  t\\in 0,T  and  u(t_0,p)=u_0  Adjoint sensitivity analysis finds the gradient of G(u,p)=G(u(\\cdot,p))=\\int_{t_{0}}^{T}g(u(t,p),p)dt some integral of the solution It does so by solving the adjoint problem frac{d\\lambda^{\\star}}{dt}=g_{u}(u(t,p),p)-\\lambda^{\\star}(t)f_{u}(t,u(t,p),p),\\thinspace\\thinspace\\thinspace\\lambda^{\\star}(T)=0 where  f_u  is the Jacobian of the system with respect to the state  u  while  f_p  is the Jacobian with respect to the parameters The adjoint problem's solution gives the sensitivities through the integral frac{dG}{dp}=\\int_{t_{0}}^{T}\\lambda^{\\star}(t)f_{p}(t)+g_{p}(t)dt+\\lambda^{\\star}(t_{0})u_{p}(t_{0 Notice that since the adjoints require the Jacobian of the system at the state it requires the ability to evaluate the state at any point in time Thus it requires the continuous forward solution in order to solve the adjoint solution and the adjoint solution is required to be continuous in order to calculate the resulting integral There is one extra detail to consider In many cases we would like to calculate the adjoint sensitivity of some discontinuous functional of the solution One canonical function is the L2 loss against some data points that is L(u,p)=\\sum_{i=1}^{n}\\Vert\\tilde{u}(t_{i})-u(t_{i},p)\\Vert^{2 In this case we can reinterpret our summation as the distribution integral G(u,p)=\\int_{0}^{T}\\sum_{i=1}^{n}\\Vert\\tilde{u}(t_{i})-u(t_{i},p)\\Vert^{2}\\delta(t_{i}-t)dt where  δ  is the Dirac distribution In this case the integral is continuous except at finitely many points Thus it can be calculated between each  t_i  At a given  t_i  given that the  t_i  are unique we have that g_{u}(t_{i})=2\\left(\\tilde{u}(t_{i})-u(t_{i},p)\\right Thus the adjoint solution  lambda^{\\star}(t  is given by integrating between the integrals and applying the jump function  g_u  at every data point  t_i  We note that lambda^{\\star}(t)f_{u}(t is a vector-transpose Jacobian product also known as a  vjp  which can be efficiently computed using the pullback of backpropogation on the user function  f  with a forward pass at  u  with a pullback vector  lambda^{\\star  For more information consult the  MIT 18.337 lecture notes on reverse mode AD"},{"doctype":"documentation","id":"references/DiffEqSensitivity.discretize_ref_trajectory!","title":"discretize_ref_trajectory!","text":""},{"doctype":"documentation","id":"references/SciMLOperators.DiagonalOperator","title":"DiagonalOperator","text":"Diagonal Operator"},{"doctype":"documentation","id":"references/ExponentialUtilities._phiv_timestep_adapt","title":"_phiv_timestep_adapt","text":""},{"doctype":"documentation","id":"references/LabelledArrays.symbols","title":"symbols","text":"julia z Float64 a b c d julia z a b c d Returns the labels of the  SLArray   For example Returns the labels of the  LArray   For example"},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timeseries_steps_mean","title":"timeseries_steps_mean","text":""},{"doctype":"documentation","id":"references/SciMLBase.SciMLProblem","title":"SciMLProblem","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/NeuralPDE.NNPDEHan","title":"NNPDEHan","text":""},{"doctype":"documentation","id":"references/Catalyst.make_hillr_exp","title":"make_hillr_exp","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.CenteredDifference","title":"CenteredDifference","text":"See also  UpwindDifference"},{"doctype":"documentation","id":"references/SciMLBase.AbstractSensitivitySolution","title":"AbstractSensitivitySolution","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/ModelingToolkit.IsHistory","title":"IsHistory","text":""},{"doctype":"documentation","id":"references/Surrogates.GEKStructure","title":"GEKStructure","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.calculate_control_jacobian","title":"calculate_control_jacobian","text":"sys Calculate the jacobian matrix of a system with respect to the system's controls Returns a matrix of  Num  instances The result from the first call will be cached in the system object"},{"doctype":"documentation","id":"references/SciMLBase.ConstantInterpolation","title":"ConstantInterpolation","text":"DocStringExtensions.TypeDefinition"},{"doctype":"document","id":"DiffEqFlux/examples/neural_ode.md","title":"Neural Ordinary Differential Equations","text":"Lux DifferentialEquations OptimizationOptimJL Random Plots rng Random default_rng u0 Float32 datasize tspan tsteps range tspan tspan length datasize trueODEfunc du u p t true_A du u true_A prob_trueode trueODEfunc u0 tspan ode_data Array prob_trueode Tsit5 saveat tsteps dudt2 Lux ActivationFunction x x Lux Dense tanh Lux Dense p st Lux setup rng dudt2 prob_neuralode dudt2 tspan Tsit5 saveat tsteps predict_neuralode p Array prob_neuralode u0 p st loss_neuralode p pred predict_neuralode p loss sum abs2 ode_data pred loss pred callback p l pred doplot display l plt scatter tsteps ode_data label scatter! plt tsteps pred label doplot display plot plt adtype optf x p loss_neuralode x adtype optprob optf Lux ComponentArray p result_neuralode optprob ADAM cb callback maxiters optprob2 optprob u0 result_neuralode u result_neuralode2 optprob2 LBFGS allow_f_increases Lux DifferentialEquations OptimizationOptimJL Random Plots rng Random default_rng u0 Float32 datasize tspan tsteps range tspan tspan length datasize trueODEfunc du u p t true_A du u true_A prob_trueode trueODEfunc u0 tspan ode_data Array prob_trueode Tsit5 saveat tsteps dudt2 Lux ActivationFunction x x Lux Dense tanh Lux Dense p st Lux setup rng dudt2 prob_neuralode dudt2 tspan Tsit5 saveat tsteps dudt2 x x Dense tanh Dense predict_neuralode p Array prob_neuralode u0 p st loss_neuralode p pred predict_neuralode p loss sum abs2 ode_data pred loss pred callback p l pred doplot display l plt scatter tsteps ode_data label scatter! plt tsteps pred label doplot display plot plt Neural Ordinary Differential Equations A neural ODE is an ODE where a neural network defines its derivative function Thus for example with the multilayer perceptron neural network  Lux.Chain(Lux.Dense(2 50 tanh Lux.Dense(50 2  we can define a differential equation which is  u  NN(u  This is done simply by the  NeuralODE  struct Let's take a look at an example Copy-Pasteable Code Before getting to the explanation here's some code to start with We will follow a full explanation of the definition and training process Neural ODE Explanation Let's get a time series array from a sprial ODE to train against Now let's define a neural network with a  NeuralODE  layer First we define the layer Here we're going to use  Lux.Chain  which is a suitable neural network structure for NeuralODEs with separate handling of state variables Note that we can directly use  Chain s from Flux.jl as well for example In our model we used the  x  x.^3  assumption in the model By incorporating structure into our equations we can reduce the required size and training time for the neural network but a good guess needs to be known From here we build a loss function around it The  NeuralODE  has an optional second argument for new parameters which we will use to iteratively change the neural network in our training loop We will use the L2 loss of the network's output against the time series data We define a callback function We then train the neural network to learn the ODE Here we showcase starting the optimization with  ADAM  to more quickly find a minimum and then honing in on the minimum by using  LBFGS  By using the two together we are able to fit the neural ODE in 9 seconds Note the timing commented out the plotting You can easily incorporate the procedure below to set up custom optimization problems For more information on the usage of  Optimization.jl  please consult  this  documentation The  x  and  p  variables in the optimization function are different than  x  and  p  above The optimization function runs over the space of parameters of the original problem so  x_optimization    p_original  We then complete the training using a different optimizer starting from where  ADAM  stopped We do  allow_f_increases=false  to make the optimization automatically halt when near the minimum"},{"doctype":"document","id":"PolyChaos/DCsOPF.md","title":"Chance-Constrained DC Optimal Power Flow","text":"Chance-Constrained DC Optimal Power Flow The purpose of this tutorial is to show how polynomial chaos can be leveraged to solve optimization problems under uncertainty Specifically we study chance-constrained DC optimal power flow as it is presented in  this paper  We consider the following 4-bus system that has a total of two generators buses 1 and 3 and two loads buses 2 and 4 4-bus system We formalize the numbering of the generators superscript  g  loads superscript  d  for demand and branches superscript  br  as follows mathcal{N}^g   1 3  mathcal{N}^d   2 4  mathcal{N}^{br   1 2 3 4 5  With each generator we associate a linear cost with cost coefficient  c_i  for all  i in mathcal{N}^g  Each generator must adhere to its engineering limits given by  underline{p}_i^g  overline{p}_i^g   for all  i in mathcal{N}^g  Also each line is constrained by its limits  underline{p}_i^{br overline{p}_i^{br  for all  i in mathcal{N}^{br  We model the demand at the buses  i in mathcal{N}^d  in terms of uniform distributions with known mean  mu_i  and standard deviation  sigma_i  We concisely write mathsf{p}_i^d sim mathsf{U}(\\mu_i sigma_i quad forall i in mathcal{N}^d For simplicity we consider DC conditions Hence energy balance reads sum_{i in mathcal{N}^g mathsf{p}_i^g  sum_{i in mathcal{N}^d mathsf{p}_i^d  0 and the vector of branch flows is computed from the power transfer distribution factor PTDF matrix  Psi  via mathsf{p}^{br  Psi C^p\\mathsf{p}^g  C^d\\mathsf{p}^d The matrices  C^p  and  C^d  map the generators and the loads to the correct buses respectively We want to solve a chance-constrained optimal power flow problem under DC conditions According to  this paper  we can formulate the problem as underset   sum__g c_i mathbb mathsf_i^g subject to sum_{i in mathcal{N}^g mathsf{p}_i^g  sum_{i in mathcal{N}^d mathsf{p}_i^d  0 \n\\underline{p}_i^g leq mathbb{E}(\\mathsf{p}_i^g pm lambda_i^g sqrt{\\mathbb{V}(\\mathsf{p}_i^g leq overline{p}_i^g  forall i in mathcal{N}^g,\\\\\n\\underline{p}_i^{br leq mathbb{E}(\\mathsf{p}_i^{br pm lambda_i^{br sqrt{\\mathbb{V}(\\mathsf{p}_i^{br leq overline{p}_i^{br forall i in mathcal{N}^{br which minimizes the total expected generation cost subject to the DC power flow equations and chance-constrained engineering limits Let's solve the problem using  PolyChaos  and  JuMP  using  Mosek  as a solver Let's define system-specific quantities such as the incidence matrix and the branch flow parameters From these we can compute the PTDF matrix  Psi  assuming the slack is at bus 1 Now we can continue the remaining ingredients that specify our systems We specify the uncertainty using  PolyChaos  It remains to specify the PCE coefficients for which we will use  convert2affine  Now let's put it all into an optimization problem specifically a second-order cone program To build the second-order cone constraints we define a helper function  buildSOC  Finally let's use  JuMP  to formulate and then solve the problem We use  Mosek  to solve the problem for academic use there are  free license  Let's extract the numerical values of the optimal solution Great we've solved the problem How do we now make sense of the solution For instance we can look at the moments of the generated power Simiarly we can study the moments for the branch flows"},{"doctype":"documentation","id":"references/Catalyst.reorder_states!","title":"reorder_states!","text":"Given a  ReactionSystem  and a vector  neworder  orders the states of  rn  accordingly to  neworder  Notes Currently only supports  ReactionSystem s without constraints or subsystems"},{"doctype":"documentation","id":"references/ModelingToolkit.SystemStructures.complete!","title":"complete!","text":""},{"doctype":"documentation","id":"references/NeuralPDE.NeuralPDEAlgorithm","title":"NeuralPDEAlgorithm","text":""},{"doctype":"document","id":"NeuralPDE/pinn/3rd.md","title":"ODE with a 3rd-Order Derivative","text":"Flux GalacticOptimJL Interval infimum supremum x u Dxxx Differential x Dx Differential x eq Dxxx u x cos pi x bcs u u cos pi Dx u domains x Interval Flux σ discretization pde_system eq bcs domains x u x prob pde_system discretization callback p l println l res prob ADAM callback callback maxiters discretization Plots analytic_sol_func x π x x π x sin π x π dx xs infimum d domain dx supremum d domain d domains u_real analytic_sol_func x x xs u_predict first x res minimizer x xs x_plot collect xs plot x_plot u_real title plot! x_plot u_predict title ODE with a 3rd-Order Derivative Let's consider the ODE with a 3rd-order derivative begin{align*}\n∂^3_x u(x  cos(\\pi x  \nu(0  0  \nu(1  cos(\\pi  \n∂_x u(0  1  \nx in 0 1  \n\\end{align We will use physics-informed neural networks We can plot the predicted solution of the ODE and its analytical solution hodeplot"},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.is_abs_half","title":"is_abs_half","text":""},{"doctype":"document","id":"Optimization/optimization_packages/nonconvex.md","title":"Nonconvex.jl","text":"Pkg Pkg add rosenbrock x p p x p x x x0 zeros p f rosenbrock prob f x0 p lb ub sol prob MMA02 maxiters maxtime rosenbrock x p p x p x x x0 zeros p f rosenbrock prob f x0 p lb ub sol prob HyperoptAlg IpoptAlg sub_options max_iter Nonconvex.jl Nonconvex  is a is a Julia package implementing and wrapping nonconvex constrained optimization algorithms Installation OptimizationNonconvex.jl To use this package install the OptimizationNonconvex package Global Optimizer Without Constraint Equations A  Nonconvex  algorithm is called using one of the following Method of moving asymptotes MMA  MMA87 MMA02 Ipopt  IpoptAlg NLopt  NLoptAlg(solver  where solver can be any of the  NLopt  algorithms Augmented Lagrangian algorithm  AugLag only works with constraints Mixed integer nonlinear programming MINLP  Juniper  Ipopt  JuniperIpoptAlg Pavito  Ipopt  Cbc  PavitoIpoptCbcAlg Multi-start optimization  HyperoptAlg(subsolver  where  subalg  can be any of the described  Nonconvex  algorithm Surrogate-assisted Bayesian optimization BayesOptAlg(subsolver  where  subalg  can be any of the described  Nonconvex  algorithm Multiple Trajectory Search MTSAlg When performing optimizing a combination of integer and floating-point parameters the  integer  keyword has to be set It takes a boolean vector indicating which parameter is an integer Notes Some optimizer may require further options to be defined in order to work The currently available algorithms are listed  here The algorithms in  Nonconvex  are performing global optimization on problems without constraint equations However lower and upper constraints set by  lb  and  ub  in the  OptimizationProblem  are required Examples The Rosenbrock function can optimized using the Method of moving asymptotes algorithm  MMA02  as follows The options of for a sub-algorithm are passed simply as a NamedTuple and GalactcOptim infers the correct  Nonconvex  options struct With Constraint Equations While  Nonconvex.jl  supports such constraints  Optimization.jl  currently does not relay these constraints"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.VirtualBrownianTree!","title":"VirtualBrownianTree!","text":""},{"doctype":"documentation","id":"references/PolyChaos.w_jacobi","title":"w_jacobi","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.assemble_crj_expr","title":"assemble_crj_expr","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.equations","title":"equations","text":""},{"doctype":"document","id":"PolyChaos/random_ode.md","title":"Galerkin-based Solution of Random Differential Equation","text":"Galerkin-based Solution of Random Differential Equation This tutorial demonstrates how random differential equations can be solved using polynomial chaos expansions PCE Theory A random differential equation is an ordinary differential equation that has random parameters hence its solution is itself a time-varying random variable Perhaps the simplest non-trivial example is the following scalar linear ordinary differential equation dot{x}(t  a x(t quad x(0  x_{0 where  a  is the realization of a Gaussian random variable  mathsf{a sim mathcal{N}(\\mu sigma^2  with mean  mu  and variance  sigma^2  Arguably for every realization  a  we can solve the differential equation and obtain x(t  x_0 mathrm{e}^{a t from which we find that ln x(t  ln x_0  at sim mathcal{N}(\\ln(x_0  mu t sigma t)^2 In other words the logarithm of the solution is normally distributed so-called  log-normal distribution  We'd like to obtain this result numerically with the help of PCE The first step is to define the truncated PCE for the random variable  mathsf{a mathsf{a  sum_{i=0}^{L a_i phi_i where  a_i  are the so-called PCE coefficients and  phi_i  are the orthogonal basis polynomials As the solution to the random differential equation is itself a random variable we treat  x(t  as the realization of the random variable  mathsf{x}(t  and define its PCE mathsf{x}(t  sum_{i=0}^{L x_i(t phi_i The question is how to obtain the unknown PCE coefficients  x_i(t  from the known PCE coefficients  a_i  relative to the orthogonal basis polynomials  phi_i  This can be done using Galerkin projection which is nothing else than projecting onto the orthogonal basis Think of a three-dimensional space in which you have placed some three-dimensional object If you know project the silhouett of the object onto every axis of the three-dimensional space then you are doing a Galerkin projection With PCE the concept is equivalent but the imagination has a harder time The first step for Galerkin projection is to insert the PCEs sum_{i=0}^{L dot{x}_i(t phi_i  sum_{j=0}^{L a_j phi_j sum_{k=0}^{L x_k(t phi_k the second step is to project onto every basis polynomial  phi_m  for  m  0 1 dots L  and to exploit orthogonality of the basis This gives dot{x}_m(t langle phi_m phi_m rangle  sum_{j=0}^{L sum_{k=0}^{L a_j x_k(t langle phi_l phi_k phi_m rangle quad m  0 1 dots L Of course the initial condition must not be forgotten x_0(0  x_0 quad x_m(0  0 quad m  1 dots L If we can solve this enlarged system of ordinary random differential equations we can reconstruct the analytic solution Practice We begin by defining the random differential equation Next we define an orthogonal basis and its quadrature rule relative to the Gaussian measure using  PolyChaos  We choose a maximum degree of  L  Now we can define the PCE for  mathsf{a  and solve the Galerkin-projected ordinary differential equation using  DifferentialEquations.jl  For later purposes we compute the expected value and the standard deviation at all time instants using PCE We compare the solution from PCE to a Monte-Carlo-based solution That means to solve the ordinary differential equation for many samples of  mathsf{a  We first sample from the measure using  sampleMeasure  and then generate samples of  mathsf{a  using  evaluatePCE  After that we solve the ODE and store the results in  xmc  Now we can compare the Monte Carlo mean and standard deviation to the expression from PCE for every time instant Clearly the accuracy of PCE deteriorates over time Possible remedies are to increase the dimension of PCE and to tweak the tolerances of the integrator Finally we compare whether the samples follow a log-normal distribution and compare the result to the analytic mean and standard deviation"},{"doctype":"documentation","id":"references/LabelledArrays.symnames","title":"symnames","text":""},{"doctype":"documentation","id":"references/Optimization.maybe_with_logger","title":"maybe_with_logger","text":""},{"doctype":"documentation","id":"references/NeuralPDE.build_symbolic_loss_function","title":"build_symbolic_loss_function","text":"Build a loss function for a PDE or a boundary condition Examples System of PDEs Take expressions in the form Dx(u1(x,y  4 Dy(u2(x,y  0   Dx(u2(x,y  9 Dy(u1(x,y  0 to cord θ phi derivative u)->begin       begin θ1 θ2  θ[1:33 θ\"[34:66 phi1 phi2  phi[1 phi[2 let x y  cord[1 cord[2 derivative(phi1 u x y ε 0.0 1 θ1  4 derivative(phi2 u x y 0.0 ε 1 θ2  0   derivative(phi2 u x y ε 0.0 1 θ2  9 derivative(phi1 u x y 0.0 ε 1 θ1  0 end end end"},{"doctype":"documentation","id":"references/SciMLBase.warn_compat","title":"warn_compat","text":"DocStringExtensions.MethodSignatures Emit a warning with a link to the solver compatibility chart in the documentation"},{"doctype":"documentation","id":"references/Integrals.v_semiinf","title":"v_semiinf","text":""},{"doctype":"documentation","id":"references/SciMLBase.addat_non_user_cache!","title":"addat_non_user_cache!","text":"addat s the non-user facing caches at indices  idxs  This includes resizing Jacobian caches Note In many cases  addat  simply  addat s  full_cache  variables and then calls this function This finer control is required for some  AbstractArray  operations"},{"doctype":"documentation","id":"references/ExponentialUtilities.getV","title":"getV","text":""},{"doctype":"documentation","id":"references/Optimization.AutoModelingToolkit","title":"AutoModelingToolkit","text":""},{"doctype":"documentation","id":"references/NeuralPDE","title":"NeuralPDE","text":"DocStringExtensions.Readme"},{"doctype":"documentation","id":"references/SciMLBase.__has_jac","title":"__has_jac","text":""},{"doctype":"documentation","id":"references/MethodOfLines.generate_boundary_val_funcs","title":"generate_boundary_val_funcs","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.terminate_conditions","title":"terminate_conditions","text":""},{"doctype":"document","id":"QuasiMonteCarlo/samplers.md","title":"Sampler APIs","text":"Sampler APIs Sample Samplers"},{"doctype":"documentation","id":"references/DiffEqOperators.ComposedBoundaryPaddedArray","title":"ComposedBoundaryPaddedArray","text":""},{"doctype":"documentation","id":"references/Catalyst.get_pexprs","title":"get_pexprs","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.structural_simplify","title":"structural_simplify","text":"DocStringExtensions.MethodSignatures Structurally simplify algebraic equations in a system and compute the topological sort of the observed equations When  simplify=true  the  simplify  function will be applied during the tearing process It also takes kwargs  allow_symbolic=false  and  allow_parameter=true  which limits the coefficient types during tearing"},{"doctype":"documentation","id":"references/SciMLBase.DiscreteCallback","title":"DiscreteCallback","text":"condition affect! initialize finalize save_positions Arguments condition  This is a function  condition(u,t,integrator  for declaring when the callback should be used A callback is initiated if the condition evaluates to  true  See the Integrator Interface  integrator documentation for information about  integrator  affect  This is the function  affect!(integrator  where one is allowed to modify the current state of the integrator For more information on what can be done see the Integrator Interface  integrator manual page save_positions  Boolean tuple for whether to save before and after the  affect  This saving will occur just before and after the event only at event times and does not depend on options like  saveat   save_everystep  etc i.e if  saveat=[1.0,2.0,3.0  this can still add a save point at  2.1  if true For discontinuous changes like a modification to  u  to be handled correctly without error one should set  save_positions=(true,true  initialize  This is a function  c,u,t,integrator  which can be used to initialize the state of the callback  c  It should modify the argument  c  and the return is ignored finalize  This is a function  c,u,t,integrator  which can be used to finalize the state of the callback  c  It should can the argument  c  and the return is ignored"},{"doctype":"documentation","id":"references/Catalyst.fwd_arrows","title":"fwd_arrows","text":""},{"doctype":"document","id":"DiffEqSensitivity/bayesian/turing_bayesian.md","title":"Bayesian Estimation of Differential Equations with Probabilistic Programming","text":"Bayesian Estimation of Differential Equations with Probabilistic Programming For a good overview of how to use the tools of SciML in conjunction with the Turing.jl probabilistic programming language see the  Bayesian Differential Equation Tutorial "},{"doctype":"documentation","id":"references/SciMLBase.auto_dt_reset!","title":"auto_dt_reset!","text":"Run the auto  dt  initialization algorithm"},{"doctype":"documentation","id":"references/Catalyst.params","title":"params","text":""},{"doctype":"documentation","id":"references/Integrals.QuadGKJL","title":"QuadGKJL","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.var","title":"var","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.generate_jacobian","title":"generate_jacobian","text":"sys dvs sys ps sys expression Val sparse kwargs Generates a function for the jacobian matrix matrix of a system Extra arguments control the arguments to the internal  build_function  call"},{"doctype":"documentation","id":"references/PolyChaos.w_gamma","title":"w_gamma","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.AffineBC","title":"AffineBC","text":"Robin General and in general Neumann Dirichlet and Bridge BCs are not necessarily linear operators Instead they are affine operators with a constant term Q x  Qa x  Qb"},{"doctype":"documentation","id":"references/DiffEqSensitivity.compile_tape","title":"compile_tape","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.has_original_jac","title":"has_original_jac","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.SDEFunctionExpr","title":"SDEFunctionExpr","text":"Create a Julia expression for an  SDEFunction  from the  SDESystem  The arguments  dvs  and  ps  are used to set the order of the dependent variable and parameter vectors respectively"},{"doctype":"document","id":"SciMLOperators/interface.md","title":"The AbstractSciMLOperator Interface","text":"The AbstractSciMLOperator Interface Formal Properties of DiffEqOperators These are the formal properties that an  AbstractSciMLOperator  should obey for it to work in the solvers AbstractDiffEqOperator Interface Description Function call and multiplication  L(du,u,p,t  for inplace and  du  L(u,p,t  for out-of-place meaning  L*u  and  mul  If the operator is not a constant update it with  u,p,t  A mutating form i.e  update_coefficients!(A,u,p,t  that changes the internal coefficients and a out-of-place form  B  update_coefficients(A,u,p,t  isconstant(A  trait for whether the operator is constant or not AbstractDiffEqLinearOperator Interface Description AbstractSciMLLinearOperator  AbstractSciMLOperator Can absorb under multiplication by a scalar In all algorithms things like  dt*L  show up all the time so the linear operator must be able to absorb such constants isconstant(A  trait for whether the operator is constant or not Optional  diagonal   symmetric  etc traits from LinearMaps.jl Optional  exp(A  Required for simple exponential integration Optional  expv(A,u,t  exp(t*A)*u  and  expv!(v,A::AbstractSciMLOperator,u,t  Required for sparse-saving exponential integration Optional factorizations  ldiv   factorize  et al This is only required for algorithms which use the factorization of the operator Crank-Nicolson and only for when the default linear solve is used Note About Affine Operators Affine operators are operators which have the action  Q*x  A*x  b  These operators have no matrix representation since if there was it would be a linear operator instead of an affine operator You can only represent an affine operator as a linear operator in a dimension of one larger via the operation  A b  u;1  so it would require something modified to the input as well As such affine operators are a distinct generalization of linear operators While it this seems like it might doom the idea of using matrix-free affine operators it turns out that affine operators can be used in all cases where matrix-free linear solvers are used due to an easy genearlization of the standard convergence proofs If Q is the affine operator  Q(x  Ax  b  then solving  Qx  c  is equivalent to solving  Ax  b  c  or  Ax  c-b  If you know do this same plug-and-chug handling of the affine operator in into the GMRES/CG/etc convergence proofs move the affine part to the rhs residual and show it converges to solving  Ax  c-b  and thus GMRES/CG/etc solves  Q(x  c  for an affine operator properly That same trick then can be used pretty much anywhere you would've had a linear operator to extend the proof to affine operators so then  exp(A*t)*v  operations via Krylov methods work for A being affine as well and all sorts of things Thus affine operators have no matrix representation but they are still compatible with essentially any Krylov method which would otherwise be compatible with matrix-free representations hence their support in the SciMLOperators interface"},{"doctype":"documentation","id":"references/ModelingToolkit.vars!","title":"vars!","text":""},{"doctype":"documentation","id":"references/NeuralOperators.einsum","title":"einsum","text":""},{"doctype":"documentation","id":"references/Surrogates.select_evaluation_point_1D","title":"select_evaluation_point_1D","text":""},{"doctype":"documentation","id":"references/LabelledArrays","title":"LabelledArrays","text":""},{"doctype":"documentation","id":"references/MethodOfLines.edge","title":"edge","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.sparse_fit","title":"sparse_fit","text":""},{"doctype":"document","id":"ModelingToolkit/tutorials/nonlinear_optimal_control.md","title":"Nonlinear Optimal Control","text":"t x t v t u t p D Differential t loss x v u eqs D x v p x D v p u v sys loss eqs t x v u p dt tspan sys sys dt tspan u0 rand length sys prob sys u0 grad GalacticOptim Optim sol prob BFGS Nonlinear Optimal Control Note this is still a work in progress The  ControlSystem  type is an interesting system because unlike other system types it cannot be numerically solved on its own Instead it must be transformed into another system before solving Standard methods such as the direct method multiple shooting or discretize-then-optimize can all be phrased as symbolic transformations to a  ControlSystem  this is the strategy of this methodology Defining a Nonlinear Optimal Control Problem Here we will start by defining a classic optimal control problem Let x^{′′  u^3(t where we want to optimize our controller  u(t  such that the following is minimized L(\\theta  sum_i Vert 4  x(t_i Vert  2 Vert x^\\prime(t_i Vert  Vert u(t_i Vert where  i  is measured on 0,8 at 0.01 intervals To do this we rewrite the ODE in first order form begin{aligned}\nx^\\prime  v \nv^′  u^3(t \n\\end{aligned and thus L(\\theta  sum_i Vert 4  x(t_i Vert  2 Vert v(t_i Vert  Vert u(t_i Vert is our loss function on the first order system Defining such a control system is similar to an  ODESystem  except we must also specify a control variable  u(t  and a loss function Together this problem looks as follows Solving a Control Problem via Discretize-Then-Optimize One common way to solve nonlinear optimal control problems is by transforming them into an optimization problem by performing a Runge-Kutta discretization of the differential equation system and imposing equalities between variables in the same steps This can be done via the  runge_kutta_discretize  transformation on the  ControlSystem  While a tableau  tab  can be specified it defaults to a 5th order RadauIIA collocation which is a common method in the field To perform this discretization we simply need to give a  dt  and a timespan on which to discretize Now  sys  is an  OptimizationSystem  which when solved gives the values of  x(t   v(t  and  u(t  Thus we solve the  OptimizationSystem  using GalacticOptim.jl And this is missing some nice interfaces and ignores the equality constraints right now so the tutorial is not complete"},{"doctype":"documentation","id":"references/SciMLBase.__parameterless_type","title":"__parameterless_type","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.AtomicBC","title":"AtomicBC","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.is_pos_half","title":"is_pos_half","text":""},{"doctype":"documentation","id":"references/SciMLBase.has_tstop","title":"has_tstop","text":"Checks if integrator has any stopping times defined"},{"doctype":"documentation","id":"references/SciMLBase.interpolation","title":"interpolation","text":"DocStringExtensions.MethodSignatures Get the value at tval where the solution is known at the times t sorted with values u and derivatives ks"},{"doctype":"documentation","id":"references/ModelingToolkit.mergedefaults","title":"mergedefaults","text":""},{"doctype":"documentation","id":"references/PolyChaos.GammaOrthoPoly","title":"GammaOrthoPoly","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractNoiseProblem","title":"AbstractNoiseProblem","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.unassigned","title":"unassigned","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.SteadyStateAdjointProblem","title":"SteadyStateAdjointProblem","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.group_ranges","title":"group_ranges","text":"datasize groupsize Get ranges that partition data of length  datasize  in groups of  groupsize  observations If the data isn't perfectly dividable by  groupsize  the last group contains the reminding observations Arguments datasize  amount of data points to be partitioned groupsize  maximum amount of observations in each group Example"},{"doctype":"document","id":"RecursiveArrayTools/recursive_array_functions.md","title":"Recursive Array Functions","text":"Recursive Array Functions These are functions designed for recursive arrays like arrays of arrays and do not require that the RecursiveArrayTools types are used Function List"},{"doctype":"documentation","id":"references/ModelingToolkit.jacobian_dae_sparsity","title":"jacobian_dae_sparsity","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.swaprows!","title":"swaprows!","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timeseries_point_meancor","title":"timeseries_point_meancor","text":""},{"doctype":"documentation","id":"references/Catalyst.get_netstoich","title":"get_netstoich","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities._const","title":"_const","text":""},{"doctype":"documentation","id":"references/PolyChaos.r_scale","title":"r_scale","text":"Given the recursion coefficients  α,β  for a system of orthogonal polynomials that are orthogonal with respect to some positive weight  m(t  this function returns the recursion coefficients  α_,β  for the scaled measure  c m(t  for some positive  c "},{"doctype":"documentation","id":"references/SciMLBase.allows_arbitrary_number_types","title":"allows_arbitrary_number_types","text":"allows_arbitrary_number_types(alg::DEAlgorithm Trait declaration for whether an algorithm is compatible with direct automatic differentiation i.e can have algorithms like ForwardDiff or ReverseDiff attempt to differentiate directly through the solver Defaults to false as only pure-Julia algorithms can have this be true"},{"doctype":"document","id":"DiffEqSensitivity/ode_fitting/prediction_error_method.md","title":"Prediction error method (PEM)","text":"DifferentialEquations OptimizationOptimJL OptimizationPolyalgorithms Plots Statistics DataInterpolations tspan Float32 tsteps range tspan tspan length u0 simulator du u p t g L p Number p p gL g L θ u dθ u du dθ du gL sin θ prob simulator u0 tspan sol prob Tsit5 saveat tsteps abstol reltol y sol plot y title label simulate p _prob prob p p _prob Tsit5 saveat tsteps abstol reltol simloss p yh simulate p e2 yh e2 abs2 y yh mean e2 Ls simlosses simloss Ls fig_loss plot Ls simlosses title xlabel ylabel lab y_int y tsteps predictor du u p t g L K y p gL g L θ u dθ u yt y t e yt θ du dθ K e du gL sin θ predprob predictor u0 tspan nothing prediction p p_full p y_int _prob predprob p p_full _prob Tsit5 saveat tsteps abstol reltol predloss p yh prediction p e2 yh e2 abs2 y yh mean e2 predlosses map Ls L p L predloss p plot! Ls predlosses lab L0 adtype optf x p simloss x adtype optprob optf L0 ressim optprob PolyOpt maxiters ysim simulate ressim u plot tsteps y ysim label p0 optf2 p predloss p adtype optfunc2 optf2 p0 adtype nothing optprob2 optfunc2 p0 respred optprob2 PolyOpt maxiters ypred simulate respred u plot! tsteps ypred label respred u yn y randn Float32 y_int yn tsteps optf x p predloss x adtype optprob optf p0 resprednoise optprob PolyOpt maxiters yprednoise prediction resprednoise u plot! tsteps yprednoise label resprednoise u Prediction error method PEM When identifying linear systems from noisy data the prediction-error method   is close to a gold standard when it comes to the quality of the models it produces but is also one of the computationally more expensive methods due to its reliance on iterative gradient-based estimation When we are identifying nonlinear models we typically do not have the luxury of closed-form non-iterative solutions while PEM is easier to adopt to the nonlinear setting Fundamentally PEM changes the problem from minimizing a loss based on the simulation performance to minimizing a loss based on shorter-term predictions There are several benefits of doing so and this example will highlight two The loss is often easier to optimize In addition to an accurate simulator you also obtain a prediction for the system With PEM it's possible to estimate  disturbance models  The last point will not be illustrated in this tutorial but we will briefly expand upon it here Gaussian zero-mean measurement noise is usually not very hard to handle Disturbances that affect the state of the system may however cause all sorts of havoc on the estimate Consider wind affecting an aircraft deriving a statistical and dynamical model of the wind may be doable but unless you measure the exact wind affecting the aircraft making use of the model during parameter estimation is impossible The wind is an  unmeasured load disturbance  that affects the state of the system through its own dynamics model Using the techniques illustrated in this tutorial it's possible to estimate the influence of the wind during the experiment that generated the data and reduce or eliminate the bias it otherwise causes in the parameter estimates We will start by illustrating a common problem with simulation-error minimization Imagine a pendulum with unknown length that is to be estimated A small error in the pendulum length causes the frequency of oscillation to change Over sufficiently large horizon two sinusoidal signals with different frequencies become close to orthogonal to each other If some form of squared-error loss is used the loss landscape will be horribly non-convex in this case indeed we will illustrate exactly this below Another case that poses a problem for simulation-error estimation is when the system is unstable or chaotic A small error in either the initial condition or the parameters may cause the simulation error to diverge and its gradient to become meaningless In both of these examples we may make use of measurements we have of the evolution of the system to prevent the simulation error from diverging For instance if we have measured the angle of the pendulum we can make use of this measurement to adjust the angle during the simulation to make sure it stays close to the measured angle Instead of performing a pure simulation we instead say that we  predict  the state a while forward in time given all the measurements up until the current time point By minimizing this prediction rather than the pure simulation we can often prevent the model error from diverging even though we have a poor initial guess We start by defining a model of the pendulum The model takes a parameter  L  corresponding to the length of the pendulum We assume that the true length of the pendulum is  L  1  and generate some data from this system img1 We also define functions that simulate the system and calculate the loss given a parameter  p  corresponding to the length We now look at the loss landscape as a function of the pendulum length img2 This figure is interesting the loss is of course 0 for the true value  L=1  but for values  L  1  the overall slope actually points in the wrong direction Moreover the loss is oscillatory indicating that this is a terrible function to optimize and that we would need a very good initial guess for a local search to converge to the true value Note this example is chosen to be one-dimensional in order to allow these kinds of visualizations and one-dimensional problems are typically not hard to solve but the reasoning extends to higher-dimensional and harder problems We will now move on to defining a  predictor  model Our predictor will be very simple each time step we will calculate the error  e  between the simulated angle  theta  and the measured angle  y  A part of this error will be used to correct the state of the pendulum The correction we use is linear and looks like  Ke  K(y  theta  We have formed what is commonly referred to as a linear  observer  The  Kalman filter  is a particular kind of linear observer where  K  is calculated based on a statistical model of the disturbances that act on the system We will stay with a simple fixed-gain observer here for simplicity To feed the sampled data into the continuous-time simulation we make use of an interpolator We also define new functions  predictor  that contains the pendulum dynamics with the observer correction a  prediction  function that performs the rollout we're not using the word simulation to not confuse with the setting above and a loss function img3 Once gain we look at the loss as a function of the parameter and this time it looks a lot better The loss is not convex but the gradient points in the right direction over a much larger interval Here we arbitrarily set the observer gain to  K=1  we will later let the optimizer learn this parameter For completeness we also perform estimation using both losses We choose an initial guess we know will be hard for the simulation-error minimization just to drive home the point img4 The estimated parameters  L K  are Now we might ask ourselves why we used a correct on the form  Ke  and didn't instead set the angle in the simulation  equal  to the measurement The reason is twofold If our prediction of the angle is 100 based on the measurements the model parameters do not matter for the prediction and we can thus not hope to learn their values The measurement is usually noisy and we thus want to  fuse  the predictive power of the model with the information of the measurements The Kalman filter is an optimal approach to this information fusion under special circumstances linear model Gaussian noise We thus let the optimization  learn  the best value of the observer gain in order to make the best predictions As a last step we perform the estimation also with some measurement noise to verify that it does something reasonable img5 This example has illustrated basic use of the prediction-error method for parameter estimation In our example the measurement we had corresponded directly to one of the states and coming up with an observer/predictor that worked was not too hard For more difficult cases we may opt to use a nonlinear observer such as an extended Kalman filter EKF or design a Kalman filter based on a linearization of the system around some operating point As a last note there are several other methods available to improve the loss landscape and avoid local minima such as multiple-shooting The prediction-error method can easily be combined with most of those methods References Ljung Lennart System identification---Theory for the user Larsson Roger et al Direct prediction-error identification of unstable nonlinear systems applied to flight test data"},{"doctype":"document","id":"Surrogates/gek.md","title":"gek","text":"Gradient Enhanced Kriging Gradient-enhanced Kriging is an extension of kriging which supports gradient information GEK is usually more accurate than kriging however it is not computationally efficient when the number of inputs the number of sampling points or both are high This is mainly due to the size of the corresponding correlation matrix that increases proportionally with both the number of inputs and the number of sampling points Let's have a look to the following function to use Gradient Enhanced Surrogate  f(x  sin(x  2*x^2 First of all we will import  Surrogates  and  Plots  packages Sampling We choose to sample f in 8 points between 0 to 1 using the  sample  function The sampling points are chosen using a Sobol sequence this can be done by passing  SobolSample  to the  sample  function Building a surrogate With our sampled points we can build the Gradient Enhanced Kriging surrogate using the  GEK  function Gradient Enhanced Kriging Surrogate Tutorial ND First of all let's define the function we are going to build a surrogate for Now let's define the function Sampling Let's define our bounds this time we are working in two dimensions In particular we want our first dimension  x  to have bounds  0 10  and  0 10  for the second dimension We are taking 80 samples of the space using Sobol Sequences We then evaluate our function on all of the sampling points Building a surrogate Using the sampled points we build the surrogate the steps are analogous to the 1-dimensional case"},{"doctype":"documentation","id":"references/Catalyst.mm_names","title":"mm_names","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.find_candidates","title":"find_candidates","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis","title":"EnsembleAnalysis","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_torn_matching","title":"get_torn_matching","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.ODEForwardSensitivityFunction","title":"ODEForwardSensitivityFunction","text":"ODEForwardSensitivityFunction  DiffEqBase.AbstractODEFunction ODEForwardSensitivityFunction is an internal to the ODEForwardSensitivityProblem which extends the AbstractODEFunction to be used in an ODEProblem but defines the tools requires for calculating the extra differential equations associated with the derivative terms ODEForwardSensitivityFunction is not intended to be part of the public API"},{"doctype":"documentation","id":"references/DiffEqSensitivity.compute_Cinv!","title":"compute_Cinv!","text":""},{"doctype":"document","id":"CommonSolve/index.md","title":"CommonSolve.jl: The Common Solve Definition and Interface","text":"args kwargs args kwargs ProblemType args kwargs SolverType SolverType SolutionType AbstractVector AlgorithmType CommonSolve.jl The Common Solve Definition and Interface This holds the common  solve   init  and  solve  commands By using the same definition solver libraries from other completely different ecosystems can extend the functions and thus not clash with SciML if both ecosystems export the  solve  command The rules are that you must dispatch on one of your own types That's it No pirates General recommendation solve  function has the default definition So we recommend defining where  ProblemType   SolverType  and  SolutionType  are the types defined in your package To avoid method ambiguity the first argument of  solve   solve  and  init   must  be dispatched on the type defined in your package  For example do  not  define a method such as API Contributing Please refer to the  SciML ColPrac Contributor's Guide on Collaborative Practices for Community Packages  for guidance on PRs issues and other matters relating to contributing to SciML There are a few community forums The diffeq-bridged and sciml-bridged channels in the  Julia Slack JuliaDiffEq  on Gitter On the Julia Discourse forums look for the  modelingtoolkit tag See also  SciML Community page"},{"doctype":"documentation","id":"references/DiffEqSensitivity.NILSSSensitivityFunction","title":"NILSSSensitivityFunction","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.JumpType","title":"JumpType","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.arnoldi_step!","title":"arnoldi_step!","text":"Take the  j th step of the Lanczos iteration"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.sample_tail","title":"sample_tail","text":""},{"doctype":"documentation","id":"references/DiffEqOperators._slice_rmul!","title":"_slice_rmul!","text":""},{"doctype":"documentation","id":"references/SciMLBase.u_n","title":"u_n","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.split_assign","title":"split_assign","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.AbstractCondensationGraph","title":"AbstractCondensationGraph","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.expv!","title":"expv!","text":"Non-allocating version of  expv  that uses precomputed Krylov subspace  Ks  Alternative interface for calculating the action of  exp(t*A  on the vector  b  storing the result in  w  The Krylov iteration is terminated when an error estimate for the matrix exponential in the generated subspace is below the requested tolerance  Ks  is a  KrylovSubspace  and  typeof(cache)<:HermitianSubspaceCache  the exact type decides which algorithm is used to compute the subspace exponential"},{"doctype":"documentation","id":"references/DiffEqSensitivity.shadow_forward","title":"shadow_forward","text":""},{"doctype":"documentation","id":"references/PolyChaos.findUnivariateIndices","title":"findUnivariateIndices","text":"Given the multi-index  ind  this function returns all entries of the multivariate basis that correspond to the  i th univariate basis"},{"doctype":"documentation","id":"references/ModelingToolkit.has_loss","title":"has_loss","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.SystemStructures.isdiffvar","title":"isdiffvar","text":""},{"doctype":"documentation","id":"references/PolyChaos.Quad","title":"Quad","text":""},{"doctype":"documentation","id":"references/LinearSolve.ComposePreconditioner","title":"ComposePreconditioner","text":""},{"doctype":"documentation","id":"references/SciMLOperators.has_exp","title":"has_exp","text":""},{"doctype":"document","id":"LabelledArrays/NamedTuples_relation.md","title":"Relation to NamedTuples","text":"p σ ρ β Relation to NamedTuples Julia's Base has NamedTuples in v0.7 They are constructed as and they support  p[1  and  p.σ  as well The  LVector   SLVector   LArray  and  SLArray  constructors also support named tuples as their arguments Converting to a named tuple from a labelled array x is available using  convert(NamedTuple x  Furthermore  pairs(x  creates an iterator that is functionally the same as  pairs(convert(NamedTuple x  yielding  label  x.label  for each label of the array There are some crucial differences between a labelled array and a named tuple Labelled arrays can have any dimensions while named tuples are always 1D A named tuple can have different types on each element while an  SLArray  can only have one element type and furthermore it has the actions of a static vector As a result  SLArray  has less element type information which improves compilation speed while giving more vector functionality than a NamedTuple  LArray  also only has a single element type and unlike a named tuple is mutable"},{"doctype":"documentation","id":"references/SciMLBase.__has_vjp","title":"__has_vjp","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.HamiltonianNN","title":"HamiltonianNN","text":"model p nothing model p model Constructs a Hamiltonian Neural Network 1 This neural network is useful for learning symmetries and conservation laws by supervision on the gradients of the trajectories It takes as input a concatenated vector of length  2n  containing the position of size  n  and momentum of size  n  of the particles It then returns the time derivatives for position and momentum Note This doesn't solve the Hamiltonian Problem Use  NeuralHamiltonianDE  for such applications Note This layer currently doesn't support GPU The support will be added in future with some AD fixes To obtain the gradients to train this network ReverseDiff.gradient is supposed to be used This prevents the usage of  DiffEqFlux.sciml_train  or  Flux.train  Follow this  tutorial  to see how to define a training loop to circumvent this issue Arguments model  A Chain or FastChain neural network that returns the Hamiltonian of the system p  The initial parameters of the neural network References 1 Greydanus Samuel Misko Dzamba and Jason Yosinski Hamiltonian Neural Networks Advances in Neural Information Processing Systems 32 2019 15379-15389"},{"doctype":"documentation","id":"references/Surrogates.LobachevskySurrogate","title":"LobachevskySurrogate","text":"Lobachevsky interpolation suggested parameters 0  alpha  4 n must be even LobachevskySurrogate(x,y,alpha,n::Int,lb,ub,sparse  false Build the Lobachevsky surrogate with parameters alpha and n"},{"doctype":"documentation","id":"references/Catalyst.reactionparamsmap","title":"reactionparamsmap","text":"Given a  ReactionSystem  return a Dictionary mapping from parameters that appear within  Reaction s to their index within  reactionparams(network "},{"doctype":"documentation","id":"references/ModelingToolkit.ODEFunctionExpr","title":"ODEFunctionExpr","text":"Create a Julia expression for an  ODEFunction  from the  ODESystem  The arguments  dvs  and  ps  are used to set the order of the dependent variable and parameter vectors respectively"},{"doctype":"documentation","id":"references/NeuralPDE.NNRODE","title":"NNRODE","text":""},{"doctype":"documentation","id":"references/SciMLBase.solplot_vecs_and_labels","title":"solplot_vecs_and_labels","text":""},{"doctype":"documentation","id":"references/SciMLBase.split_callbacks","title":"split_callbacks","text":"Split comma seperated callbacks into sets of continous and discrete callbacks"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.construct_correlated_noisefunc","title":"construct_correlated_noisefunc","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractDDEProblem","title":"AbstractDDEProblem","text":"DocStringExtensions.TypeDefinition Base for types which define DDE problems"},{"doctype":"documentation","id":"references/DiffEqOperators.diff_axis","title":"diff_axis","text":""},{"doctype":"document","id":"Surrogates/tensor_prod.md","title":"Tensor product function","text":"Tensor product function The tensor product function is defined as  f(x  prod_{i=1}^d cos(a\\pi x_i Let's import Surrogates and Plots Define the 1D objective function Fitting and plotting different surrogates"},{"doctype":"documentation","id":"references/SciMLBase.AbstractIntegralAlgorithm","title":"AbstractIntegralAlgorithm","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/Catalyst.has_constraints","title":"has_constraints","text":""},{"doctype":"documentation","id":"references/SciMLBase.ODESolution","title":"ODESolution","text":"DocStringExtensions.TypeDefinition Representation of the solution to an ordinary differential equation defined by an ODEProblem DESolution Interface For more information on interacting with  DESolution  types check out the Solution Handling page of the DifferentialEquations.jl documentation https://diffeq.sciml.ai/stable/basics/solution Fields u  the representation of the ODE solution Given as an array of solutions where  u[i  corresponds to the solution at time  t[i  It is recommended in most cases one does not access  sol.u  directly and instead use the array interface described in the Solution Handling page of the DifferentialEquations.jl documentation t  the time points corresponding to the saved values of the ODE solution prob  the original ODEProblem that was solved alg  the algorithm type used by the solver destats  statistics of the solver such as the number of function evaluations required number of Jacobians computed and more retcode  the return code from the solver Used to determine whether the solver solved successfully  sol.retcode  Success  whether it terminated due to a user-defined callback  sol.retcode  Terminated  or whether it exited due to an error For more details see the return code section of the DifferentialEquations.jl documentation"},{"doctype":"documentation","id":"references/Optimization.AutoFiniteDiff","title":"AutoFiniteDiff","text":"f kwargs fdtype Val forward fdhtype Val hcentral AutoFiniteDiff  AbstractADType An AbstractADType choice for use in OptimizationFunction for automatically generating the unspecified derivative functions Usage This uses  FiniteDiff.jl  While to necessarily the most efficient in any case this is the only choice that doesn't require the  f  function to be automatically differentiable which means it applies to any choice However because it's using finite differencing one needs to be careful as this procedure introduces numerical error into the derivative estimates Compatible with GPUs Compatible with Hessian-based optimization Compatible with Hv-based optimization Not compatible with constraint functions Note that only the unspecified derivative functions are defined For example if a  hess  function is supplied to the  OptimizationFunction  then the Hessian is not defined via FiniteDiff Constructor fdtype  the method used for defining the gradient fdhtype  the method used for defining the Hessian For more information on the derivative type specifiers see the  FiniteDiff.jl documentation "},{"doctype":"documentation","id":"references/SciMLBase.deleteat_non_user_cache!","title":"deleteat_non_user_cache!","text":"deleteat s the non-user facing caches at indices  idxs  This includes resizing Jacobian caches Note In many cases  deleteat  simply  deleteat s  full_cache  variables and then calls this function This finer control is required for some  AbstractArray  operations"},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.CMONeighbors","title":"CMONeighbors","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractIncrementingODEProblem","title":"AbstractIncrementingODEProblem","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.connection_error","title":"connection_error","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.naivemul!","title":"naivemul!","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.AbstractBoxGeneration","title":"AbstractBoxGeneration","text":""},{"doctype":"documentation","id":"references/PolyChaos.Uniform_11Measure","title":"Uniform_11Measure","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.nice_parameter","title":"nice_parameter","text":""},{"doctype":"documentation","id":"references/NeuralPDE.QuasiRandomTraining","title":"QuasiRandomTraining","text":"points   the number of quasi-random points in a sample bcs_points  the number of quasi-random points in a sample for boundry conditions by default it equals  points  sampling_alg  the quasi-Monte Carlo sampling algorithm resampling  if it's false  the full training set is generated in advance before training and at each iteration one subset is randomly selected out of the batch if it's true  the training set isn't generated beforehand and one set of quasi-random points is generated directly at each iteration in runtime In this case  minibatch  has no effect minibatch  the number of subsets if resampling  false For more information look QuasiMonteCarlo.jl https://github.com/SciML/QuasiMonteCarlo.jl"},{"doctype":"documentation","id":"references/ModelingToolkit._named_idxs","title":"_named_idxs","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.exp_pade_q","title":"exp_pade_q","text":""},{"doctype":"documentation","id":"references/SciMLBase.SDDEProblem","title":"SDDEProblem","text":"Defines a stochastic delay differential equation SDDE problem Documentation Page https://diffeq.sciml.ai/stable/types/sdde_types Mathematical Specification of a Stochastic Delay Differential Equation SDDE Problem To define a SDDE Problem you simply need to give the drift function  f  the diffusion function  g  the initial condition  u_0  at time point  t_0  and the history function  h  which together define a SDDE du  f(u,h,p,t)dt  g(u,h,p,t)dW_t qquad t geq t_0 u(t_0  u_0 u(t  h(t qquad t  t_0 f  should be specified as  f(u h p t  or in-place as  f(du u h p t  and  g  should match  u_0  should be an AbstractArray or number whose geometry matches the desired geometry of  u  and  h  should be specified as described below The history function  h  is accessed for all delayed values Note that we are not limited to numbers or vectors for  u_0  one is allowed to provide  u_0  as arbitrary matrices  higher dimension tensors as well Note that this functionality should be considered experimental Functional Forms of the History Function The history function  h  can be called in the following ways h(p t  out-of-place calculation h(out p t  in-place calculation h(p t deriv::Type{Val{i  out-of-place calculation of the  i th derivative h(out p t deriv::Type{Val{i  in-place calculation of the  i th derivative h(args idxs  calculation of  h(args  for indices  idxs Note that a dispatch for the supplied history function of matching form is required for whichever function forms are used in the user derivative function  f  Declaring Lags Lags are declared separately from their use One can use any lag by simply using the interpolant of  h  at that point However one should use caution in order to achieve the best accuracy When lags are declared the solvers can more efficiently be more accurate and thus this is recommended Neutral Retarded and Algebraic Stochastic Delay Differential Equations Note that the history function specification can be used to specify general retarded arguments i.e  h(p,α(u,t  Neutral delay differential equations can be specified by using the  deriv  value in the history interpolation For example  h(p,t-τ Val{1  returns the first derivative of the history values at time  t-τ  Note that algebraic equations can be specified by using a singular mass matrix Problem Type Constructors Parameter  isinplace  optionally sets whether the function is inplace or not This is determined automatically but not inferred Parameters are optional and if not given then a  NullParameters  singleton will be used which will throw nice errors if you try to index non-existent parameters Any extra keyword arguments are passed on to the solvers For example if you set a  callback  in the problem then that  callback  will be added in every solve call For specifying Jacobians and mass matrices see the DiffEqFunctions  performance_overloads page Arguments f  The drift function in the SDDE g  The diffusion function in the SDDE u0  The initial condition Defaults to the value  h(p first(tspan  of the history function evaluated at the initial time point h  The history function for the DDE before  t0  tspan  The timespan for the problem p  The parameters with which function  f  is called Defaults to  NullParameters  constant_lags  A collection of constant lags used by the history function  h  Defaults to    dependent_lags  A tuple of functions  u p t  lag  for the state-dependent lags used by the history function  h  Defaults to    neutral  If the DDE is neutral i.e if delays appear in derivative terms order_discontinuity_t0  The order of the discontinuity at the initial time point Defaults to  0  if an initial condition  u0  is provided Otherwise it is forced to be greater or equal than  1  kwargs  The keyword arguments passed onto the solves"},{"doctype":"documentation","id":"references/ModelingToolkit.parameters","title":"parameters","text":"DocStringExtensions.TypedMethodSignatures Get the set of parameters variables for the given system"},{"doctype":"documentation","id":"references/SciMLBase.NO_COLOR","title":"NO_COLOR","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.has_tgrad","title":"has_tgrad","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.CosineKernel","title":"CosineKernel","text":""},{"doctype":"documentation","id":"references/SciMLOperators.FunctionOperator","title":"FunctionOperator","text":""},{"doctype":"documentation","id":"references/Catalyst.addreaction!","title":"addreaction!","text":"Add the passed in reaction to the  ReactionSystem  Returns the integer id of  rx  in the list of  Reaction s within  network  Notes Any new species or parameters used in  rx  should be separately added to  network  using  addspecies  and  addparam "},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.tearing","title":"tearing","text":"Tear the nonlinear equations in system When  simplify=true  we simplify the new residual residual equations after tearing End users are encouraged to call  structural_simplify  instead which calls this function internally"},{"doctype":"documentation","id":"references/NonlinearSolve.DefaultLinSolve","title":"DefaultLinSolve","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.define_params","title":"define_params","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timeseries_steps_median","title":"timeseries_steps_median","text":""},{"doctype":"documentation","id":"references/SciMLBase.set_u!","title":"set_u!","text":"Set current state of the  integrator  to  u "},{"doctype":"document","id":"LinearSolve/index.md","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"Pkg Pkg add LinearSolve.jl High-Performance Unified Linear Solvers LinearSolve.jl is a unified interface for the linear solving packages of Julia It interfaces with other packages of the Julia ecosystem to make it easy to test alternative solver packages and pass small types to control algorithm swapping It also interfaces with the  ModelingToolkit.jl  world of symbolic modeling to allow for automatically generating high-performance code Performance is key the current methods are made to be highly performant on scalar and statically sized small problems with options for large-scale systems If you run into any performance issues please file an issue Installation To install LinearSolve.jl use the Julia package manager Contributing Please refer to the  SciML ColPrac Contributor's Guide on Collaborative Practices for Community Packages  for guidance on PRs issues and other matters relating to contributing to ModelingToolkit There are a few community forums the diffeq-bridged channel in the  Julia Slack JuliaDiffEq  on Gitter on the  Julia Discourse forums see also  SciML Community page Roadmap Wrappers for every linear solver in the Julia language is on the roadmap If there are any important ones that are missing that you would like to see added please open an issue The current algorithms should support automatic differentiation Pre-defined preconditioners would be a welcome addition"},{"doctype":"documentation","id":"references/DiffEqSensitivity.QuadratureAdjoint","title":"QuadratureAdjoint","text":"QuadratureAdjoint  AbstractAdjointSensitivityAlgorithm An implementation of adjoint sensitivity analysis which develops a full continuous solution of the reverse solve in order to perform a post-ODE quadrature This method requires the the dense solution and will ignore saving arguments during the gradient calculation The tolerances in the constructor control the inner quadrature The inner quadrature uses a ReverseDiff vjp if autojacvec and  compile=false  by default but can compile the tape under the same circumstances as  ReverseDiffVJP  This method is O(n^3  p for stiff  implicit equations as opposed to the O((n+p)^3 scaling of BacksolveAdjoint and InterpolatingAdjoint and thus is much more compute efficient However it requires holding a dense reverse pass and is thus memory intensive Constructor Keyword Arguments autodiff  Use automatic differentiation for constructing the Jacobian if the Jacobian needs to be constructed  Defaults to  true  chunk_size  Chunk size for forward-mode differentiation if full Jacobians are built  autojacvec=false  and  autodiff=true  Default is  0  for automatic choice of chunk size diff_type  The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with  autodiff=false  autojacvec  Calculate the vector-Jacobian product  J'*v  via automatic differentiation with special seeding The default is  true  The total set of choices are false  the Jacobian is constructed via FiniteDiff.jl true  the Jacobian is constructed via ForwardDiff.jl TrackerVJP  Uses Tracker.jl for the vjp ZygoteVJP  Uses Zygote.jl for the vjp EnzymeVJP  Uses Enzyme.jl for the vjp ReverseDiffVJP(compile=false  Uses ReverseDiff.jl for the vjp  compile  is a boolean for whether to precompile the tape which should only be done if there are no branches  if  or  while  statements in the  f  function abstol  absolute tolerance for the quadrature calculation reltol  relative tolerance for the quadrature calculation compile  whether to compile the vjp calculation for the integrand calculation See  ReverseDiffVJP  for more details For more details on the vjp choices please consult the sensitivity algorithms documentation page or the docstrings of the vjp types SciMLProblem Support This  sensealg  only supports  ODEProblem s This  sensealg  supports events callbacks References Rackauckas C and Ma Y and Martensen J and Warner C and Zubov K and Supekar R and Skinner D and Ramadhana A and Edelman A Universal Differential Equations for Scientific Machine Learning,\tarXiv:2001.04385 Hindmarsh A C and Brown P N and Grant K E and Lee S L and Serban R and Shumaker D E and Woodward C S SUNDIALS Suite of nonlinear and differential/algebraic equation solvers ACM Transactions on Mathematical Software TOMS 31 pp:363–396 2005 Rackauckas C and Ma Y and Dixit V and Guo X and Innes M and Revels J and Nyberg J and Ivaturi V A comparison of automatic differentiation and continuous sensitivity analysis for derivatives of differential equation solutions arXiv:1812.01892 Kim S Ji W Deng S Ma Y  Rackauckas C 2021 Stiff neural ordinary differential equations Chaos An Interdisciplinary Journal of Nonlinear Science 31(9 093122"},{"doctype":"documentation","id":"references/ModelingToolkit.observed","title":"observed","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.RODEUJacobianWrapper","title":"RODEUJacobianWrapper","text":""},{"doctype":"documentation","id":"references/LinearSolve.purge_history!","title":"purge_history!","text":""},{"doctype":"documentation","id":"references/SciMLBase.has_adjoint","title":"has_adjoint","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.GeometricBrownianMotion","title":"GeometricBrownianMotion","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.collect_var!","title":"collect_var!","text":""},{"doctype":"documentation","id":"references/CommonSolve.solve!","title":"solve!","text":"iter iter prob ProblemType alg SolverType kwargs IterType iter SolutionType Solves an equation or other mathematical problem using the algorithm specified in the arguments Generally the interface is where the keyword arguments are uniform across all choices of algorithms The  iter  type will be different for the different problem types"},{"doctype":"documentation","id":"references/DiffEqSensitivity.ForwardDiffSensitivityParameterCompatibilityError","title":"ForwardDiffSensitivityParameterCompatibilityError","text":""},{"doctype":"documentation","id":"references/LabelledArrays.lazypair","title":"lazypair","text":""},{"doctype":"documentation","id":"references/MethodOfLines.DifferentialDiscretizer","title":"DifferentialDiscretizer","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.isdiffeq","title":"isdiffeq","text":""},{"doctype":"documentation","id":"references/GlobalSensitivity.gsa","title":"gsa","text":"The inputs for DGSM are as follows 1.f This is the input function based on which the values of DGSM are to be evaluated Eg f(x  x[1]+x[2]^2 This is function in 2 variables 2.samples Depicts the number of sampling set of points to be used for evaluation of E(a E(|a and E(a^2 a  partial derivative of f wrt x_i 3.distri Array of distribution of respective variables Eg dist  Normal(5,6),Uniform(2,3 for two variables 4.crossed A string(True/False which act as indicator for computation of DGSM crossed indices Eg a True value over there will lead to evauation of crossed indices"},{"doctype":"documentation","id":"references/DiffEqSensitivity.B!","title":"B!","text":""},{"doctype":"documentation","id":"references/PolyChaos.golubwelsch","title":"golubwelsch","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.states_to_sym","title":"states_to_sym","text":""},{"doctype":"documentation","id":"references/Catalyst.Graph","title":"Graph","text":"Converts a  ReactionSystem  into a Graphviz graph Reactions correspond to small green circles and species to blue circles Notes Black arrows from species to reactions indicate reactants and are labelled with their input stoichiometry Black arrows from reactions to species indicate products and are labelled with their output stoichiometry Red arrows from species to reactions indicate that species is used within the rate expression For example in the reaction  k*A B  C  there would be a red arrow from  A  to the reaction node In  k*A A+B  C  there would be red and black arrows from  A  to the reaction node Requires the Graphviz jll to be installed or Graphviz to be installed and commandline accessible"},{"doctype":"documentation","id":"references/ModelingToolkit.ConnectionElement","title":"ConnectionElement","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.screen_unit","title":"screen_unit","text":"Throw exception on invalid unit types otherwise return argument"},{"doctype":"document","id":"Catalyst/tutorials/using_catalyst.md","title":"Using Catalyst","text":"repressilator Repressilator P₃ α K n ∅ m₁ P₁ α K n ∅ m₂ P₂ α K n ∅ m₃ δ γ m₁ ∅ δ γ m₂ ∅ δ γ m₃ ∅ β m₁ m₁ P₁ β m₂ m₂ P₂ β m₃ m₃ P₃ μ P₁ ∅ μ P₂ ∅ μ P₃ ∅ α K n δ γ β μ repressilator repressilator repressilator latexify repressilator starred g repressilator P₃ α K n ∅ m₁ P₁ α K n ∅ m₂ P₂ α K n ∅ m₃ odesys convert repressilator pmap α K n δ log γ β log μ log u₀map m₁ m₂ m₃ P₁ P₂ P₃ α K n δ γ β μ t m₁ t m₂ t m₃ t P₁ t P₂ t P₃ t pmap α K n δ log γ β log μ log u₀map m₁ m₂ m₃ P₁ P₂ P₃ tspan oprob repressilator u₀map tspan pmap oprob2 odesys u₀map tspan pmap sol oprob Tsit5 saveat plot sol u₀map m₁ m₂ m₃ P₁ P₂ P₃ dprob repressilator u₀map tspan pmap jprob JumpProblem repressilator dprob Direct save_positions sol jprob SSAStepper saveat plot sol bdp c₁ X X c₂ X c₃ X c₁ c₂ c₃ p c₁ c₂ c₃ u₀ X tspan sprob bdp u₀ tspan p sol sprob LambaEM tstops range step length plot sol Using Catalyst In this tutorial we provide an introduction to using Catalyst to specify chemical reaction networks and then to solve ODE jump and SDE models generated from them At the end we show what mathematical rate laws and transition rate functions i.e intensities or propensities are generated by Catalyst for ODE SDE and jump process models Let's start by using the Catalyst  reaction_network  macro to specify a simple chemical reaction network the well-known repressilator We first import the basic packages we'll need We now construct the reaction network The basic types of arrows and predefined rate laws one can use are discussed in detail within the tutorial  The   Reaction DSL  Here we use a mix of first order zero order and repressive Hill function rate laws Note  varnothing  corresponds to the empty state and is used for zeroth order production and first order degradation reactions which gives showing that we've created a new network model named  Repressilator  with the listed chemical species and states  reaction_network  returns a  ReactionSystem  which we saved in the  repressilator  variable It can be converted to a variety of other mathematical models represented as  ModelingToolkit.AbstractSystem s or analyzed in various ways using the  Catalyst.jl API  For example to see the chemical species parameters and reactions we can use which gives which gives and which gives We can also use Latexify to see the corresponding reactions which shows what the  hillr  terms correspond to mathematically begin{align*}\n\\require{mhchem}\n\\ce varnothing frac{\\alpha K^{n}}{K^{n  P{_3}^{n m{_1}}\\\\\n\\ce varnothing frac{\\alpha K^{n}}{K^{n  P{_1}^{n m{_2}}\\\\\n\\ce varnothing frac{\\alpha K^{n}}{K^{n  P{_2}^{n m{_3}}\\\\\n\\ce m{_1 delta][\\gamma varnothing}\\\\\n\\ce m{_2 delta][\\gamma varnothing}\\\\\n\\ce m{_3 delta][\\gamma varnothing}\\\\\n\\ce m{_1 beta m{_1  P{_1}}\\\\\n\\ce m{_2 beta m{_2  P{_2}}\\\\\n\\ce m{_3 beta m{_3  P{_3}}\\\\\n\\ce P{_1 mu varnothing}\\\\\n\\ce P{_2 mu varnothing}\\\\\n\\ce P{_3 mu varnothing}\n\\end{align Assuming  Graphviz  is installed and commandline accessible within a Jupyter notebook we can also graph the reaction network by giving Repressilator solution The network graph shows a variety of information representing each species as a blue node and each reaction as an orange dot Black arrows from species to reactions indicate reactants and are labelled with their input stoichiometry Similarly black arrows from reactions to species indicate products and are labelled with their output stoichiometry In contrast red arrows from a species to reactions indicate the species is used within the reactions rate expressions For the repressilator the reactions have rates that depend on the proteins and hence lead to red arrows from each  Pᵢ  Note from the REPL or scripts one can always use  savegraph  to save the graph assuming  Graphviz  is installed Mass Action ODE Models Let's now use our  ReactionSystem  to generate and solve a corresponding mass action ODE model We first convert the system to a  ModelingToolkit.ODESystem  by We can once again use Latexify to look at the corresponding ODE model begin{aligned}\n\\frac{dm_1(t)}{dt  frac{\\alpha K^{n}}{K^{n  left mathrm{P_3}\\left t right right)^{n  delta mathrm{m_1}\\left t right  gamma \n\\frac{dm_2(t)}{dt  frac{\\alpha K^{n}}{K^{n  left mathrm{P_1}\\left t right right)^{n  delta mathrm{m_2}\\left t right  gamma \n\\frac{dm_3(t)}{dt  frac{\\alpha K^{n}}{K^{n  left mathrm{P_2}\\left t right right)^{n  delta mathrm{m_3}\\left t right  gamma \n\\frac{dP_1(t)}{dt  beta mathrm{m_1}\\left t right  mu mathrm{P_1}\\left t right \n\\frac{dP_2(t)}{dt  beta mathrm{m_2}\\left t right  mu mathrm{P_2}\\left t right \n\\frac{dP_3(t)}{dt  beta mathrm{m_3}\\left t right  mu mathrm{P_3}\\left t right)\n\\end{aligned Note there is currently a Latexify bug that causes different fonts to be used for the species symbols on each side of the equations Before we can solve the ODEs we need to specify the values of the parameters in the model the initial condition and the time interval to solve the model on To do this we need to build mappings from the symbolic parameters and the species to the corresponding numerical values for parameters and initial conditions We can build such mappings in several ways One is to use Julia  Symbols  to specify the values like Alternatively we can use ModelingToolkit symbolic variables to specify these mappings like Knowing these mappings we can set up the  ODEProblem  we want to solve Note by passing  repressilator  directly to the  ODEProblem  Catalyst has to internally call  convert(ODESystem repressilator  again to generate the symbolic ODEs We could instead pass  odesys  directly like oprob  and  oprob2  are functionally equivalent each representing the same underlying problem At this point we are all set to solve the ODEs We can now use any ODE solver from within the  DifferentialEquations.jl  package We'll use the recommended default explicit solver  Tsit5  and then plot the solutions Repressilator ODE Solutions We see the well-known oscillatory behavior of the repressilator For more on the choices of ODE solvers see the  DifferentialEquations.jl   documentation  Stochastic Simulation Algorithms SSAs for Stochastic Chemical Kinetics Let's now look at a stochastic chemical kinetics model of the repressilator modeling it with jump processes Here we will construct a  DiffEqJump   JumpProblem  that uses Gillespie's  Direct  method and then solve it to generate one realization of the jump process Repressilator SSA Solutions We see that oscillations remain but become much noisier Note in constructing the  JumpProblem  we could have used any of the SSAs that are part of DiffEqJump instead of the  Direct  method see the list of SSAs i.e constant rate jump aggregators in the  documentation  Common questions that arise in using the DiffEqJump SSAs i.e Gillespie methods are collated in the  DiffEqJump FAQ  Chemical Langevin Equation CLE Stochastic Differential Equation SDE Models At an intermediate physical scale between macroscopic ODE models and microscopic stochastic chemical kinetics models lies the CLE given by a system of SDEs that add to each ODE above a noise term As the repressilator has species that get very close to zero in size it is not a good candidate to model with the CLE where solutions can then go negative and become unphysical Let's create a simpler reaction network for a birth-death process that will stay non-negative The corresponding Chemical Langevin Equation SDE is then dX(t  left c_1 X\\left t right  c_2 X\\left t right  c_3 right dt  sqrt{c_1 X(t dW_1(t  sqrt{c_2 X(t dW_2(t  sqrt{c_3 dW_3(t where each  W_i(t  denotes an independent Brownian Motion We can solve the CLE model by creating an  SDEProblem  and solving it similarly to what we did for ODEs above CLE Solution We again have complete freedom to select any of the StochasticDiffEq.jl SDE solvers see the  documentation  Reaction rate laws used in simulations In generating mathematical models from a  ReactionSystem  reaction rates are treated as  microscopic  rates That is for a general mass action reaction of the form  n_1 S_1  n_2 S_2  dots n_M S_M to dots  with stoichiometric substrate coefficients  n_i\\}_{i=1}^M  and rate constant  k  the corresponding ODE and SDE rate laws are taken to be k prod_{i=1}^M frac{(S_i)^{n_i}}{n_i while the jump process transition rate i.e the propensity or intensity function is k prod_{i=1}^M frac{S_i S_i-1 dots S_i-n_i+1)}{n_i For example the rate law of the reaction  2X  3Y to Z  with rate constant  k  would be k frac{X^2}{2 frac{Y^3}{3  giving the ODE model begin{align*}\n\\frac{dX}{dt   2 k frac{X^2}{2 frac{Y^3}{3 \n\\frac{dY}{dt   3 k frac{X^2}{2 frac{Y^3}{3 \n\\frac{dZ}{dt  k frac{X^2}{2 frac{Y^3}{3!}.\n\\end{align This implicit rescaling of rate constants can be disabled through explicit conversion of a  ReactionSystem  to another system via  Base.convert  using the  combinatoric_ratelaws=false  keyword argument i.e For the previous example using this keyword argument would give the rate law k X^2 Y^3  and the ODE model begin{align*}\n\\frac{dX}{dt   2 k X^2 Y^3 \n\\frac{dY}{dt   3 k X^2 Y^3 \n\\frac{dZ}{dt  k X^2 Y^3.\n\\end{align Notes For each of the preceding models we converted the  ReactionSystem  to i.e ODEs jumps or SDEs we had two paths for conversion a Convert to the corresponding ModelingToolkit system and then use it in creating the corresponding problem b Directly create the desired problem type from the  ReactionSystem  The latter is more convenient however the former will be more efficient if one needs to repeatedly create the associated  Problem  ModelingToolkit offers many options for optimizing the generated ODEs and SDEs including options to build functions for evaluating Jacobians and/or multithreaded versions of derivative evaluation functions See the options for  ODEProblem s  and  SDEProblem s "},{"doctype":"documentation","id":"references/DiffEqSensitivity.NILSSProblem","title":"NILSSProblem","text":""},{"doctype":"documentation","id":"references/MethodOfLines.upwind_difference","title":"upwind_difference","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.VariableTunable","title":"VariableTunable","text":""},{"doctype":"documentation","id":"references/PolyChaos.rm_legendre01","title":"rm_legendre01","text":"Creates  N  recurrence coefficients for monic Legendre polynomials that are orthogonal on  0,1  relative to  w(t  1 "},{"doctype":"documentation","id":"references/SciMLBase.__init","title":"__init","text":""},{"doctype":"documentation","id":"references/Surrogates.EarthSurrogate","title":"EarthSurrogate","text":""},{"doctype":"documentation","id":"references/SciMLBase.symbolic_discretize","title":"symbolic_discretize","text":""},{"doctype":"document","id":"SciMLBase/interfaces/Problems.md","title":"SciMLProblems","text":"f u0 tspan p iip f u0 tspan p SciMLProblems The cornerstone of the SciML common interface is the problem type definition These definitions are the encoding of mathematical problems into a numerically computable form Note About Symbolics and ModelingToolkit The symbolic analog to the problem interface is the ModelingToolkit  AbstractSystem  For example  ODESystem  is the symbolic analog to  ODEProblem  Each of these system types have a method for constructing the associated problem and function types Definition of the SciMLProblem Interface The following standard principles should be adhered to across all  SciMLProblem  instantiations In-place Specification Each  SciMLProblem  type can be called with an is inplace iip choice For example which is a boolean for whether the function is in the inplace form mutating to change the first value This is automatically determined using the methods table but note that for full type-inferrability of the  SciMLProblem  this iip-ness should be specified Additionally the functions are fully specialized to reduce the runtimes If one would instead like to not specialize on the functions to reduce compile time then one can set  recompile  to false Default Parameters By default  SciMLProblem  types use the  SciMLBase.NullParameters  singleton to define the absence of parameters by default The reason is because this throws an informative error if the parameter is used or accessed within the user's function for example  p[1  will throw an informative error about forgetting to pass parameters Keyword Argument Splatting All  SciMLProblem  types allow for passing keyword arguments that would get forwarded to the solver The reason for this is that in many cases like in  EnsembleProblem  usage a  SciMLProblem  might be associated with some solver configuration such as a callback or tolerance Thus for flexibility the extra keyword arguments to the  SciMLProblem  are carried to the solver problem_type SciMLProblem  types include a non-public API definition of  problem_type  which holds a trait type corresponding to the way the  SciMLProblem  was constructed For example if a  SecondOrderODEProblem  constructor is used the returned problem is simply a  ODEProblem  for interopability with any  ODEProblem  algorithm However in this case the  problem_type  will be populated with the  SecondOrderODEProblem  type indicating the original definition and extra structure Remake Problem Traits SciMLProblem API Abstract SciMLProblems Concrete SciMLProblems"},{"doctype":"documentation","id":"references/Catalyst.make_hillar_exp","title":"make_hillar_exp","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.inplace_sensitivity","title":"inplace_sensitivity","text":""},{"doctype":"documentation","id":"references/NeuralOperators.MarkovNeuralOperator","title":"MarkovNeuralOperator","text":"Markov neural operator learns a neural operator with Fourier operators With only one time step information of learning it can predict the following few steps with low loss by linking the operators into a Markov chain"},{"doctype":"documentation","id":"references/ModelingToolkit.find_duplicates","title":"find_duplicates","text":"DocStringExtensions.MethodSignatures find duplicates in an iterable object"},{"doctype":"document","id":"Surrogates/sphere_function.md","title":"Sphere function","text":"Sphere function The sphere function of dimension d is defined as  f(x  sum_{i=1}^d x_i  with lower bound 10 and upper bound 10 Let's import Surrogates and Plots Define the objective function The 1D case is just a simple parabola let's plot it Fitting RadialSurrogate with different radial basis Fitting Lobachevsky Surrogate with different values of hyperparameters alpha"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.pCN","title":"pCN","text":"Create a new but correlated noise process from  noise  and additional entropy with correlation ρ Create a new but correlated noise process from  noise  and additional entropy with correlation ρ This update defines an autoregressive process in the space of Wiener or noise process trajectories which can be used as proposal distribution in Metropolis-Hastings algorithms often called preconditioned Crank–Nicolson scheme External links Preconditioned Crank–Nicolson algorithm on Wikipedia"},{"doctype":"documentation","id":"references/PolyChaos.rm_chebyshev1","title":"rm_chebyshev1","text":""},{"doctype":"documentation","id":"references/Optimization.instantiate_function","title":"instantiate_function","text":"instantiate_function(f x AbstractADType p num_cons  0)::OptimizationFunction This function is used internally by Optimization.jl to construct the necessary extra functions gradients Hessians etc before optimization Each of the ADType dispatches use the supplied automatic differentiation type in order to specify how the construction process occurs If no ADType is given then the default  NoAD  dispatch simply defines closures on any supplied gradient function to enclose the parameters to match the interfaces for the specific optimization libraries i.e G,x)->f.grad(G,x,p If a function is not given and the  NoAD  dispatch is used or if the AD dispatch is currently not capable of defining said derivative then the constructed  OptimizationFunction  will simply use  nothing  to specify and undefined function The return of  instantiate_function  is an  OptimizationFunction  which is then used in the optimization process If an optimizer requires a function that is not defined an error is thrown For more information on the use of automatic differentiation see the documentation of the  AbstractADType  types"},{"doctype":"document","id":"Catalyst/index.md","title":"Catalyst.jl for Reaction Models","text":"rn α S I I β I R α β rn OrdinaryDiffEq p α β tspan u0 S I R op rn u0 tspan p sol op Tsit5 Plots plot sol lw Catalyst.jl for Reaction Models Catalyst.jl is a symbolic modeling package for analysis and high performance simulation of chemical reaction networks Catalyst defines symbolic  ReactionSystem s which can be created programmatically or easily specified using Catalyst's domain specific language DSL Leveraging  ModelingToolkit  and  Symbolics.jl  Catalyst enables large-scale simulations through auto-vectorization and parallelism Symbolic  ReactionSystem s can be used to generate ModelingToolkit-based models allowing the easy simulation and parameter estimation of mass action ODE models Chemical Langevin SDE models stochastic chemical kinetics jump process models and more Generated models can be used with solvers throughout the broader  SciML  ecosystem including higher level SciML packages e.g for sensitivity analysis parameter estimation machine learning applications etc Features DSL provides a simple and readable format for manually specifying chemical reactions Catalyst  ReactionSystem s provide a symbolic representation of reaction networks built on  ModelingToolkit.jl  and  Symbolics.jl  Non-integer e.g  Float64  stoichiometric coefficients are supported for generating ODE models and symbolic expressions for stoichiometric coefficients are supported for all system types The  Catalyst.jl API  provides functionality for extending networks building networks programmatically network analysis and for composing multiple networks together ReactionSystem s generated by the DSL can be converted to a variety of  ModelingToolkit.AbstractSystem s including symbolic ODE SDE and jump process representations Conservation laws can be detected and applied to reduce system sizes and generate non-singular Jacobians during conversion to ODEs SDEs and steady-state equations By leveraging ModelingToolkit users have a variety of options for generating optimized system representations to use in solvers These include construction of dense or sparse Jacobians multithreading or parallelization of generated derivative functions automatic classification of reactions into optimized jump types for Gillespie type simulations automatic construction of dependency graphs for jump systems and more Generated systems can be solved using any  DifferentialEquations.jl  ODE/SDE/jump solver and can be used within  EnsembleProblem s for carrying out parallelized parameter sweeps and statistical sampling Plot recipes are available for visualizing the solutions Julia  Expr s can be obtained for all rate laws and functions determining the deterministic and stochastic terms within resulting ODE SDE or jump models Latexify  can be used to generate LaTeX expressions corresponding to generated mathematical models or the underlying set of reactions Graphviz  can be used to generate and visualize reaction network graphs Reusing the Graphviz interface created in  Catlab.jl  Catalyst  ReactionSystem s can be imported from SBML files via  SBMLToolkit.jl  and from BioNetGen net files and various matrix network representations using  ReactionNetworkImporters.jl  Installation Catalyst can be installed through the Julia package manager Illustrative Example Here is a simple example of generating visualizing and solving an SIR ODE model We first define the SIR reaction model using Catalyst Assuming  Graphviz  and is installed and  command line   accessible  the network can be visualized using the  Graph  command which in Jupyter notebooks will give the figure SIR Network Graph To generate and solve a mass action ODE version of the model we use which we can plot as SIR Solution Getting Help Catalyst developers are active on the  Julia   Discourse  and the  Julia   Slack  channels  sciml-bridged and  sciml-sysbio For bugs or feature requests  open an   issue "},{"doctype":"documentation","id":"references/SciMLBase.SensitivityInterpolation","title":"SensitivityInterpolation","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/LinearSolve.AbstractFactorization","title":"AbstractFactorization","text":""},{"doctype":"document","id":"Surrogates/abstractgps.md","title":"Gaussian Process Surrogate Tutorial","text":"Gaussian Process Surrogate Tutorial Note This surrogate requires the SurrogatesAbstractGPs module which can be added by inputting add SurrogatesAbstractGPs from the Julia command line Gaussian Process regression in Surrogates.jl is implemented as a simple wrapper around the  AbstractGPs.jl  package AbstractGPs comes with a variety of covariance functions kernels See  KernelFunctions.jl  for examples Tip The examples below demonstrate the use of AbstractGPs with out-of-the-box settings without hyperparameter optimization i.e without changing parameters like lengthscale signal variance and noise variance Beyond hyperparameter optimization careful initialization of hyperparameters and priors on the parameters is required for this surrogate to work properly For more details on how to fit GPs in practice check out  A Practical Guide to Gaussian Processes  Also see this  example  to understand hyperparameter optimization with AbstractGPs 1D Example In the example below the gp_surrogate assignment code can be commented  uncommented to see how the different kernels influence the predictions Optimization Example This example shows the use of AbstractGP Surrogates to find the minima of a function Plotting the function and the sampled points ND Example Now let's see how our surrogate performs And this is our log marginal posterior predictive probability"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.Tail3","title":"Tail3","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.build_explicit_observed_function","title":"build_explicit_observed_function","text":"DocStringExtensions.MethodSignatures Build the observed function assuming the observed equations are all explicit i.e there are no cycles"},{"doctype":"documentation","id":"references/RecursiveArrayTools.Chain","title":"Chain","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.init_basis_matrix!","title":"init_basis_matrix!","text":""},{"doctype":"documentation","id":"references/SciMLBase.NonlinearSolution","title":"NonlinearSolution","text":"DocStringExtensions.TypeDefinition Representation of the solution to an nonlinear equation defined by an NonlinearProblem or the steady state solution to a differential equation defined by a SteadyStateProblem Fields u  the representation of the nonlinear equation's solution resid  the residual of the solution prob  the original NonlinearProblem/SteadyStateProblem that was solved alg  the algorithm type used by the solver original  if the solver is wrapped from an alternative solver ecosystem such as NLsolve.jl then this is the original return from said solver library retcode  the return code from the solver Used to determine whether the solver solved successfully  sol.retcode  Success  whether it terminated due to a user-defined callback  sol.retcode  Terminated  or whether it exited due to an error For more details see the return code section of the DifferentialEquations.jl documentation left  if the solver is bracketing method this is the final left bracket value right  if the solver is bracketing method this is the final right bracket value"},{"doctype":"documentation","id":"references/PolyChaos.mcdiscretization","title":"mcdiscretization","text":"This routine returns  N  recurrence coefficients of the polynomials that are orthogonal relative to a weight function  w  that is decomposed as a sum of  m  weights  w_i  with domains  a_i,b_i  for  i=1,\\dots,m  w(t  sum_{i}^{m w_i(t quad text{with  operatorname{dom}(w_i  a_i b_i For each weight  w_i  and its domain  a_i b_i  the function  mcdiscretization  expects a quadrature rule of the form nodes::AbstractVector weights::AbstractVector  my_quad_i(N::Int all of which are stacked in the parameter  quad  quad   my_quad_1  my_quad_m  If the weight function has a discrete part specified by  discretemeasure  it is added on to the discretized continuous weight function The function  mcdiscretization  performs a sequence of discretizations of the given weight  w(t  each discretization being followed by an application of the Stieltjes or Lanczos procedure keyword  discretization in stieltjes lanczos  to produce approximations to the desired recurrence coefficients The function applies to each subinterval  i  an  N point quadrature rule the  i th entry of  quad  to discretize the weight function  w_i  on that subinterval If the procedure converges to within a prescribed accuracy  ε  before  N  reaches its maximum allowed value  Nmax  If the function does not converge the function prompts an error message The keyword  gaussquad  should be set to  true  if Gauss quadrature rules are available  for all   m  weights  w_i(t  with  i  1 dots m  For further information please see W Gautschi Orthogonal Polynomials Approximation and Computation Section 2.2.4"},{"doctype":"documentation","id":"references/MethodOfLines.split_terms","title":"split_terms","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.Substitutions","title":"Substitutions","text":""},{"doctype":"documentation","id":"references/Catalyst.any_nonrx_subsys","title":"any_nonrx_subsys","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.ODAEProblem","title":"ODAEProblem","text":"This constructor acts similar to the one for  ODEProblem  with the following changes  ODESystem s can sometimes be further reduced if  structural_simplify  has already been applied to them In these cases the constructor uses the knowledge of the strongly connected components calculated during the process of simplification as the basis for building pre-simplified nonlinear systems in the implicit solving In summary these problems are structurally modified but could be more efficient and more stable Note the returned object is still of type  ODEProblem "},{"doctype":"documentation","id":"references/SciMLBase.VectorContinuousCallback","title":"VectorContinuousCallback","text":"condition affect! affect_neg! len initialize finalize idxs nothing rootfind save_positions interp_points abstol eps reltol repeat_nudge condition affect! len initialize finalize idxs nothing rootfind save_positions affect_neg! affect! interp_points abstol eps reltol repeat_nudge This is also a subtype of  AbstractContinuousCallback   CallbackSet  is not feasible when you have a large number of callbacks as it doesn't scale well For this reason we have  VectorContinuousCallback   it allows you to have a single callback for multiple events Arguments condition  This is a function  condition(out u t integrator  which should save the condition value in the array  out  at the right index Maximum index of  out  should be specified in the  len  property of callback So this way you can have a chain of  len  events which would cause the  i th event to trigger when  out[i  0  affect  This is a function  affect!(integrator event_index  which lets you modify  integrator  and it tells you about which event occured using  event_idx  i.e gives you index  i  for which  out[i  came out to be zero len  Number of callbacks chained This is compulsory to be specified Rest of the arguments have the same meaning as in  ContinuousCallback "},{"doctype":"documentation","id":"references/ModelingToolkit.modified_states!","title":"modified_states!","text":""},{"doctype":"document","id":"MethodOfLines/tutorials/icbc_sampled.md","title":"Initial and Boundary Conditions with sampled/measured Data","text":"Interpolations A_x A log10 x x A_x itp interpolate A BSpline Cubic Line OnGrid sitp1 scale itp A_x sitp1 sitp1 Interpolations A_x1 A_x2 f x1 x2 log10 x1 x2 A f x1 x2 x1 A_x1 x2 A_x2 itp interpolate A BSpline Cubic Line OnGrid sitp2 scale itp A_x1 A_x2 sitp2 sitp2 sitp1_f x sitp1 x sitp2_f x y sitp2 x y sitp1_f y sitp2_f x y Initial and Boundary Conditions with sampled/measured Data Initial and boundary conditions are sometimes applied with measured data that is itself pre-discretized In order to use such data it is recommended to leverage  Interpolations.jl  or  DataInterpolations.jl  for better dealing with possibly noisy data currently limited to 1D To create a callable effectively continuous function for example from the  Interpolations.jl   docs  1D Multidimensional Then register the functions with ModelingToolkit Then as a BC or IC Note that the measured data need not be measured on the same grid as will be generated for the discretization in  MethodOfLines.jl  as long as it is defined upon the whole simulation domain it will be automatically re-sampled If you are using an  edge_align  grid  molfd your interpolation will need to be defined  ±dx/2   above and below the edges of the simulation domain where  dx  is the step size in the direction of that edge  Extrapolation  may prove useful here"},{"doctype":"documentation","id":"references/MethodOfLines.AbstractBoundary","title":"AbstractBoundary","text":""},{"doctype":"documentation","id":"references/LinearSolve.IS_OPENBLAS","title":"IS_OPENBLAS","text":""},{"doctype":"documentation","id":"references/NeuralOperators.c_glorot_uniform","title":"c_glorot_uniform","text":""},{"doctype":"document","id":"LabelledArrays/LArrays.md","title":"LArrays","text":"LArrays LArrays  are fully mutable arrays with labels There is no performance loss by using labelled indexing instead of purely numerical indexing Using the macro with values and labels generates the labelled array with the given values Users interested in using labelled elements in their arrays should also consider  ComponentArrays  from the  ComponentArrays.jl  library  ComponentArrays  are well integrated into the SciML ecosystem LArray  and  LVector  macros Macro constructors are convenient for building most  LArray  objects An  LArray  may be of arbitrary dimension while an  LVector  is a one dimensional array LArray  and  LVector  constructors The original constructors for  LArray s and  LVector s are as follows Manipulating  LArrays  and  LVectors User may want a list of the labels or keys in an  LArray  or  LVector  The  symbols(::LArray  function returns a tuple of array labels"},{"doctype":"document","id":"Optimization/optimization_packages/optim.md","title":"[Optim.jl]( optim)","text":"Pkg Pkg add rosenbrock x p p x p x x cons x p x x x0 zeros p prob rosenbrock cons cons prob prob x0 p lcons ucons sol prob IPNewton rosenbrock x p x x x x0 zeros p prob rosenbrock x0 p sol prob Optim NelderMead rosenbrock x p x x x x0 zeros p optprob rosenbrock prob optprob x0 p lb ub sol prob NLopt LD_LBFGS rosenbrock x p x x x x0 zeros p f rosenbrock x0 p grad hess prob f x0 p sol prob Optim Newton rosenbrock x p x x x cons x p x x x0 zeros p optprob rosenbrock cons cons prob optprob x0 p sol prob Optim KrylovTrustRegion rosenbrock x p p x p x x x0 zeros p f rosenbrock prob f x0 p lb ub sol prob Optim ParticleSwarm lower prob lb upper prob ub n_particles rosenbrock x p x x x x0 zeros p f rosenbrock prob f x0 p lb ub sol prob Optim SAMIN Optim.jl  optim Optim  is Julia package implementing various algorithm to perform univariate and multivariate optimization Installation OptimizationOptimJL.jl To use this package install the OptimizationOptimJL package Methods Optim.jl  algorithms can be one of the following Optim.NelderMead Optim.SimulatedAnnealing Optim.ParticleSwarm Optim.ConjugateGradient Optim.GradientDescent Optim.BFGS Optim.LBFGS Optim.NGMRES Optim.OACCEL Optim.NewtonTrustRegion Optim.Newton Optim.KrylovTrustRegion Optim.ParticleSwarm Optim.SAMIN Each optimizer also takes special arguments which are outlined in the sections below The following special keyword arguments which are not covered by the common  solve  arguments can be used with Optim.jl optimizers x_tol  Absolute tolerance in changes of the input vector  x  in infinity norm Defaults to  0.0  g_tol  Absolute tolerance in the gradient in infinity norm Defaults to  1e-8  For gradient free methods this will control the main convergence tolerance which is solver specific f_calls_limit  A soft upper limit on the number of objective calls Defaults to  0  unlimited g_calls_limit  A soft upper limit on the number of gradient calls Defaults to  0  unlimited h_calls_limit  A soft upper limit on the number of Hessian calls Defaults to  0  unlimited allow_f_increases  Allow steps that increase the objective value Defaults to  false  Note that when setting this to  true  the last iterate will be returned as the minimizer even if the objective increased store_trace  Should a trace of the optimization algorithm's state be stored Defaults to  false  show_trace  Should a trace of the optimization algorithm's state be shown on  stdout  Defaults to  false  extended_trace  Save additional information Solver dependent Defaults to  false  trace_simplex  Include the full simplex in the trace for  NelderMead  Defaults to  false  show_every  Trace output is printed every  show_every th iteration For a more extensive documentation of all the algorithms and options please consult the  Documentation Local Optimizer Local Constraint Optim.jl  implements the following local constraint algorithms Optim.IPNewton linesearch  specifies the line search algorithm for more information consult  this source  and  this example  available line search algorithms HaegerZhang MoreThuente BackTracking StrongWolfe Static μ0  specifies the initial barrier penalty coefficient as either a number or  auto show_linesearch  is an option to turn on linesearch verbosity Defaults linesearch::Function  Optim.backtrack_constrained_grad μ0::Union{Symbol,Number  auto show_linesearch::Bool  false The Rosenbrock function can optimized using the  Optim.IPNewton  as follows Derivative-Free Derivative-free optimizers are optimizers that can be used even in cases where no derivatives or automatic differentiation is specified While they tend to be less efficient than derivative-based optimizers they can be easily applied to cases where defining derivatives is difficult Note that while these methods do not support general constraints all support bounds constraints via  lb  and  ub  in the  Optimization.OptimizationProblem  Optim.jl  implements the following derivative-free algorithms Optim.NelderMead   Nelder-Mead optimizer solve(problem NelderMead(parameters initial_simplex parameters  AdaptiveParameters  or  parameters  FixedParameters initial_simplex  AffineSimplexer Defaults parameters  AdaptiveParameters initial_simplex  AffineSimplexer Optim.SimulatedAnnealing   Simulated Annealing solve(problem SimulatedAnnealing(neighbor T p neighbor  is a mutating function of the current and proposed  x T  is a function of the current iteration that returns a temperature p  is a function of the current temperature Defaults neighbor  default_neighbor T  default_temperature p  kirkpatrick Optim.ParticleSwarm The Rosenbrock function can optimized using the  Optim.NelderMead  as follows Gradient-Based Gradient-based optimizers are optimizers which utilise the gradient information based on derivatives defined or automatic differentiation Optim.jl  implements the following gradient-based algorithms Optim.ConjugateGradient   Conjugate Gradient Descent solve(problem ConjugateGradient(alphaguess linesearch eta P precondprep alphaguess  computes the initial step length for more information consult  this source  and  this example  available initial step length procedures InitialPrevious InitialStatic InitialHagerZhang InitialQuadratic InitialConstantChange linesearch  specifies the line search algorithm for more information consult  this source  and  this example  available line search algorithms HaegerZhang MoreThuente BackTracking StrongWolfe Static eta  determines the next step direction P  is an optional preconditioner for more information see  this source  precondpred  is used to update  P  as the state variable  x  changes Defaults alphaguess  LineSearches.InitialHagerZhang linesearch  LineSearches.HagerZhang eta  0.4 P  nothing precondprep  P x  nothing Optim.GradientDescent   Gradient Descent a quasi-Newton solver solve(problem GradientDescent(alphaguess linesearch P precondprep alphaguess  computes the initial step length for more information consult  this source  and  this example  available initial step length procedures InitialPrevious InitialStatic InitialHagerZhang InitialQuadratic InitialConstantChange linesearch  specifies the line search algorithm for more information consult  this source  and  this example  available line search algorithms HaegerZhang MoreThuente BackTracking StrongWolfe Static P  is an optional preconditioner for more information see  this source  precondpred  is used to update  P  as the state variable  x  changes Defaults alphaguess  LineSearches.InitialPrevious linesearch  LineSearches.HagerZhang P  nothing precondprep  P x  nothing Optim.BFGS   Broyden-Fletcher-Goldfarb-Shanno algorithm solve(problem BFGS(alpaguess linesearch initial_invH initial_stepnorm manifold alphaguess  computes the initial step length for more information consult  this source  and  this example  available initial step length procedures InitialPrevious InitialStatic InitialHagerZhang InitialQuadratic InitialConstantChange linesearch  specifies the line search algorithm for more information consult  this source  and  this example  available line search algorithms HaegerZhang MoreThuente BackTracking StrongWolfe Static initial_invH  specifies an optional initial matrix initial_stepnorm  determines that  initial_invH  is an identity matrix scaled by the value of  initial_stepnorm  multiplied by the sup-norm of the gradient at the initial point manifold  specifies a Riemannian manifold on which the function is to be minimized for more information consult  this source  available manifolds Flat Sphere Stiefel meta-manifolds PowerManifold ProductManifold custom manifolds Defaults alphaguess  LineSearches.InitialStatic linesearch  LineSearches.HagerZhang initial_invH  nothing initial_stepnorm  nothing manifold  Flat Optim.LBFGS   Limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm m  is the number of history points alphaguess  computes the initial step length for more information consult  this source  and  this example  available initial step length procedures InitialPrevious InitialStatic InitialHagerZhang InitialQuadratic InitialConstantChange linesearch  specifies the line search algorithm for more information consult  this source  and  this example  available line search algorithms HaegerZhang MoreThuente BackTracking StrongWolfe Static P  is an optional preconditioner for more information see  this source  precondpred  is used to update  P  as the state variable  x  changes manifold  specifies a Riemannian manifold on which the function is to be minimized for more information consult  this source  available manifolds Flat Sphere Stiefel meta-manifolds PowerManifold ProductManifold custom manifolds scaleinvH0  whether to scale the initial Hessian approximation Defaults m  10 alphaguess  LineSearches.InitialStatic linesearch  LineSearches.HagerZhang P  nothing precondprep  P x  nothing manifold  Flat scaleinvH0::Bool  true  typeof(P  Nothing Optim.NGMRES Optim.OACCEL The Rosenbrock function can optimized using the  Optim.LD_LBFGS  as follows Hessian-Based Second Order Hessian-based optimization methods are second order optimization methods which use the direct computation of the Hessian These can converge faster but require fast and accurate methods for calulating the Hessian in order to be appropriate Optim.jl  implements the following hessian-based algorithms Optim.NewtonTrustRegion   Newton Trust Region method initial_delta  The starting trust region radius delta_hat  The largest allowable trust region radius eta  When rho is at least eta accept the step rho_lower  When rho is less than rho_lower shrink the trust region rho_upper  When rho is greater than rho_upper grow the trust region though no greater than delta_hat Defaults initial_delta  1.0 delta_hat  100.0 eta  0.1 rho_lower  0.25 rho_upper  0.75 Optim.Newton   Newton's method with line search alphaguess  computes the initial step length for more information consult  this source  and  this example  available initial step length procedures InitialPrevious InitialStatic InitialHagerZhang InitialQuadratic InitialConstantChange linesearch  specifies the line search algorithm for more information consult  this source  and  this example  available line search algorithms HaegerZhang MoreThuente BackTracking StrongWolfe Static Defaults alphaguess  LineSearches.InitialStatic linesearch  LineSearches.HagerZhang The Rosenbrock function can optimized using the  Optim.Newton  as follows Hessian-Free Second Order Hessian-free methods are methods which perform second order optimization by direct computation of Hessian-vector products  Hv  without requiring the construction of the full Hessian As such these methods can perform well for large second order optimization problems but can require special case when considering conditioning of the Hessian Optim.jl  implements the following hessian-free algorithms Optim.KrylovTrustRegion   A Newton-Krylov method with Trust Regions initial_delta  The starting trust region radius delta_hat  The largest allowable trust region radius eta  When rho is at least eta accept the step rho_lower  When rho is less than rho_lower shrink the trust region rho_upper  When rho is greater than rho_upper grow the trust region though no greater than delta_hat Defaults initial_delta  1.0 delta_hat  100.0 eta  0.1 rho_lower  0.25 rho_upper  0.75 The Rosenbrock function can optimized using the  Optim.KrylovTrustRegion  as follows Global Optimizer Without Constraint Equations The following method in  Optim  is performing global optimization on problems without constraint equations It works both with and without lower and upper constraints set by  lb  and  ub  in the  Optimization.OptimizationProblem  Optim.ParticleSwarm   Particle Swarm Optimization solve(problem ParticleSwarm(lower upper n_particles lower  upper  are vectors of lower/upper bounds respectively n_particles  is the number of particles in the swarm defaults to  lower     upper     n_particles  0 The Rosenbrock function can optimized using the  Optim.ParticleSwarm  as follows With Constraint Equations The following method in  Optim  is performing global optimization on problems with constraint equations Optim.SAMIN   Simulated Annealing with bounds solve(problem SAMIN(nt ns rt neps f_tol x_tol coverage_ok verbosity Defaults nt  5 ns  5 rt  0.9 neps  5 f_tol  1e-12 x_tol  1e-6 coverage_ok  false verbosity  0 The Rosenbrock function can optimized using the  Optim.SAMIN  as follows"},{"doctype":"documentation","id":"references/DiffEqSensitivity.extract_local_sensitivities","title":"extract_local_sensitivities","text":"sol asmatrix Val Val sol i Integer asmatrix Val Val sol t Union Number AbstractVector asmatrix Val Val extract_local_sensitivities Extracts the time series for the local sensitivities from the ODE solution This requires that the ODE was defined via  ODEForwardSensitivityProblem "},{"doctype":"documentation","id":"references/MethodOfLines.idx","title":"idx","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.has_tearing_state","title":"has_tearing_state","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.linear_interpolation_wedges","title":"linear_interpolation_wedges","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.isinput","title":"isinput","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.NILSS","title":"NILSS","text":"nseg nstep nus nothing rng Xorshifts Xoroshiro128Plus rand UInt64 chunk_size autodiff Val central autojacvec autodiff g nothing struct NILSS  AbstractShadowingSensitivityAlgorithm An implementation of the forward-mode continuous  non-intrusive least squares shadowing  method  NILSS  allows for computing sensitivities of long-time averaged quantities with respect to the parameters of an  ODEProblem  by constraining the computation to the unstable subspace  NILSS  employs the continuous-time  ForwardSensitivity  method as tangent solver To avoid an exponential blow-up of the homogenous and inhomogenous tangent solutions the trajectory should be divided into sufficiently small segments where the tangent solutions are rescaled on the interfaces The computational and memory cost of NILSS scale with the number of unstable positive Lyapunov exponents instead of the number of states as in the LSS method  NILSS  avoids the explicit construction of the Jacobian at each time step and thus should generally be preferred for large system sizes over  ForwardLSS  Constructor Arguments nseg  Number of segments on full time interval on the attractor nstep  number of steps on each segment Keyword Arguments nus  Dimension of the unstable subspace Default is  nothing   nus  must be smaller or equal to the state dimension  length(u0  With the default choice  nus  length(u0  1  will be set at compile time rng  Pseudo random number generator Used for initializing the homogenous tangent states  w  Default is  Xorshifts.Xoroshiro128Plus(rand(UInt64  autodiff  Use automatic differentiation in the internal sensitivity algorithm computations Default is  true  chunk_size  Chunk size for forward mode differentiation if full Jacobians are built  autojacvec=false  and  autodiff=true  Default is  0  for automatic choice of chunk size autojacvec  Calculate the Jacobian-vector product via automatic differentiation with special seeding diff_type  The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with  autodiff=false  g  instantaneous objective function of the long-time averaged objective SciMLProblem Support This  sensealg  only supports  ODEProblem s This  sensealg  does not support events callbacks This  sensealg  assumes that the objective is a long-time averaged quantity and ergodic i.e the time evolution of the system behaves qualitatively the same over infinite time independent of the specified initial conditions such that only the sensitivity with respect to the parameters is of interest References Ni A Blonigan P J Chater M Wang Q Zhang Z Sensitivity analy sis on chaotic dynamical system by Non-Intrusive Least Square Shadowing NI-LSS in 46th AIAA Fluid Dynamics Conference AIAA AVIATION Forum AIAA 2016-4399 American Institute of Aeronautics and Astronautics 1–16 2016 Ni A and Wang Q Sensitivity analysis on chaotic dynamical systems by Non-Intrusive Least Squares Shadowing NILSS Journal of Computational Physics 347 56-77 2017"},{"doctype":"documentation","id":"references/Catalyst.ismassaction","title":"ismassaction","text":"rx rs rxvars rx rate haveivdep any isequal rs rxvars stateset Set rs True if a given reaction is of mass action form i.e  rx.rate  does not depend on any chemical species that correspond to states of the system and does not depend explicitly on the independent variable usually time Arguments rx  the  Reaction  rs  a  ReactionSystem  containing the reaction Optional  rxvars   Variable s which are not in  rxvars  are ignored as possible dependencies Optional  haveivdep   true  if the  Reaction   rate  field explicitly depends on the independent variable Optional  stateset  set of states which if the rxvars are within mean rx is non-mass action Notes Non-integer stoichiometry is treated as non-mass action This includes symbolic variables/terms or floating point numbers for stoichiometric coefficients"},{"doctype":"documentation","id":"references/SciMLOperators.AbstractSciMLOperator","title":"AbstractSciMLOperator","text":"Function call and multiplication  L(du u p t for in-place operator evaluation  du  L(u p t for out-of-place operator evaluation If the operator is not a constant update it with u,p,t A mutating form i.e update_coefficients!(A,u,p,t that changes the internal coefficients and a out-of-place form B  update_coefficients(A,u,p,t"},{"doctype":"documentation","id":"references/PolyChaos.rm_logistic","title":"rm_logistic","text":"Creates  N  recurrence coefficients for monic polynomials that are orthogonal on  infty,\\infty  relative to  w(t  frac{\\mathrm{e}^{-t}}{(1  mathrm{e}^{-t})^2"},{"doctype":"documentation","id":"references/ModelingToolkit.setdefault","title":"setdefault","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.getH","title":"getH","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleDistributed","title":"EnsembleDistributed","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/Surrogates.add_point!","title":"add_point!","text":"Add new samples x and y and update the coefficients Return the new object radial Adds the new point and its respective value to the sample points Warning If you are just adding a single point you have to wrap it with  Returns the updated Kriging model add_point!(varfid::VariableFidelitySurrogate,x_new,y_new I expect to add low fidelity data to the surrogate"},{"doctype":"documentation","id":"references/ModelingToolkit.swap!","title":"swap!","text":""},{"doctype":"documentation","id":"references/RecursiveArrayTools.combine_styles","title":"combine_styles","text":""},{"doctype":"documentation","id":"references/RecursiveArrayTools.recursivecopy!","title":"recursivecopy!","text":"b AbstractArray T N a AbstractArray T N A recursive  copy  function Acts like a  deepcopy  on arrays of arrays but like  copy  on arrays of scalars"},{"doctype":"documentation","id":"references/Surrogates.surrogate_optimize","title":"surrogate_optimize","text":"The main idea is to pick the new evaluations from a set of candidate points where each candidate point is generated as an N(0 sigma^2 distributed perturbation from the current best solution The value of sigma is modified based on progress and follows the same logic as in many trust region methods we increase sigma if we make a lot of progress the surrogate is accurate and decrease sigma when we aren’t able to make progress the surrogate model is inaccurate More details about how sigma is updated is given in the original papers After generating the candidate points we predict their objective function value and compute the minimum distance to the previously evaluated point Let the candidate points be denoted by C and let the function value predictions be s(x i and the distance values be d(x i both rescaled through a linear transformation to the interval 0,1 This is done to put the values on the same scale The next point selected for evaluation is the candidate point x that minimizes the weighted-distance merit function merit(x  ws(x  1-w)(1-d(x where  0 leq w leq 1  That is we want a small function value prediction and a large minimum distance from the previously evaluated points The weight w is commonly cycled between a few values to achieve both exploitation and exploration When w is close to zero we do pure exploration while w close to 1 corresponds to exploitation SRBF 1D surrogate_optimize(obj::Function,::SRBF,lb::Number,ub::Number,surr::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100 This is an implementation of Lower Confidence Bound LCB a popular acquisition function in Bayesian optimization Under a Gaussian process GP prior the goal is to minimize  LCB(x  E[x  k  sqrt{(V[x  default value  k  2  This is an implementation of Lower Confidence Bound LCB a popular acquisition function in Bayesian optimization Under a Gaussian process GP prior the goal is to minimize LCB(x  E[x  k  sqrt{(V[x default value  k  2  Expected improvement method 1D This is an implementation of Expected Improvement EI arguably the most popular acquisition function in Bayesian optimization Under a Gaussian process GP prior the goal is to maximize expected improvement EI(x  E[max(f_{best}-f(x),0 surrogate_optimize(obj::Function,::DYCORS,lb::Number,ub::Number,surr1::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100 DYCORS optimization method in 1D following closely Combining radial basis function surrogates and dynamic coordinate search in high-dimensional expensive black-box optimization This is an implementation of the DYCORS strategy by Regis and Shoemaker Rommel G Regis and Christine A Shoemaker Combining radial basis function surrogates and dynamic coordinate search in high-dimensional expensive black-box optimization Engineering Optimization 45(5 529–555 2013 This is an extension of the SRBF strategy that changes how the candidate points are generated The main idea is that many objective functions depend only on a few directions so it may be advantageous to perturb only a few directions In particular we use a perturbation probability to perturb a given coordinate and decrease this probability after each function evaluation so fewer coordinates are perturbed later in the optimization surrogate_optimize(obj::Function,::SOP,lb::Number,ub::Number,surr::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100 SOP Surrogate optimization method following closely the following papers Suggested number of new_samples  min(500*d,5000"},{"doctype":"documentation","id":"references/LinearSolve.LinearSolveFunction","title":"LinearSolveFunction","text":""},{"doctype":"documentation","id":"references/LinearSolve.set_p","title":"set_p","text":"DocStringExtensions.MethodSignatures"},{"doctype":"document","id":"GlobalSensitivity/methods/morris.md","title":"Morris Method","text":"p_steps Array Int relative_scale Bool num_trajectory Int total_num_trajectory Int len_design_mat Int ishi X A B sin X A sin X B X sin X lb ones π ub ones π m ishi num_trajectory lb i ub i i Morris Method Morris  has the following keyword arguments p_steps   Vector of  Delta  for the step sizes in each direction Required relative_scale   The elementary effects are calculated with the assumption that the parameters lie in the range  0,1  but as this is not always the case scaling is used to get more informative scaled effects Defaults to  false  total_num_trajectory   num_trajectory   The total number of design matrices that are generated out of which  num_trajectory  matrices with the highest spread are used in calculation len_design_mat   The size of a design matrix Morris Method Details The Morris method also known as Morris’s OAT method where OAT stands for One At a Time can be described in the following steps We calculate local sensitivity measures known as “elementary effects” which are calculated by measuring the perturbation in the output of the model on changing one parameter EE_i  frac{f(x_1,x_2,..x_i Delta,..x_k  y}{\\Delta These are evaluated at various points in the input chosen such that a wide “spread” of the parameter space is explored and considered in the analysis to provide an approximate global importance measure The mean and variance of these elementary effects is computed A high value of the mean implies that a parameter is important a high variance implies that its effects are non-linear or the result of interactions with other inputs This method does not evaluate separately the contribution from the interaction and the contribution of the parameters individually and gives the effects for each parameter which takes into consideration all the interactions and its individual contribution API function gsa(f method::Morris p_range::AbstractVector batch=false kwargs Example Morris method on Ishigami function"},{"doctype":"document","id":"SciMLOperators/index.md","title":"SciMLOperators.jl: The SciML Operators Interface","text":"SciMLOperators.jl The SciML Operators Interface Many functions from linear solvers to differential equations require the use of matrix-free operators in order to achieve maximum performance in many scenarios SciMLOperators.jl defines the abstract interface for how operators in the SciML ecosystem are supposed to be defined It gives the common set of functions and traits which solvers can rely on for properly performing their tasks Along with that SciMLOperators.jl provides definitions for the basic standard operators which are used in building blocks for most tasks both simplifying the use of operators while also demonstrating to users how such operators can be built and used in practice Why SciMLOperators SciMLOperators.jl has the design that is required in order to be used in all scenarios of equation solvers For example Magnus integrators for differential equations require defining an operator  u  A(t)u  while Munthe-Kaas methods require defining operators of the form  u  A(u)u  Thus the operators need some form of time and state dependence which the solvers can update and query when they are non-constant  update_coefficients  Additionally the operators need the ability to act like normal functions for equation solvers For example if  A(u,p,t  has the same operation as  update_coefficients(A,u,p,t A*u  then  A  can be used in any place where a differential equation definition  f(u,p,t  is used without requring the user or solver to do any extra work Thus while previous good efforts for matrix-free operators have existed in the Julia ecosystem such as  LinearMaps.jl  those operator interfaces lack these aspects in order to actually be fully seamless with downstream equation solvers This necessitates the definition and use of an extended operator interface with all of these properties hence the  AbstractSciMLOperator  interface Contributing Please refer to the  SciML ColPrac Contributor's Guide on Collaborative Practices for Community Packages  for guidance on PRs issues and other matters relating to contributing to SciML There are a few community forums The diffeq-bridged and sciml-bridged channels in the  Julia Slack JuliaDiffEq  on Gitter On the Julia Discourse forums look for the  modelingtoolkit tag See also  SciML Community page"},{"doctype":"documentation","id":"references/SciMLBase.AbstractSecondOrderODEIntegrator","title":"AbstractSecondOrderODEIntegrator","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/ModelingToolkit.namespace_controls","title":"namespace_controls","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.convolve_BC_right!","title":"convolve_BC_right!","text":""},{"doctype":"documentation","id":"references/NeuralOperators.low_pass","title":"low_pass","text":""},{"doctype":"documentation","id":"references/Surrogates.merit_function","title":"merit_function","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.simplify_trigs","title":"simplify_trigs","text":""},{"doctype":"documentation","id":"references/ModelingToolkit._defvar","title":"_defvar","text":""},{"doctype":"documentation","id":"references/Surrogates.RandomForestStructure","title":"RandomForestStructure","text":""},{"doctype":"documentation","id":"references/SciMLBase.SDEProblem","title":"SDEProblem","text":"DiffEqProblemLibrary SDEProblemLibrary SDEProblemLibrary importsdeproblems prob SDEProblemLibrary prob_sde_linear sol prob Defines an stochastic differential equation SDE problem Documentation Page https://diffeq.sciml.ai/stable/types/sde_types Mathematical Specification of a SDE Problem To define an SDE Problem you simply need to give the forcing function  f  the noise function  g  and the initial condition  u₀  which define an SDE du  f(u,p,t)dt  Σgᵢ(u,p,t)dWⁱ f  and  g  should be specified as  f(u,p,t  and   g(u,p,t  respectively and  u₀  should be an AbstractArray whose geometry matches the desired geometry of  u  Note that we are not limited to numbers or vectors for  u₀  one is allowed to provide  u₀  as arbitrary matrices  higher dimension tensors as well A vector of  g s can also be defined to determine an SDE of higher Ito dimension Problem Type Wraps the data which defines an SDE problem u  f(u,p,t)dt  Σgᵢ(u,p,t)dWⁱ with initial condition  u0  Constructors SDEProblem(f::SDEFunction,g,u0,tspan,p=NullParameters();noise=WHITE_NOISE,noise_rate_prototype=nothing SDEProblem{isinplace}(f,g,u0,tspan,p=NullParameters();noise=WHITE_NOISE,noise_rate_prototype=nothing   Defines the SDE with the specified functions The default noise is  WHITE_NOISE   isinplace  optionally sets whether the function is inplace or not This is determined automatically but not inferred Parameters are optional and if not given then a  NullParameters  singleton will be used which will throw nice errors if you try to index non-existent parameters Any extra keyword arguments are passed on to the solvers For example if you set a  callback  in the problem then that  callback  will be added in every solve call For specifying Jacobians and mass matrices see the DiffEqFunctions  performance_overloads page Fields f  The drift function in the SDE g  The noise function in the SDE u0  The initial condition tspan  The timespan for the problem p  The optional parameters for the problem Defaults to  NullParameters  noise  The noise process applied to the noise upon generation Defaults to Gaussian white noise For information on defining different noise processes see the noise process documentation page  noise_process noise_rate_prototype  A prototype type instance for the noise rates that is the output  g  It can be any type which overloads  A_mul_B  with itself being the middle argument Commonly this is a matrix or sparse matrix If this is not given it defaults to  nothing  which means the problem should be interpreted as having diagonal noise kwargs  The keyword arguments passed onto the solves Example Problems Examples problems can be found in  DiffEqProblemLibrary.jl  To use a sample problem such as  prob_sde_linear  you can do something like"},{"doctype":"documentation","id":"references/Optimization.@withprogress","title":"@withprogress","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.isnoise","title":"isnoise","text":""},{"doctype":"document","id":"ModelingToolkit/mtkitize_tutorials/sparse_jacobians.md","title":"Automated Sparse Analytical Jacobians","text":"BenchmarkTools prob save_everystep sparseprob save_everystep sparsepatternprob sys Pair sparse sparsepatternprob save_everystep Automated Sparse Analytical Jacobians In many cases where you have large stiff differential equations getting a sparse Jacobian can be essential for performance In this tutorial we will show how to use  modelingtoolkitize  to regenerate an  ODEProblem  code with the analytical solution to the sparse Jacobian along with the sparsity pattern required by DifferentialEquations.jl's solvers to specialize the solving process First let's start out with an implementation of the 2-dimensional Brusselator partial differential equation discretized using finite differences Now let's use  modelingtoolkitize  to generate the symbolic version Now we regenerate the problem using  jac=true  for the analytical Jacobian and  sparse=true  to make it sparse Hard No How much did that help Notice though that the analytical solution to the Jacobian can be quite expensive Thus in some cases we may only want to get the sparsity pattern In this case we can simply do"},{"doctype":"document","id":"NeuralPDE/pinn/ks.md","title":"Kuramoto–Sivashinsky equation","text":"Flux GalacticOptimJL Interval infimum supremum x t u Dt Differential t Dx Differential x Dx2 Differential x Dx3 Differential x Dx4 Differential x α β γ eq Dt u x t u x t Dx u x t α Dx2 u x t β Dx3 u x t γ Dx4 u x t u_analytic x t z x t tanh z tanh z tanh z du x t z x t tanh z tanh z sech z bcs u x u_analytic x u t u_analytic t u t u_analytic t Dx u t du t Dx u t du t domains x Interval t Interval dx dt Flux σ Flux σ discretization dx dt pde_system eq bcs domains x t u x t prob pde_system discretization callback p l println l opt GalacticOptimJL BFGS res prob opt callback callback maxiters discretization Plots xs ts infimum d domain dx supremum d domain d dx zip domains dx dt u_predict first x t res minimizer x xs t ts u_real u_analytic x t x xs t ts diff_u abs u_analytic x t first x t res minimizer x xs t ts p1 plot xs u_predict title p2 plot xs u_real title p3 plot xs diff_u title plot p1 p2 p3 Kuramoto–Sivashinsky equation Let's consider the Kuramoto–Sivashinsky equation which contains a 4th-order derivative ∂_t u(x t  u(x t ∂_x u(x t  alpha ∂^2_x u(x t  beta ∂^3_x u(x t  gamma ∂^4_x u(x t   0   where  alpha  gamma  1  and  beta  4  The exact solution is u_e(x t  11  15 tanh theta  15 tanh^2 theta  15 tanh^3 theta   where  theta  1  x/2  and with initial and boundary conditions begin{align*}\n    u  x 0      u_e  x 0  \n    u 10 t      u_e 10 t  \n    u(-10 t      u_e(-10 t  \n∂_x u 10 t  ∂_x u_e 10 t  \n∂_x u(-10 t  ∂_x u_e(-10 t  \n\\end{align We use physics-informed neural networks And some analysis plotks"},{"doctype":"documentation","id":"references/DiffEqFlux.collocate_data","title":"collocate_data","text":"u′ u data tpoints u′ u data tpoints tpoints_sample interp args Computes a non-parametrically smoothed estimate of  u  and  u  given the  data  where each column is a snapshot of the timeseries at  tpoints[i  For kernels the following exist EpanechnikovKernel UniformKernel TriangularKernel QuarticKernel TriweightKernel TricubeKernel GaussianKernel CosineKernel LogisticKernel SigmoidKernel SilvermanKernel https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2631937 Additionally we can use interpolation methods from  DataInterpolations.jl  to generate data from intermediate timesteps In this case pass any of the methods like  QuadraticInterpolation  as  interp  and the timestamps to sample from as  tpoints_sample "},{"doctype":"documentation","id":"references/DiffEqOperators","title":"DiffEqOperators","text":""},{"doctype":"documentation","id":"references/SciMLBase.LinearSolution","title":"LinearSolution","text":"DocStringExtensions.TypeDefinition Representation of the solution to an linear system Ax=b defined by a LinearProblem Fields u  the representation of the optimization's solution resid  the residual of the solver if the method is an iterative method alg  the algorithm type used by the solver iters  the number of iterations used to solve the equation if the method is an iterative method retcode  the return code from the solver Used to determine whether the solver solved successfully  sol.retcode  Success  whether it terminated due to a user-defined callback  sol.retcode  Terminated  or whether it exited due to an error For more details see the return code section of the DifferentialEquations.jl documentation cache  the  LinearCache  object containing the solver's internal cached variables This is given to allow continuation of solver usage for example solving  Ax=b  with the same  A  and a new  b  without refactorizing  A  See the caching interface tutorial for details on how to use the  cache  effectively http://linearsolve.sciml.ai/dev/tutorials/caching_interface"},{"doctype":"documentation","id":"references/SciMLBase.DEProblem","title":"DEProblem","text":"DocStringExtensions.TypeDefinition Base type for all DifferentialEquations.jl problems Concrete subtypes of  DEProblem  contain the necessary information to fully define a differential equation of the corresponding type"},{"doctype":"documentation","id":"references/DiffEqSensitivity.ODELocalSensitivityProblem","title":"ODELocalSensitivityProblem","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.gbm_bridge!","title":"gbm_bridge!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.generate_rate_function","title":"generate_rate_function","text":""},{"doctype":"document","id":"DiffEqSensitivity/neural_ode/mnist_conv_neural_ode.md","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"DifferentialEquations Printf Flux Losses logitcrossentropy Flux Data DataLoader MLDatasets MLDataUtils LabelEnc convertlabel stratifiedobs CUDA CUDA allowscalar loadmnist batchsize bs train_split onehot labels_raw convertlabel LabelEnc OneOfK labels_raw LabelEnc NativeLabels collect imgs labels_raw MNIST traindata x_data Float32 reshape imgs size imgs size imgs size imgs y_data onehot labels_raw x_train y_train x_test y_test stratifiedobs x_data y_data p train_split DataLoader gpu collect x_train y_train batchsize batchsize shuffle DataLoader gpu collect x_test y_test batchsize batchsize shuffle bs train_split train_dataloader test_dataloader loadmnist bs train_split down Conv relu stride GroupNorm Conv relu stride pad GroupNorm Conv stride pad gpu dudt Conv tanh stride pad Conv tanh stride pad gpu fc GroupNorm x relu x MeanPool x reshape x Dense gpu nn_ode dudt Tsit5 save_everystep reltol abstol save_start gpu DiffEqArray_to_Array x xarr gpu x xarr model down nn_ode DiffEqArray_to_Array fc img lab train_dataloader data train_dataloader data x_d down img x_m model img classify x argmax eachcol x accuracy model data n_batches total_correct total i x y enumerate data i n_batches target_class classify cpu y predicted_class classify cpu model x total_correct sum target_class predicted_class total length target_class total_correct total accuracy model train_dataloader loss x y logitcrossentropy model x y loss img lab opt ADAM iter cb iter iter train_accuracy accuracy model train_dataloader test_accuracy accuracy model test_dataloader n_batches length test_dataloader iter train_accuracy test_accuracy Flux train! loss Flux down nn_ode p fc train_dataloader opt cb cb DifferentialEquations Printf Flux Losses logitcrossentropy Flux Data DataLoader MLDatasets MLDataUtils LabelEnc convertlabel stratifiedobs CUDA CUDA allowscalar loadmnist batchsize bs train_split onehot labels_raw convertlabel LabelEnc OneOfK labels_raw LabelEnc NativeLabels collect imgs labels_raw MNIST traindata x_data Float32 reshape imgs size imgs size imgs size imgs y_data onehot labels_raw x_train y_train x_test y_test stratifiedobs x_data y_data p train_split DataLoader gpu collect x_train y_train batchsize batchsize shuffle DataLoader gpu collect x_test y_test batchsize batchsize shuffle bs train_split train_dataloader test_dataloader loadmnist bs train_split down Conv relu stride GroupNorm Conv relu stride pad GroupNorm Conv stride pad gpu dudt Conv tanh stride pad Conv tanh stride pad gpu fc GroupNorm x relu x MeanPool x reshape x Dense gpu nn_ode dudt Tsit5 save_everystep reltol abstol save_start gpu DiffEqArray_to_Array x xarr gpu x xarr model down nn_ode DiffEqArray_to_Array fc img lab train_dataloader data train_dataloader data x_d down img x_m model img m_no_ode down nn fc gpu x_m m_no_ode img classify x argmax eachcol x accuracy model data n_batches total_correct total i x y enumerate data i n_batches target_class classify cpu y predicted_class classify cpu model x total_correct sum target_class predicted_class total length target_class total_correct total accuracy model train_dataloader loss x y logitcrossentropy model x y loss img lab opt ADAM cb iter iter train_accuracy accuracy model train_dataloader test_accuracy accuracy model test_dataloader n_batches length test_dataloader iter train_accuracy test_accuracy Flux train! loss Flux down nn_ode p fc train_dataloader opt cb cb Convolutional Neural ODE MNIST Classifier on GPU Training a Convolutional Neural Net Classifier for  MNIST  using a neural ordinary differential equation  NN-ODE  on  GPUs  with  Minibatching  Step-by-step description below Step-by-Step Description Load Packages GPU A good trick used here Ensures that only optimized kernels are called when using the GPU Additionally the  gpu  function is shown as a way to translate models and data over to the GPU Note that this function is CPU-safe so if the GPU is disabled or unavailable this code will fallback to the CPU Load MNIST Dataset into Minibatches The preprocessing is done in  loadmnist  where the raw MNIST data is split into features  x_train  and labels  y_train  by specifying batchsize  bs  The function  convertlabel  will then transform the current labels  labels_raw  from numbers 0 to 9  LabelEnc.NativeLabels(collect(0:9  into one hot encoding  LabelEnc.OneOfK  Features are reshaped into format  Height Width Color BatchSize  or in this case  28 28 1 128  meaning that every minibatch will contain 128 images with a single color channel of 28x28 pixels The entire dataset of 60,000 images is split into the train and test dataset ensuring a balanced ratio of labels These splits are then passed to Flux's DataLoader This automatically minibatches both the images and labels Additionally it allows us to shuffle the train dataset in each epoch while keeping the order of the test data the same and then loaded from main Layers The Neural Network requires passing inputs sequentially through multiple layers We use  Chain  which allows inputs to functions to come from previous layer and sends the outputs to the next Four different sets of layers are used here down  This layer downsamples our images into  6 x 6 x 64  dimensional features It takes a 28 x 28 image and passes it through a convolutional neural network layer with  relu  activation nn  A 2 layer Convolutional Neural Network Chain with  tanh  activation which is used to model our differential equation nn_ode  ODE solver layer fc  The final fully connected layer which maps our learned features to the probability of the feature vector of belonging to a particular class gpu  A utility function which transfers our model to GPU if one is available Array Conversion When using  NeuralODE  we can use the following function as a cheap conversion of  DiffEqArray  from the ODE solver into a Matrix that can be used in the following layer For CPU If this function does not automatically fallback to CPU when no GPU is present we can change  gpu(x  with  Array(x  Build Topology Next we connect all layers together in a single chain There are a few things we can do to examine the inner workings of our neural network This can also be built without the NN-ODE by replacing  nn-ode  with a simple  nn  Prediction To convert the classification back into readable numbers we use  classify  which returns the prediction by taking the arg max of the output for each column of the minibatch Accuracy We then evaluate the accuracy on  n_batches  at a time through the entire network Training Parameters Once we have our model we can train our neural network by backpropagation using  Flux.train  This function requires  Loss   Optimizer  and  Callback  functions Loss Cross Entropy  is the loss function computed here which applies a  Softmax  operation on the final output of our model  logitcrossentropy  takes in the prediction from our model  model(x  and compares it to actual output  y  Optimizer ADAM  is specified here as our optimizer with a  learning rate of 0.05  CallBack This callback function is used to print both the training and testing accuracy after 10 training iterations Train To train our model we select the appropriate trainable parameters of our network with  params  In our case backpropagation is required for  down   nn_ode  and  fc  Notice that the parameters for Neural ODE is given by  nn_ode.p  Expected Output"},{"doctype":"documentation","id":"references/DiffEqSensitivity.setup_reverse_callbacks","title":"setup_reverse_callbacks","text":"Sets up callbacks for the adjoint pass This is a version that has an effect at each event of the forward pass and defines the reverse pass values via the vjps as described in https://arxiv.org/pdf/1905.10403.pdf Equation 13 For more information see https://github.com/SciML/DiffEqSensitivity.jl/issues/4"},{"doctype":"documentation","id":"references/SciMLOperators.getindex","title":"getindex","text":""},{"doctype":"documentation","id":"references/NeuralOperators.l₂loss","title":"l₂loss","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.add_integrator_header","title":"add_integrator_header","text":""},{"doctype":"documentation","id":"references/Catalyst.namespace_reactions","title":"namespace_reactions","text":""},{"doctype":"documentation","id":"references/Surrogates._coeff_nd","title":"_coeff_nd","text":""},{"doctype":"documentation","id":"references/GlobalSensitivity._expanding_window_hadamard","title":"_expanding_window_hadamard","text":""},{"doctype":"documentation","id":"references/LabelledArrays.LArray","title":"LArray","text":"Tuple NamedTuple Tuple kwargs a b c d a b c d The standard constructors for  LArray  For example Creates a copy of v1 with corresponding items in kwargs replaced For example"},{"doctype":"documentation","id":"references/ModelingToolkit.AbstractSystem","title":"AbstractSystem","text":"DocStringExtensions.TypeDefinition TODO"},{"doctype":"documentation","id":"references/DiffEqFlux.calckernel","title":"calckernel","text":""},{"doctype":"document","id":"DiffEqFlux/layers/TensorLayer.md","title":"Tensor Product Layer","text":"Tensor Product Layer The following layer is a helper function for easily constructing a TensorLayer which takes as input an array of n tensor product basis  B_1 B_2  B_n  a data point x computes  z[i  W[i ⨀ B_1(x[1 ⨂ B_2(x[2 ⨂  ⨂ B_n(x[n  where W is the layer's weight and returns z[1  z[out"},{"doctype":"documentation","id":"references/DiffEqSensitivity.ImplicitCorrection","title":"ImplicitCorrection","text":""},{"doctype":"document","id":"Catalyst/tutorials/bifurcation_diagram.md","title":"Bifurcation Diagrams","text":"rn v0 v S X n S X n D A n K n d ∅ X τ X τ ∅ A S D τ v0 v K n d odefun convert rn jac F u p odefun u p J u p odefun jac u p p_idx p_span plot_var_idx BifurcationKit Plots LinearAlgebra Setfield opts ContinuationPar dsmax dsmin ds maxSteps pMin p_span pMax p_span detectBifurcation newtonOptions NewtonPar tol verbose maxIter DO DeflationOperator dot fill length rn params_input setindex! copy p_span p_idx branches continuation F J params_input _ p_idx opts DO verbosity plot recordFromSolution x p x plot_var_idx perturbSolution x p id x rand length x callbackN BifurcationKit cbMaxNorm plot branches xlabel rn ps ylabel Symbol rn val f markersize ylim Inf color blue plotbifpoints putbifptlegend linewidthstable linewidthunstable plot branches lw color map i i blue red getproperty branches branch n_unstable plot! branches lw color map i i blue red getproperty branches branch n_unstable plot! branches lw color map i i blue red getproperty branches branch n_unstable plotbifpoints xlabel rn ps ylabel Symbol rn val f Bifurcation Diagrams Bifurcation diagrams can be produced from Catalyst generated models through the use of the  BifurcationKit.jl  package This tutorial gives a simple example of how to create such a bifurcation diagram This tutorial is written for BifurcationKit version 0.1.11 First we declare our model For our example we will use a bistable switch but which also contains a Hopf bifurcation Next BifurcationKit requires another form for the system function and Jacobian than what is used by the SciML ecosystem so we need to declare these We also need to specify the system parameters for which we wish to plot the bifurcation diagram Finally we also need to set the input required to make the diagram This is the index in the parameter array of the bifurcation parameter the range over which we wish to plot the bifurcation diagram as well as for which variable we wish to plot the steady state values in the diagram Now we need to fetch the required packages to create the bifurcation diagram Next we need to specify the input options for the pseudo-arclength continuation method which produces the diagram We will use a  deflated continuation  With all this done we can compute the bifurcations which can then be plotted using bifurcation_diagram1 Here the Hopf bifurcation is amrked with a blue square The region with a thiner linewidth corresponds to unstable steady states If one wishes to mark these differently it is possible to plot the individual brances separatly bifurcation_diagram2 Note that the second branch corresponds to a negative steady state which is biological irrelevant and we hence do not plot"},{"doctype":"document","id":"Surrogates/polychaos.md","title":"Polynomial chaos surrogate","text":"Polynomial chaos surrogate Note This surrogate requires the SurrogatesPolyChaos module which can be added by inputting add SurrogatesPolyChaos from the Julia command line We can create a surrogate using a polynomial expansion with a different polynomial basis depending on the distribution of the data we are trying to fit Under the hood PolyChaos.jl has been used It is possible to specify a type of polynomial for each dimension of the problem Sampling We choose to sample f in 25 points between 0 and 10 using the  sample  function The sampling points are chosen using a Low Discrepancy this can be done by passing  LowDiscrepancySample  to the  sample  function Building a Surrogate"},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.find_solvables!","title":"find_solvables!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.namespace_equation","title":"namespace_equation","text":""},{"doctype":"documentation","id":"references/MethodOfLines.generate_cartesian_rules","title":"generate_cartesian_rules","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.inplace_vjp","title":"inplace_vjp","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess","title":"DiffEqNoiseProcess","text":""},{"doctype":"documentation","id":"references/ModelingToolkit._config","title":"_config","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.PeriodicBC","title":"PeriodicBC","text":"q  PeriodicBC Qx Qy   PeriodicBC(size(u When all dimensions are to be extended with a periodic boundary condition Creates a periodic boundary condition where the lower index end of some u is extended with the upper index end and vice versa It is not recommended to concretize this BC type in to a BandedMatrix since the vast majority of bands will be all 0s SparseMatrix concretization is recommended Currently periodic boundary conditions are only implemented in one dimension Type parameters T  is the domain type of the discretized function See also  AbstractBC"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.sample_box","title":"sample_box","text":""},{"doctype":"document","id":"LabelledArrays/index.md","title":"LabelledArrays.jl: Arrays with Label Goodness","text":"Pkg Pkg add LabelledArrays.jl Arrays with Label Goodness LabelledArrays.jl is a package which provides arrays with labels i.e they are arrays which  map   broadcast  and all of that good stuff but their components are labelled For instance users can name the second component of an array to  second  and retrieve it with  A.second  Installation To install LabelledArrays.jl use the Julia package manager Contributing Please refer to the  SciML ColPrac Contributor's Guide on Collaborative Practices for Community Packages  for guidance on PRs issues and other matters relating to contributing to SciML There are a few community forums the diffeq-bridged channel in the  Julia Slack JuliaDiffEq  on Gitter on the  Julia Discourse forums see also  SciML Community page"},{"doctype":"documentation","id":"references/Optimization.AutoTracker","title":"AutoTracker","text":"f kwargs AutoTracker  AbstractADType An AbstractADType choice for use in OptimizationFunction for automatically generating the unspecified derivative functions Usage This uses the  Tracker.jl  package Generally slower than ReverseDiff it is generally applicable to many pure Julia codes Compatible with GPUs Not compatible with Hessian-based optimization Not compatible with Hv-based optimization Not compatible with constraint functions Note that only the unspecified derivative functions are defined For example if a  hess  function is supplied to the  OptimizationFunction  then the Hessian is not defined via Tracker"},{"doctype":"documentation","id":"references/PolyChaos.HermiteMeasure","title":"HermiteMeasure","text":""},{"doctype":"documentation","id":"references/SciMLBase.DiffEqIdentity","title":"DiffEqIdentity","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.apply_partial_int_rules","title":"apply_partial_int_rules","text":""},{"doctype":"document","id":"NeuralPDE/pinn/neural_adapter.md","title":"Transfer Learning with Neural Adapter","text":"Flux GalacticOptimJL DiffEqBase Interval infimum supremum x y u Dxx Differential x Dyy Differential y eq Dxx u x y Dyy u x y sin pi x sin pi y bcs u y u y sin pi sin pi y u x u x sin pi x sin pi domains x Interval y Interval quadrature_strategy reltol abstol maxiters batch inner af Flux tanh chain1 Dense inner af Dense inner inner af Dense inner initθ Float64 chain1 discretization chain1 quadrature_strategy init_params initθ pde_system eq bcs domains x y u x y prob pde_system discretization sym_prob pde_system discretization res prob BFGS maxiters discretization inner_ af Flux tanh chain2 inner_ af inner_ inner_ af inner_ inner_ af inner_ initθ2 Float64 chain2 loss cord θ chain2 cord θ cord res minimizer strategy prob_ loss initθ2 pde_system strategy callback p l println l res_ prob_ BFGS callback callback maxiters parameterless_type_θ DiffEqBase initθ2 phi_ chain2 parameterless_type_θ xs ys infimum d domain supremum d domain d domains analytic_sol_func x y sin pi x sin pi y pi u_predict reshape first x y res minimizer x xs y ys length xs length ys u_predict_ reshape first phi_ x y res_ minimizer x xs y ys length xs length ys u_real reshape analytic_sol_func x y x xs y ys length xs length ys diff_u u_predict u_real diff_u_ u_predict_ u_real Plots p1 plot xs ys u_predict linetype contourf title p2 plot xs ys u_predict_ linetype contourf title p3 plot xs ys u_real linetype contourf title p4 plot xs ys diff_u linetype contourf title p5 plot xs ys diff_u_ linetype contourf title plot p1 p2 p3 p4 p5 Flux GalacticOptimJL DiffEqBase Interval infimum supremum x y u Dxx Differential x Dyy Differential y eq Dxx u x y Dyy u x y sin pi x sin pi y bcs u y u y sin pi sin pi y u x u x sin pi x sin pi x_0 x_end x_domain Interval x_0 x_end y_domain Interval domains x x_domain y y_domain count_decomp af Flux tanh inner chains inner af inner inner af inner _ count_decomp initθs map c Float64 c chains xs_ infimum x_domain count_decomp supremum x_domain xs_domain xs_ i xs_ i i length xs_ domains_map map xs_domain xs_dom x_domain_ Interval xs_dom domains_ x x_domain_ y y_domain analytic_sol_func x y sin pi x sin pi y pi create_bcs x_domain_ phi_bound x_0 x_e x_domain_ left x_domain_ right x_0 bcs u y u x_e y analytic_sol_func x_e y u x u x sin pi x sin pi bcs bcs u x_0 y phi_bound x_0 y u x_e y analytic_sol_func x_e y u x u x sin pi x sin pi bcs reses phis pde_system_map i count_decomp println i domains_ domains_map i phi_in cord phis i cord reses i minimizer phi_bound x y phi_in vcat x y phi_bound x y Base Broadcast broadcasted typeof phi_bound x y phi_bound x y bcs_ create_bcs domains_ domain phi_bound pde_system_ eq bcs_ domains_ x y u x y push! pde_system_map pde_system_ strategy count_decomp discretization chains i strategy init_params initθs i prob pde_system_ discretization symprob pde_system_ discretization res_ prob BFGS maxiters discretization push! reses res_ push! phis compose_result dx u_predict_array Float64 diff_u_array Float64 ys infimum domains domain dx supremum domains domain xs_ infimum x_domain dx supremum x_domain xs collect xs_ index_of_interval x_ i x_domain enumerate xs_domain x_ x_domain x_ x_domain i x_ xs i index_of_interval x_ u_predict_sub first phis i x_ y reses i minimizer y ys u_real_sub analytic_sol_func x_ y y ys diff_u_sub abs u_predict_sub u_real_sub append! u_predict_array u_predict_sub append! diff_u_array diff_u_sub xs ys infimum d domain dx supremum d domain d domains u_predict reshape u_predict_array length xs length ys diff_u reshape diff_u_array length xs length ys u_predict diff_u dx u_predict diff_u compose_result dx inner_ af Flux tanh chain2 inner_ af inner_ inner_ af inner_ inner_ af inner_ inner_ af inner_ initθ2 Float64 chain2 pde_system eq bcs domains x y u x y losses map count_decomp i loss cord θ chain2 cord θ phis i cord reses i minimizer callback p l println l prob_ losses initθ2 pde_system_map count_decomp res_ prob_ BFGS callback callback maxiters prob_ losses res_ minimizer pde_system_map count_decomp res_ prob_ BFGS callback callback maxiters parameterless_type_θ DiffEqBase initθ2 phi_ chain2 parameterless_type_θ xs ys infimum d domain dx supremum d domain d domains u_predict_ reshape first phi_ x y res_ minimizer x xs y ys length xs length ys u_real reshape analytic_sol_func x y x xs y ys length xs length ys diff_u_ u_predict_ u_real Plots p1 plot xs ys u_predict linetype contourf title p2 plot xs ys u_predict_ linetype contourf title p3 plot xs ys u_real linetype contourf title p4 plot xs ys diff_u linetype contourf title p5 plot xs ys diff_u_ linetype contourf title plot p1 p2 p3 p4 p5 Transfer Learning with Neural Adapter Transfer learning is a machine learning technique where a model trained on one task is re-purposed on a second related task neural_adapter  is method that trains a neural network using the results from an already obtained prediction This allows reusing the obtained prediction results and pre-training states of the neural network to get a new prediction or reuse the results of predictions to train a related task for example the same task with a different domain It makes it possible to create more flexible training schemes Retrain the prediction Using the example of 2D Poisson equation it is shown how using method neural_adapter to retrain the prediction of one neural network to another image neural_adapter Domain decomposition In this example we first obtain a prediction of 2D Poisson equation on subdomains We split up full domain into 10 sub problems by x and create separate neural networks for each sub interval If x domain ∈ x_0 x_end so it is decomposed on 10 part sub x domains  x_0 x_1  x_i,x_i+1  x_9,x_end And then using the method neural_adapter we retrain the banch of 10 predictions to the one prediction for full domain of task domain_decomposition decomp"},{"doctype":"documentation","id":"references/PolyChaos._createMethodVector","title":"_createMethodVector","text":""},{"doctype":"document","id":"Optimization/optimization_packages/metaheuristics.md","title":"Metaheuristics.jl","text":"Pkg Pkg add rosenbrock x p p x p x x x0 zeros p f rosenbrock prob f x0 p lb ub sol prob ECA maxiters maxtime rosenbrock x p p x p x x x0 zeros p f rosenbrock prob f x0 p lb ub sol prob ECA use_initial maxiters maxtime Metaheuristics.jl Metaheuristics  is a is a Julia package implementing  metaheuristic algorithms  for global optiimization that do not require for the optimized function to be differentiable Installation OptimizationMetaheuristics.jl To use this package install the OptimizationMetaheuristics package Global Optimizer Without Constraint Equations A  Metaheuristics  Single-Objective algorithm is called using one of the following Evolutionary Centers Algorithm  ECA Differential Evolution  DE  with 5 different stratgies DE(strategy=:rand1   default strategy DE(strategy=:rand2 DE(strategy=:best1 DE(strategy=:best2 DE(strategy=:randToBest1 Particle Swarm Optimization  PSO Artificial Bee Colony  ABC Gravitational Search Algorithm  CGSA Simulated Annealing  SA Whale Optimization Algorithm  WOA Metaheuristics  also performs  Multiobjective optimization  but this is not yet supported by  Optimization  Each optimizer sets default settings based on the optimization problem but specific parameters can be set as shown in the original  Documentation Additionally  Metaheuristics  common settings which would be defined by  Metaheuristics.Options  can be simply passed as special keywoard arguments to  solve  without the need to use the  Metaheuristics.Options  struct Lastly information about the optimization problem such as the true optimum is set via  Metaheuristics.Information  and passed as part of the optimizer struct to  solve  e.g  solve(prob ECA(information=Metaheuristics.Inoformation(f_optimum  0.0 The currently available algorithms and their parameters are listed  here  Notes The algorithms in  Metaheuristics  are performing global optimization on problems without constraint equations However lower and upper constraints set by  lb  and  ub  in the  OptimizationProblem  are required Examples The Rosenbrock function can optimized using the Evolutionary Centers Algorithm  ECA  as follows Per default  Metaheuristics  ignores the initial values  x0  set in the  OptimizationProblem  In order to for  Optimization  to use  x0  we have to set  use_initial=true  With Constraint Equations While  Metaheuristics.jl  supports such constraints  Optimization.jl  currently does not relay these constraints"},{"doctype":"documentation","id":"references/ModelingToolkit.unbound_inputs","title":"unbound_inputs","text":"Return inputs that are not bound within the system i.e external inputs See also  bound_inputs   unbound_inputs   bound_outputs   unbound_outputs"},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.Matching","title":"Matching","text":""},{"doctype":"documentation","id":"references/NeuralPDE.KolmogorovParamDomain","title":"KolmogorovParamDomain","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.replace_x","title":"replace_x","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timeseries_point_quantile","title":"timeseries_point_quantile","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.result","title":"result","text":""},{"doctype":"documentation","id":"references/SciMLBase.add_analytic_labels!","title":"add_analytic_labels!","text":""},{"doctype":"documentation","id":"references/QuasiMonteCarlo.generate_design_matrices","title":"generate_design_matrices","text":"k As n lb ub sample_method k which returns  As  which is an array of  k  design matrices  A[i  that are all sampled from the same low-discrepancy sequence"},{"doctype":"documentation","id":"references/Catalyst.edgifyrates","title":"edgifyrates","text":""},{"doctype":"documentation","id":"references/SciMLOperators.InvertedOperator","title":"InvertedOperator","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.nextfloat_tdir","title":"nextfloat_tdir","text":""},{"doctype":"documentation","id":"references/MethodOfLines.flatten_division","title":"flatten_division","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.generate_innovation","title":"generate_innovation","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timepoint_meanvar","title":"timepoint_meanvar","text":""},{"doctype":"documentation","id":"references/Catalyst.edgify","title":"edgify","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.ForwardLSS","title":"ForwardLSS","text":"chunk_size autodiff Val central LSSregularizer g nothing ForwardLSS  AbstractShadowingSensitivityAlgorithm An implementation of the discrete forward-mode  least squares shadowing  LSS method LSS replaces the ill-conditioned initial value probem  ODEProblem  for chaotic systems by a well-conditioned least-squares problem This allows for computing sensitivities of long-time averaged quantities with respect to the parameters of the  ODEProblem  The computational cost of LSS scales as number of states x number of time steps Converges to the correct sensitivity at a rate of  T^(-1/2  where  T  is the time of the trajectory See  NILSS  and  NILSAS  for a more efficient non-intrusive formulation Constructor Keyword Arguments autodiff  Use automatic differentiation for constructing the Jacobian if the Jacobian needs to be constructed  Defaults to  true  chunk_size  Chunk size for forward-mode differentiation if full Jacobians are built  autojacvec=false  and  autodiff=true  Default is  0  for automatic choice of chunk size diff_type  The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with  autodiff=false  LSSregularizer  Using  LSSregularizer  one can choose between three different regularization routines The default choice is  TimeDilation(10.0,0.0,0.0  CosWindowing  cos windowing of the time grid i.e the time grid saved time steps is transformed using a cosine Cos2Windowing  cos^2 windowing of the time grid TimeDilation(alpha::Number,t0skip::Number,t1skip::Number  Corresponds to a time dilation  alpha  controls the weight  t0skip  and  t1skip  indicate the times truncated at the beginnning and end of the trajectory respectively g  instantaneous objective function of the long-time averaged objective SciMLProblem Support This  sensealg  only supports  ODEProblem s This  sensealg  does not support events callbacks This  sensealg  assumes that the objective is a long-time averaged quantity and ergodic i.e the time evolution of the system behaves qualitatively the same over infinite time independent of the specified initial conditions such that only the sensitivity with respect to the parameters is of interest References Wang Q Hu R and Blonigan P Least squares shadowing sensitivity analysis of chaotic limit cycle oscillations Journal of Computational Physics 267 210-224 2014 Wang Q Convergence of the Least Squares Shadowing Method for Computing Derivative of Ergodic Averages SIAM Journal on Numerical Analysis 52 156–170 2014 Blonigan P Gomez S Wang Q Least Squares Shadowing for sensitivity analysis of turbulent fluid flows in 52nd Aerospace Sciences Meeting 1–24 2014"},{"doctype":"documentation","id":"references/ModelingToolkit.DAEFunctionExpr","title":"DAEFunctionExpr","text":"Create a Julia expression for an  ODEFunction  from the  ODESystem  The arguments  dvs  and  ps  are used to set the order of the dependent variable and parameter vectors respectively"},{"doctype":"documentation","id":"references/RecursiveArrayTools.vecarr_to_vectors","title":"vecarr_to_vectors","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.BipartiteEdge","title":"BipartiteEdge","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractIntegralSolution","title":"AbstractIntegralSolution","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/Catalyst.@pack_Subgraph","title":"@pack_Subgraph","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractSecondOrderODEProblem","title":"AbstractSecondOrderODEProblem","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/ExponentialUtilities.get_subspace_cache","title":"get_subspace_cache","text":""},{"doctype":"documentation","id":"references/SciMLBase.unwrap_fw","title":"unwrap_fw","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.transformer","title":"transformer","text":""},{"doctype":"documentation","id":"references/Catalyst.assemble_oderhs","title":"assemble_oderhs","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.is_pos_int","title":"is_pos_int","text":""},{"doctype":"documentation","id":"references/RecursiveArrayTools.recursive_eltype","title":"recursive_eltype","text":""},{"doctype":"documentation","id":"references/SciMLOperators.has_mul!","title":"has_mul!","text":""},{"doctype":"documentation","id":"references/Surrogates.section_sampler_returner","title":"section_sampler_returner","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.AliasGraph","title":"AliasGraph","text":"When eliminating variables keeps track of which variables where eliminated in favor of which others Currently only supports elimination as direct aliases  1 We represent this as a dict from eliminated variables to a coeff var pair representing the variable that it was aliased to"},{"doctype":"documentation","id":"references/Catalyst.setdefaults!","title":"setdefaults!","text":"sir SIR β S I I ν I R β ν sir S I R β ν β ν t S t I t R t sir S I R β ν Sets the default initial values of parameters and species in the  ReactionSystem   rn  For example gives initial/default values to each of  S   I  and  β Notes Can not be used to set default values for species variables or parameters of subsystems or constraint systems Either set defaults for those systems directly or  flatten  to collate them into one system before setting defaults Defaults can be specified in any iterable container of symbols to value pairs or symbolics to value pairs"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.NoiseProcess","title":"NoiseProcess","text":"t0 W0 Z0 dist bridge iip dist rswm save_everystep rng Xorshifts Xoroshiro128Plus rand UInt64 reset reseed dist! rand_vec W dt rng rand_vec dist W dt rng bridge! rand_vec W W0 Wh q h rng rand_vec bridge! W W0 Wh q h rng W dt rng typeof W dW AbstractArray typeof W dW SArray sqrt abs dt rng W dW sqrt abs dt rng typeof W dW rand_vec W dt rng rng rand_vec sqrtabsdt sqrt abs dt rand_vec sqrtabsdt W W0 Wh q h rng typeof W dW AbstractArray sqrt q q abs h rng W dW q Wh sqrt q q abs h rng typeof W dW q Wh rand_vec W W0 Wh q h rng rng rand_vec sqrtcoeff sqrt q q abs h rand_vec sqrtcoeff rand_vec q Wh t0 W0 Z0 kwargs t0 W0 Z0 kwargs t0 W0 Z0 nothing t0 W0 Z0 kwargs t0 W0 Z0 nothing t0 W0 Z0 kwargs A  NoiseProcess  is a type defined as t0  is the first timepoint W0  is the first value of the process Z0  is the first value of the pseudo-process This is necessary for higher order algorithms If it's not needed set to  nothing  dist  the distribution for the steps over time bridge  the bridging distribution Optional but required for adaptivity and interpolating at new values save_everystep  whether to save every step of the Brownian timeseries rng  the local RNG used for generating the random numbers reset  whether to reset the process with each solve reseed  whether to reseed the process with each solve The signature for the  dist  is for inplace functions and otherwise The signature for  bridge  is and the out of place syntax is Here  W  is the noise process  W0  is the left side of the current interval  Wh  is the right side of the current interval  h  is the interval length and  q  is the proportion from the left where the interpolation is occuring Direct Construction Example The easiest way to show how to directly construct a  NoiseProcess  is by example Here we will show how to directly construct a  NoiseProcess  which generates Gaussian white noise This is the noise process which uses  randn  A special dispatch is added for complex numbers for  randn()+im*randn())/sqrt(2  This function is  DiffEqNoiseProcess.wiener_randn  or with    respectively The first function that must be defined is the noise distribution This is how to generate  W(t+dt  given that we know  W(x  for  x∈[t₀,t  For Gaussian white noise we know that W(dt ∼ N(0,dt for  W(0)=0  which defines the stepping distribution Thus its noise distribution function is for the out of place versions and for the inplace versions Optionally we can provide a bridging distribution This is the distribution of  W(qh  for  q∈[0,1  given that we know  W(0)=0  and  W(h)=Wₕ  For Brownian motion this is known as the Brownian Bridge and is well known to have the distribution W(qh ∼ N(qWₕ,(1-q)qh Thus we have the out-of-place and in-place versions as These functions are then placed in a noise process Notice that we can optionally provide an alternative adaptive algorithm for the timestepping rejections  RSWM  defaults to the Rejection Sampling with Memory 3 algorithm RSwM3 Note that the standard constructors are simply These will generate a Wiener process which can be stepped with  step!(W,dt  and interpolated as  W(t "},{"doctype":"documentation","id":"references/SciMLBase.FactorizedDiffEqArrayOperator","title":"FactorizedDiffEqArrayOperator","text":"Like DiffEqArrayOperator but stores a Factorization instead Supports left division and  ldiv  when applied to an array"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.gbm_bridge","title":"gbm_bridge","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.AbstractNonlinearSolveAlgorithm","title":"AbstractNonlinearSolveAlgorithm","text":""},{"doctype":"documentation","id":"references/SciMLBase.FINALIZE_DEFAULT","title":"FINALIZE_DEFAULT","text":""},{"doctype":"documentation","id":"references/SciMLBase.has_syms","title":"has_syms","text":""},{"doctype":"documentation","id":"references/PolyChaos.deg","title":"deg","text":""},{"doctype":"documentation","id":"references/SciMLBase.addat!","title":"addat!","text":"Grows the ODE by adding the  idxs  components Must be contiguous indices"},{"doctype":"document","id":"NeuralPDE/solvers/deep_fbsde.md","title":"Deep Forward-Backwards SDEs for Terminal Parabolic PDEs","text":"g f μ_f σ_f X0 tspan p nothing Deep Forward-Backwards SDEs for Terminal Parabolic PDEs To solve high-dimensional PDEs one should first describe the PDE in terms of the  TerminalPDEProblem  with constructor which describes the semilinear parabolic PDE of the form paraPDE with terminating condition  u(tspan[2],x  g(x  These methods solve the PDE in reverse satisfying the terminal equation and giving a point estimate at  u(tspan[1],X0  The dimensionality of the PDE is determined by the choice of  X0  which is the initial stochastic state To solve this PDE problem there exist two algorithms NNPDENS(u0,σᵀ∇u;opt=Flux.ADAM(0.1  Uses a neural stochastic differential equation which is then solved by the methods available in DifferentialEquations.jl The  alg  keyword is required for specifying the SDE solver algorithm that will be used on the internal SDE All of the other keyword arguments are passed to the SDE solver NNPDEHan(u0,σᵀ∇u;opt=Flux.ADAM(0.1  Uses the stochastic RNN algorithm  from Han  Only applicable when  μ_f  and  σ_f  result in a non-stiff SDE where low order non-adaptive time stepping is applicable Here  u0  is a Flux.jl chain with a  d dimensional input and a 1-dimensional output For  NNPDEHan   σᵀ∇u  is an array of  M  chains with a  d dimensional input and a  d dimensional output where  M  is the total number of timesteps For  NNPDENS  it is a  d+1 dimensional input where the final value is time and a  d dimensional output  opt  is a Flux.jl optimizer Each of these methods has a special keyword argument  pabstol  which specifies an absolute tolerance on the PDE's solution and will exit early if the loss reaches this value Its default value is  1f-6 "},{"doctype":"documentation","id":"references/GlobalSensitivity._recursive_hadamard","title":"_recursive_hadamard","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.NewtonRaphson","title":"NewtonRaphson","text":""},{"doctype":"documentation","id":"references/Catalyst.addspecies!","title":"addspecies!","text":"Given a  ReactionSystem  add the species corresponding to the variable  s  to the network if it is not already defined Returns the integer id of the species within the system Notes disablechecks  will disable checking for whether the passed in variable is already defined which is useful when adding many new variables to the system  Do not disable checks  unless you are sure the passed in variable is a new variable as this will potentially leave the system in an undefined state Given a  ReactionSystem  add the species corresponding to the variable  s  to the network if it is not already defined Returns the integer id of the species within the system disablechecks  will disable checking for whether the passed in variable is already defined which is useful when adding many new variables to the system  Do not disable checks  unless you are sure the passed in variable is a new variable as this will potentially leave the system in an undefined state"},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.pantelides!","title":"pantelides!","text":"Perform Pantelides algorithm"},{"doctype":"documentation","id":"references/NeuralOperators.operator_conv","title":"operator_conv","text":""},{"doctype":"documentation","id":"references/Catalyst.speciesmap","title":"speciesmap","text":"Given a  ReactionSystem  return a Dictionary mapping species that participate in  Reaction s to their index within  species(network "},{"doctype":"document","id":"GlobalSensitivity/methods/sobol.md","title":"Sobol Method","text":"order Vector Int nboot Int conf_level Float64 ishi X A B sin X A sin X B X sin X n lb ones π ub ones π sampler A B n lb ub sampler res1 ishi order A B ishi_batch X A B sin X A sin X B X sin X res2 ishi_batch A B batch Sobol Method The  Sobol  object has as its fields the  order  of the indices to be estimated order   the order of the indices to calculate Defaults to  0,1  which means the Total and First order indices Passing  2  enables calculation of the Second order indices as well For confidence interval calculation  nboot  should be specified for the number 0 of bootstrap runs and  conf_level  for the confidence level the default for which is  0.95  Sobol Method Details Sobol is a variance-based method and it decomposes the variance of the output of the model or system into fractions which can be attributed to inputs or sets of inputs This helps to get not just the individual parameter's sensitivities but also gives a way to quantify the affect and sensitivity from the interaction between the parameters  Y  f_0 sum_{i=1}^d f_i(X_i sum_{i  j}^d f_{ij}(X_i,X_j   f_{1,2...d}(X_1,X_2,..X_d  Var(Y  sum_{i=1}^d V_i  sum_{i  j}^d V_{ij    V_{1,2...,d The Sobol Indices are order\"ed the first order indices given by  S_i  frac{V_i}{Var(Y  the contribution to the output variance of the main effect of  X_i  therefore it measures the effect of varying  X_i  alone but averaged over variations in other input parameters It is standardised by the total variance to provide a fractional contribution Higher-order interaction indices  S_{i,j S_{i,j,k  and so on can be formed by dividing other terms in the variance decomposition by  Var(Y  API Ei_estimator  can take  Homma1996   Sobol2007  and  Jansen1999  for which Monte Carlo estimator is used for the  Ei  term Defaults to  Jansen1999  Details for these can be found in the corresponding papers   Homma1996    Homma T and Saltelli A 1996 Importance measures in global sensitivity analysis of nonlinear models Reliability Engineering  System Safety 52(1 pp.1-17    Sobol2007    I.M Sobol S Tarantola D Gatelli S.S Kucherenko and W Mauntz 2007 Estimating the approx imation errors when fixing unessential factors in global sensitivity analysis Reliability Engineering and System Safety 92 957–960   A Saltelli P Annoni I Azzini F Campolongo M Ratto and S Tarantola 2010 Variance based sensitivity analysis of model output Design and estimator for the total sensitivity index Computer Physics Communications 181 259–270    Jansen1999    M.J.W Jansen 1999 Analysis of variance designs for model output Computer Physics Communi cation 117 35–43 Example"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.interpolate!","title":"interpolate!","text":""},{"doctype":"documentation","id":"references/GlobalSensitivity.MatSpread","title":"MatSpread","text":""},{"doctype":"documentation","id":"references/Catalyst.__init__","title":"__init__","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.iv_from_nested_derivative","title":"iv_from_nested_derivative","text":""},{"doctype":"documentation","id":"references/MethodOfLines.error_analysis","title":"error_analysis","text":""},{"doctype":"documentation","id":"references/SciMLBase.RECOMPILE_BY_DEFAULT","title":"RECOMPILE_BY_DEFAULT","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.dae_index_lowering","title":"dae_index_lowering","text":"Perform the Pantelides algorithm to transform a higher index DAE to an index 1 DAE  kwargs  are forwarded to  pantelides  End users are encouraged to call  structural_simplify  instead which calls this function internally"},{"doctype":"documentation","id":"references/ModelingToolkit.get_defaults","title":"get_defaults","text":""},{"doctype":"documentation","id":"references/Catalyst.create_ReactionStruct","title":"create_ReactionStruct","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.exp_pade_p","title":"exp_pade_p","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractRODEAlgorithm","title":"AbstractRODEAlgorithm","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/SciMLBase.has_Wfact","title":"has_Wfact","text":""},{"doctype":"documentation","id":"references/SciMLBase.change_t_via_interpolation!","title":"change_t_via_interpolation!","text":"Modifies the current  t  and changes all of the corresponding values using the local interpolation If the current solution has already been saved one can provide the optional value  modify_save_endpoint  to also modify the endpoint of  sol  in the same manner"},{"doctype":"documentation","id":"references/SciMLBase.AbstractDiscreteCallback","title":"AbstractDiscreteCallback","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/Surrogates.Wendland","title":"Wendland","text":""},{"doctype":"documentation","id":"references/LinearSolve.set_prec","title":"set_prec","text":"DocStringExtensions.MethodSignatures"},{"doctype":"document","id":"DiffEqFlux/index.md","title":"DiffEqFlux: High Level Pre-Built Architectures for Implicit Deep Learning","text":"DiffEqFlux High Level Pre-Built Architectures for Implicit Deep Learning DiffEqFlux.jl is an implicit deep learning library built using the SciML ecosystem It is a high level interface that pulls together all of the tools with heuristics and helper functions to make training such deep implicit layer models fast and easy Note DiffEqFlux.jl is only for pre-built architectures and utility functions for deep implicit learning mixing differential equations with machine learning For details on automatic differentiation of equation solvers and adjoint techniques and using these methods for doing things like callibrating models to data nonlinear optimal control and PDE-constrained optimization see  SciMLSensitivity.jl Pre-Built Architectures The approach of this package is the easy and efficient training of  Universal Differential Equations  DiffEqFlux.jl provides architectures which match the interfaces of machine learning libraries such as  Flux.jl  and  Lux.jl  to make it easy to build continuous-time machine learning layers into larger machine learning applications The following layer functions exist Neural Ordinary Differential Equations Neural ODEs Collocation-Based Neural ODEs Neural ODEs without a solver by far the fastest way Multiple Shooting Neural Ordinary Differential Equations Neural Stochastic Differential Equations Neural SDEs Neural Differential-Algebriac Equations Neural DAEs Neural Delay Differential Equations Neural DDEs Augmented Neural ODEs Hamiltonian Neural Networks with specialized second order and symplectic integrators Continuous Normalizing Flows CNF  and  FFJORD Examples of how to build architectures from scratch with tutorials on things like Graph Neural ODEs can be found in the  SciMLSensitivity.jl documentation  WIP Lagrangian Neural Networks Galerkin Neural ODEs Citation If you use DiffEqFlux.jl or are influenced by its ideas please cite"},{"doctype":"documentation","id":"references/ModelingToolkit.extend","title":"extend","text":"DocStringExtensions.TypedMethodSignatures extend the  basesys  with  sys  the resulting system would inherit  sys s name by default"},{"doctype":"documentation","id":"references/Catalyst.conservedquantities","title":"conservedquantities","text":"Compute conserved quantities for a system with the given conservation laws"},{"doctype":"documentation","id":"references/SciMLBase.DEIntegrator","title":"DEIntegrator","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/Catalyst.@pack_Graph","title":"@pack_Graph","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_bcs","title":"get_bcs","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.TransformedFunction","title":"TransformedFunction","text":""},{"doctype":"documentation","id":"references/SciMLOperators.has_ldiv","title":"has_ldiv","text":""},{"doctype":"documentation","id":"references/Optimization.MaxSense","title":"MaxSense","text":""},{"doctype":"documentation","id":"references/Catalyst.linkagedeficiencies","title":"linkagedeficiencies","text":"sir SIR β S I I ν I R β ν rcs sir linkage_deficiencies sir Calculates the deficiency of each sub-reaction network within  network  Notes Requires the  incidencemat  to already be cached in  rn  by a previous call to  reactioncomplexes  For example"},{"doctype":"documentation","id":"references/SciMLBase.AbstractSplitSDEProblem","title":"AbstractSplitSDEProblem","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/LabelledArrays.SLArray","title":"SLArray","text":"Tuple NamedTuple Tuple kwargs Tuple a b c d These are the standard constructors for  SLArray  For general N-dimensional labelled arrays users need to specify the size  Tuple{dim1,dim2  in the type parameter to the  SLArray  constructor Constructing copies with some changed elements is supported by a keyword constructor whose first argument is the source and whose additional keyword arguments indicate the changes Additional examples Creates a copy of v1 with corresponding items in kwargs replaced For example"},{"doctype":"documentation","id":"references/DiffEqSensitivity.AbstractLSSregularizer","title":"AbstractLSSregularizer","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.SystemStructures.ALGEBRAIC_VARIABLE","title":"ALGEBRAIC_VARIABLE","text":""},{"doctype":"documentation","id":"references/SciMLBase.isadaptive","title":"isadaptive","text":"isadaptive(alg::DEAlgorithm Trait declaration for whether an algorithm uses adaptivity i.e has a non-quasi-static compute graph Defaults to true Checks if the integrator is adaptive"},{"doctype":"document","id":"SciMLBase/index.md","title":"The SciML Common Interface for Julia Equation Solvers","text":"The SciML Common Interface for Julia Equation Solvers The SciML common interface ties together the numerical solvers of the Julia package ecosystem into a single unified interface It is designed for maximal efficiency and parallelism while incorporating essential features for large-scale scientific machine learning such as differentiability composability and sparsity This documentation is made to pool together the docs of the various SciML libraries to paint the overarching picture establish development norms and document the shared/common functionality Domains of SciML The SciML common interface covers the following domains Linear systems  LinearProblem  Direct methods for dense and sparse Iterative solvers with preconditioning Nonlinear Systems  NonlinearProblem  Systems of nonlinear equations Scalar bracketing systems Integrals quadrature  QuadratureProblem  Differential Equations Discrete equations function maps discrete stochastic Gillespie/Markov simulations  DiscreteProblem  Ordinary differential equations ODEs  ODEProblem  Split and Partitioned ODEs Symplectic integrators IMEX Methods  SplitODEProblem  Stochastic ordinary differential equations SODEs or SDEs  SDEProblem  Stochastic differential-algebraic equations SDAEs  SDEProblem  with mass matrices Random differential equations RODEs or RDEs  RODEProblem  Differential algebraic equations DAEs  DAEProblem  and  ODEProblem  with mass matrices Delay differential equations DDEs  DDEProblem  Neutral retarded and algebraic delay differential equations NDDEs RDDEs and DDAEs Stochastic delay differential equations SDDEs  SDDEProblem  Experimental support for stochastic neutral retarded and algebraic delay differential equations SNDDEs SRDDEs and SDDAEs Mixed discrete and continuous equations Hybrid Equations Jump Diffusions  DEProblem s with callbacks Optimization  OptimizationProblem  Nonlinear constrained optimization Stochastic/Delay/Differential-Algebraic Partial Differential Equations  PDESystem  Finite difference and finite volume methods Interfaces to finite element methods Physics-Informed Neural Networks PINNs Integro-Differential Equations Fractional Differential Equations The SciML common interface also includes  ModelingToolkit.jl  for defining such systems symbolically allowing for optimizations like automated generation of parallel code symbolic simplification and generation of sparsity patterns Extended SciML Domain In addition to the purely numerical representations of mathematical objects there are also sets of problem types associated with common mathematical algorithms These are Data-driven modeling Discrete-time data-driven dynamical systems  DiscreteDataDrivenProblem  Continuous-time data-driven dynamical systems  ContinuousDataDrivenProblem  Symbolic regression  DirectDataDrivenProblem  Uncertainty quantification and expected values  ExpectationProblem  Inverse Problems Parameter Estimation and Structural Identification We note that parameter estimation and inverse problems are solved directly on their constituant problem types using tools like  DiffEqFlux.jl  Thus for example there is no  ODEInverseProblem  and instead  ODEProblem  is used to find the parameters  p  that solve the inverse problem Common Interface High Level The SciML interface is common as the usage of arguments is standardized across all of the problem domains Underlying high level ideas include All domains use the same interface of defining a  SciMLProblem  which is then solved via  solve(prob,alg;kwargs  where  alg  is a  SciMLAlgorithm  The keyword argument namings are standardized across the organization SciMLProblem s are generally defined by a  SciMLFunction  which can define extra details about a model function such as its analytical Jacobian its sparsity patterns and so on There is an organization-wide method for defining linear and nonlinear solvers used within other solvers giving maximum control of performance to the user Types used within the packages are defined by the input types For example packages attempt to internally use the type of the initial condition as the type for the state within differential equation solvers solve  calls should be thread-safe and parallel-safe init(prob,alg;kwargs  returns an iterator which allows for directly iterating over the solution process High performance is key Any performance that is not at the top level is considered a bug and should be reported as such All functions have an in-place and out-of-place form where the in-place form is made to utilize mutation for high performance on large-scale problems and the out-of-place form is for compatibility with tooling like static arrays and some reverse-mode automatic differentiation systems User-Facing Solver Libraries DifferentialEquations.jl Multi-package interface of high performance numerical solvers of differential equations ModelingToolkit.jl The symbolic modeling package which implements the SciML symbolic common interface LinearSolve.jl Multi-package interface for specifying linear solvers direct sparse and iterative along with tools for caching and preconditioners for use in large-scale modeling NonlinearSolve.jl High performance numerical solving of nonlinear systems Quadrature.jl Multi-package interface for high performance batched and parallelized numerical quadrature GalacticOptim.jl Multi-package interface for numerical solving of optimization problems NeuralPDE.jl Physics-Informed Neural Network PINN package for transforming partial differential equations into optimization problems DiffEqOperators.jl Automated finite difference method FDM package for transforming partial differential equations into nonlinear problems and ordinary differential equations DiffEqFlux.jl High level package for scientific machine learning applications such as neural and universal differential equations solving of inverse problems parameter estimation nonlinear optimal control and more DataDrivenDiffEq.jl Multi-package interface for data-driven modeling Koopman dynamic mode decomposition symbolic regression/sparsification and automated model discovery DiffEqUncertainty.jl Extension to the dynamical modeling tools for performing uncertainty quantification and calculating expectations Interface Implementation Libraries SciMLBase.jl The core package defining the interface which is consumed by the modeling and solver packages DiffEqBase.jl The core package defining the extended interface which is consumed by the differential equation solver packages DiffEqSensitivity.jl A package which pools together the definition of derivative overloads to define the common  sensealg  automatic differentiation interface DiffEqNoiseProcess.jl A package which defines the stochastic  AbstractNoiseProcess  interface for the SciML ecosystem RecursiveArrayTools.jl A package which defines the underlying  AbstractVectorOfArray  structure used as the output for all time series results ArrayInterface.jl The package which defines the extended  AbstractArray  interface employed throughout the SciML ecosystem Using-Facing Modeling Libraries There are too many to name here and this will be populated when there is time Flowchart Example for PDE-Constrained Optimal Control The following example showcases how the pieces of the common interface connect to solve a problem that mixes inference symbolics and numerics External Binding Libraries diffeqr Solving differential equations in R using DifferentialEquations.jl with ModelingToolkit for JIT compilation and GPU-acceleration diffeqpy Solving differential equations in Python using DifferentialEquations.jl Solver Libraries There are too many to name here Check out the  SciML Organization Github Page  for details Contributing Please refer to the  SciML ColPrac Contributor's Guide on Collaborative Practices for Community Packages  for guidance on PRs issues and other matters relating to contributing to SciML There are a few community forums The diffeq-bridged and sciml-bridged channels in the  Julia Slack JuliaDiffEq  on Gitter On the Julia Discourse forums look for the  modelingtoolkit tag See also  SciML Community page"},{"doctype":"documentation","id":"references/GlobalSensitivity.EASI","title":"EASI","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.build_observed_function","title":"build_observed_function","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.linear_interpolant!","title":"linear_interpolant!","text":""},{"doctype":"document","id":"ModelingToolkit/basics/FAQ.md","title":"Frequently Asked Questions","text":"indexof sym syms findfirst isequal sym syms indexof σ sys pnew β c γ sys Frequently Asked Questions Getting the index for a symbol Since  ordering of symbols is not guaranteed after symbolic transformations  one should normally refer to values by their name For example  sol[lorenz.x  from the solution But what if you need to get the index The following helper function will do the trick Transforming value maps to arrays ModelingToolkit.jl allows and recommends input maps like  x  2.0 y  3.0  because symbol ordering is not guaranteed However what if you want to get the lowered array You can use the internal function  varmap_to_vars  For example How do I handle  if  statements in my symbolic forms For statements that are in the  if then else  form use  IfElse.ifelse  from the  IfElse.jl  package to represent the code in a functional form For handling direct  if  statements you can use equivalent boolean mathematical expressions For example  if x  0   can be implemented as just  x  0    where if  x  0  then the boolean will evaluate to  0  and thus the term will be excluded from the model ERROR TypeError non-boolean Num used in boolean context If you see the error then it's likely you are trying to trace through a function which cannot be directly represented in Julia symbols The techniques to handle this problem such as  register_symbolic  are described in detail  in the Symbolics.jl documentation "},{"doctype":"documentation","id":"references/PolyChaos.LaguerreOrthoPoly","title":"LaguerreOrthoPoly","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.VariableIrreducible","title":"VariableIrreducible","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.positivemax","title":"positivemax","text":""},{"doctype":"documentation","id":"references/SciMLBase.DEFAULT_OBSERVED_NO_TIME","title":"DEFAULT_OBSERVED_NO_TIME","text":""},{"doctype":"documentation","id":"references/SciMLBase.JacobianWrapper","title":"JacobianWrapper","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.CosWindowing","title":"CosWindowing","text":""},{"doctype":"documentation","id":"references/SciMLOperators.update_coefficients!","title":"update_coefficients!","text":""},{"doctype":"document","id":"ModelingToolkit/basics/Composition.md","title":"[Composing Models and Building Reusable Components]( components)","text":"sys eqs indepvar ps subsys u0 x subsys x sys Pair Num Any x u y σ z u sys y u a b c d b b c c d d Composing Models and Building Reusable Components  components The symbolic models of ModelingToolkit can be composed together to easily build large models The composition is lazy and only instantiated at the time of conversion to numerical models allowing a more performant way in terms of computation time and memory Simple Model Composition Example The following is an example of building a model in a library with an optional forcing function and allowing the user to specify the forcing later Here the library author defines a component named  decay  The user then builds two  decay  components and connects them saying the forcing term of  decay1  is a constant while the forcing term of  decay2  is the value of the state variable  x  Now we can solve the system Basics of Model Composition Every  AbstractSystem  has a  system  keyword argument for specifying subsystems A model is the composition of itself and its subsystems For example if we have the  equations  of  sys  is the concatenation of  get_eqs(sys  and  equations(subsys  the states are the concatenation of their states etc When the  ODEProblem  or  ODEFunction  is generated from this system it will build and compile the functions associated with this composition The new equations within the higher level system can access the variables in the lower level system by namespacing via the  nameof(subsys  For example let's say there is a variable  x  in  states  and a variable  x  in  subsys  We can declare that these two variables are the same by specifying their equality  x  subsys.x  in the  eqs  for  sys  This algebraic relationship can then be simplified by transformations like  structural_simplify  which will be described later Numerics with Composed Models These composed models can then be directly transformed into their associated  SciMLProblem  type using the standard constructors When this is done the initial conditions and parameters must be specified in their namespaced form For example Note that any default values within the given subcomponent will be used if no override is provided at construction time If any values for initial conditions or parameters are unspecified an error will be thrown When the model is numerically solved the solution can be accessed via its symbolic values For example if  sol  is the  ODESolution  one can use  sol[x  and  sol[subsys.x  to access the respective timeseries in the solution All other indexing rules stay the same so  sol[x,1:5  accesses the first through fifth values of  x  Note that this can be done even if the variable  x  is eliminated from the system from transformations like  alias_elimination  or  tearing  the variable will be lazily reconstructed on demand Variable scope and parameter expressions In some scenarios it could be useful for model parameters to be expressed in terms of other parameters or shared between common subsystems To facilitate this ModelingToolkit supports symbolic expressions in default values and scoped variables With symbolic parameters it is possible to set the default value of a parameter or initial condition to an expression of other variables In a hierarchical system variables of the subsystem get namespaced by the name of the system they are in This prevents naming clashes but also enforces that every state and parameter is local to the subsystem it is used in In some cases it might be desirable to have variables and parameters that are shared between subsystems or even global This can be accomplished as follows Structural Simplify In many cases the nicest way to build a model may leave a lot of unnecessary variables Thus one may want to remove these equations before numerically solving The  structural_simplify  function removes these trivial equality relationships and trivial singularity equations i.e equations which result in  0~0  expressions in over-specified systems Inheritance and Combine Model inheritance can be done in two ways implicitly or explicitly First one can use the  extend  function to extend a base model with another set of equations states and parameters An example can be found in the acausal components tutorial  acausal The explicit way is to shadow variables with equality expressions For example let's assume we have three separate systems which we want to compose to a single one This is how one could explicitly forward all states and parameters to the higher level system Note that the states are forwarded by an equality relationship while the parameters are forwarded through a relationship in their default values The user of this model can then solve this model simply by specifying the values at the highest level Tearing Problem Construction Some system types specifically  ODESystem  and  NonlinearSystem  can be further reduced if  structural_simplify  has already been applied to them This is done by using the alternative problem constructors  ODAEProblem  and  BlockNonlinearProblem  respectively In these cases the constructor uses the knowledge of the strongly connected components calculated during the process of simplification as the basis for building pre-simplified nonlinear systems in the implicit solving In summary these problems are structurally modified but could be more efficient and more stable Components with discontinuous dynamics When modeling e.g impacts saturations or Coulomb friction the dynamic equations are discontinuous in either the state or one of its derivatives This causes the solver to take very small steps around the discontinuity and sometimes leads to early stopping due to  dt  dt_min  The correct way to handle such dynamics is to tell the solver about the discontinuity by means of a root-finding equation  ODEsystem s accept a keyword argument  continuous_events where equations can be added that evaluate to 0 at discontinuities To model events that have an effect on the state provide  events::Pair{Vector{Equation Vector{Equation  where the first entry in the pair is a vector of equations describing event conditions and the second vector of equations describe the effect on the state The effect equations must be of the form Example Friction The system below illustrates how this can be used to model Coulomb friction Example Bouncing ball In the documentation for DifferentialEquations we have an example where a bouncing ball is simulated using callbacks which has an  affect  on the state We can model the same system using ModelingToolkit like this Test bouncing ball in 2D with walls Multiple events No problem This example models a bouncing ball in 2D that is enclosed by two walls at  y  pm 1.5 "},{"doctype":"documentation","id":"references/PolyChaos.AbstractCanonicalMeasure","title":"AbstractCanonicalMeasure","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.collect_difference_variables","title":"collect_difference_variables","text":""},{"doctype":"document","id":"ModelingToolkit/tutorials/stochastic_diffeq.md","title":"Modeling with Stochasticity","text":"StochasticDiffEq σ ρ β t x t y t z t D Differential t eqs D x σ y x D y x ρ z y D z x y β z noiseeqs x y z de eqs noiseeqs t x y z σ ρ β u0map x y z parammap σ β ρ prob de u0map parammap sol prob SOSRI Modeling with Stochasticity All models with  ODESystem  are deterministic  SDESystem  adds another element to the model randomness This is a  stochastic differential equation  which has a deterministic drift component and a stochastic diffusion component Let's take the Lorenz equation from the first tutorial and extend it to have multiplicative noise"},{"doctype":"documentation","id":"references/ModelingToolkit.flatten","title":"flatten","text":""},{"doctype":"documentation","id":"references/MethodOfLines.unitindices","title":"unitindices","text":"A function that creates a tuple of CartesianIndices of unit length and  N  dimensions one pointing along each dimension"},{"doctype":"documentation","id":"references/NeuralPDE.get_inf_transformation_jacobian","title":"get_inf_transformation_jacobian","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.generate_coordinates","title":"generate_coordinates","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.ReverseDiffNoise","title":"ReverseDiffNoise","text":"compile ReverseDiffNoise  NoiseChoice Uses ReverseDiff.jl to compute the vector-Jacobian products for the noise term differentiation for SDE adjoints only If  f  is in-place then it uses a array of structs formulation to do scalarized reverse mode while if  f  is out-of-place then it uses an array-based reverse mode Usually the fastest when scalarized operations exist in the f function like in scientific machine learning applications like Universal Differential Equations and the boolean compilation is enabled i.e ReverseDiffVJP(true if EnzymeVJP fails on a given choice of  f  Does not support GPUs CuArrays Constructor Keyword Arguments compile  Whether to cache the compilation of the reverse tape This heavily increases the performance of the method but requires that the  f  function of the ODE/DAE/SDE/DDE has no branching"},{"doctype":"document","id":"MethodOfLines/howitworks.md","title":"[How it works] ( hiw)","text":"How it works   hiw MethodOfLines.jl makes heavy use of  Symbolics.jl  and  SymbolicUtils.jl  namely it's rule matching features to recognize terms which require particular discretizations See  here  for the highest level overview of the algorithm Given your discretization and  PDESystem  we take each independent variable defined on the space to be discretized and create a corresponding range We then take each dependant variable and create an array of symbolic variables to represent it in its discretized form This is stored in a  DiscreteSpace  object a useful abstraction We recognize boundary conditions i.e whether they are on the upper or lower ends of the domain or periodic  here  and use this information to construct the interior of the domain for each equation  here  Each PDE is matched to each dependant variable in this step by which variable is highest order in each PDE with precedance given to time derivatives This dictates which boundary conditions reduce the size of the interior for which PDE This is done to ensure that there will be the same number of equations as discrete variable states so that the system of equations is balanced Next the boundary conditions are discretized creating an equation for each point on the boundary in terms of the discretized variables replacing any space derivatives in the direction of the boundary with their upwind finite difference expressions  This  is the place to look to see how this happens After that the system of PDEs is discretized creating a finite difference equation for each point in their interior Specific terms are recognized and the best implemented scheme for these terms dispatched For example advection terms are discretized with the upwind scheme There are also special schemes for the nonlinear laplacian and spherical laplacian See  here for how this term matching occurs  note that the order the generated rules are applied is important with more specific rules applied first to avoid their terms being matched incorrectly by more general rules  The  SymbolicUtils.jl docs  are a useful companion here See  here for the practical implementation of the finite difference schemes  Now we have a system of equations which are either ODEs linear or nonlinear equations and an equal number of unknowns See here for the system  brusssys that is generated for the Brusselator at low point count The structure of the system is simplified with  ModelingToolkit.structural_simplify  and then either an  ODEProblem  or  NonlinearProblem  is returned Under the hood the  ODEProblem  generates a fast semidiscretization written in Julia with  RuntimeGeneratedFunctions  See here for an example of the generated code  brusscode for the Brusselator system at low point count"},{"doctype":"documentation","id":"references/NeuralPDE.logscalar","title":"logscalar","text":"This function is defined here as stubs to be overriden by the subpackage NeuralPDELogging if imported"},{"doctype":"document","id":"Optimization/optimization_packages/multistartoptimization.md","title":"MultiStartOptimization.jl","text":"Pkg Pkg add rosenbrock x p p x p x x x0 zeros p f rosenbrock prob f x0 p lb ub sol prob MultistartOptimization TikTak NLopt LD_LBFGS rosenbrock x p p x p x x x0 zeros p f rosenbrock prob f x0 p lb ub sol prob MultistartOptimization TikTak NLopt LD_LBFGS sol prob MultistartOptimization TikTak LBFGS MultiStartOptimization.jl MultistartOptimization  is a is a Julia package implementing a global optimization multistart method which performs local optimization after choosing multiple starting points MultistartOptimization  requires both a global and local method to be defined The global multistart method chooses a set of initial starting points from where local the local method starts from Currently only one global method  TikTak  is implemented and called by  MultiStartOptimization.TikTak(n  where  n  is the number of initial Sobol points Installation OptimizationMultiStartOptimization.jl To use this package install the OptimizationMultiStartOptimization package Note You also need to load the relevant subpackage for the local method of you choice for example if you plan to use one of the NLopt.jl's optimizers you'd install and load OptimizationNLopt as described in the  NLopt.jl s section Global Optimizer Without Constraint Equations The methods in  MultistartOptimization  is performing global optimization on problems without constraint equations However lower and upper constraints set by  lb  and  ub  in the  OptimizationProblem  are required Examples The Rosenbrock function can optimized using  MultistartOptimization.TikTak  with 100 initial points and the local method  NLopt.LD_LBFGS  as follows You can use any  Optimization  optimizers you like The global method of the  MultiStartOptimization  is a positional argument and followed by the local method This for example means we can perform a multistartoptimization with LBFGS as the optimizer using either the  NLopt.jl  or  Optim.jl  implementation as follows Moreover this interface allows you access and adjust all the optimizer settings as you normally would"},{"doctype":"documentation","id":"references/ExponentialUtilities.compatible_multiplicative_operand","title":"compatible_multiplicative_operand","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.second_order_sensitivity_product","title":"second_order_sensitivity_product","text":"OrdinaryDiffEq ForwardDiff Test lotka! du u p t du dx p u p u u du dy p u p u u p u0 prob lotka! u0 p loss sol sum sol v ones Hv loss v prob Vern9 saveat abstol reltol Hv  second order sensitivity_product(loss,v,prob,alg,args                                sensealg=ForwardDiffOverAdjoint(InterpolatingAdjoint(autojacvec=ReverseDiffVJP                                kwargs Second order sensitivity analysis product is used for the fast calculation of  Hessian-vector products  Hv  without requiring the construction of the Hessian matrix Warning Adjoint sensitivity analysis functionality requires being able to solve   a differential equation defined by the parameter struct  p  Thus while   DifferentialEquations.jl can support any parameter struct type usage   with adjoint sensitivity analysis requires that  p  could be a valid   type for being the initial condition  u0  of an array This means that   many simple types such as  Tuple s and  NamedTuple s will work as   parameters in normal contexts but will fail during adjoint differentiation   To work around this issue for complicated cases like nested structs look   into defining  p  using  AbstractArray  libraries such as RecursiveArrayTools.jl    or ComponentArrays.jl so that  p  is an  AbstractArray  with a concrete element type Example second order sensitivity analysis calculation Arguments The arguments for this function match  adjoint_sensitivities  The only notable difference is  sensealg  which requires a second order sensitivity algorithm of which currently the only choice is  ForwardDiffOverAdjoint  which uses forward-over-reverse to mix a forward-mode sensitivity analysis with an adjoint sensitivity analysis for a faster computation than either double forward or double reverse  ForwardDiffOverAdjoint s positional argument just accepts a first order sensitivity algorithm"},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.generate_homotopy","title":"generate_homotopy","text":""},{"doctype":"document","id":"DiffEqFlux/layers/NeuralDELayers.md","title":"Neural Differential Equation Layer Functions","text":"Neural Differential Equation Layer Functions The following layers are helper functions for easily building neural differential equation architectures in the currently most efficient way As demonstrated in the tutorials they do not have to be used since automatic differentiation will just work over  solve  but these cover common use cases and choose what's known to be the optimal mode of AD for the respective equation type"},{"doctype":"documentation","id":"references/ModelingToolkit._match_eqs","title":"_match_eqs","text":""},{"doctype":"documentation","id":"references/Catalyst.isbc","title":"isbc","text":"Tests if the given symbolic variable corresponds to a boundary condition species"},{"doctype":"documentation","id":"references/ExponentialUtilities.HermitianSubspaceCache","title":"HermitianSubspaceCache","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.get_jacmat","title":"get_jacmat","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.ODESystem","title":"ODESystem","text":"σ ρ β t x t y t z t D Differential t eqs D x σ y x D y x ρ z y D z x y β z de eqs t x y z σ ρ β DocStringExtensions.TypeDefinition A system of ordinary differential equations Fields DocStringExtensions.TypeFields(false Example"},{"doctype":"documentation","id":"references/ModelingToolkit.IntervalDomain","title":"IntervalDomain","text":""},{"doctype":"documentation","id":"references/Surrogates._make_combination","title":"_make_combination","text":""},{"doctype":"documentation","id":"references/LinearSolve.KrylovJL","title":"KrylovJL","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.componentwise_meanvar","title":"componentwise_meanvar","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.gpu_or_cpu","title":"gpu_or_cpu","text":""},{"doctype":"documentation","id":"references/SciMLBase.RODESolution","title":"RODESolution","text":"DocStringExtensions.TypeDefinition Representation of the solution to an stochastic differential equation defined by an SDEProblem or of a random ordinary differential equation defined by an RODEProblem DESolution Interface For more information on interacting with  DESolution  types check out the Solution Handling page of the DifferentialEquations.jl documentation https://diffeq.sciml.ai/stable/basics/solution Fields u  the representation of the SDE or RODE solution Given as an array of solutions where  u[i  corresponds to the solution at time  t[i  It is recommended in most cases one does not access  sol.u  directly and instead use the array interface described in the Solution Handling page of the DifferentialEquations.jl documentation t  the time points corresponding to the saved values of the ODE solution W  the representation of the saved noise process from the solution See the Noise Processes page of the DifferentialEquations.jl documentation for more details https://diffeq.sciml.ai/stable/features/noise_process  Note that this noise is only saved in full if  save_noise=true  in the solver prob  the original SDEProblem/RODEProblem that was solved alg  the algorithm type used by the solver destats  statistics of the solver such as the number of function evaluations required number of Jacobians computed and more retcode  the return code from the solver Used to determine whether the solver solved successfully  sol.retcode  Success  whether it terminated due to a user-defined callback  sol.retcode  Terminated  or whether it exited due to an error For more details see the return code section of the DifferentialEquations.jl documentation"},{"doctype":"documentation","id":"references/PolyChaos.AbstractCanonicalOrthoPoly","title":"AbstractCanonicalOrthoPoly","text":""},{"doctype":"documentation","id":"references/SciMLBase.isfunctionwrapper","title":"isfunctionwrapper","text":""},{"doctype":"document","id":"DiffEqNoiseProcess/noise_processes.md","title":"Classic Noise Processes","text":"Classic Noise Processes This section describes the available  NoiseProcess  types Note that all keyword arguments are splatted into the  NoiseProcess  constructor and thus options like  reset  are available on the pre-built processes Bridges"},{"doctype":"documentation","id":"references/DiffEqSensitivity.SteadyStateAdjoint","title":"SteadyStateAdjoint","text":"chunk_size autodiff Val central autojacvec autodiff linsolve nothing SteadyStateAdjoint  AbstractAdjointSensitivityAlgorithm An implementation of the adjoint differentiation of a nonlinear solve Uses the implicit function theorem to directly compute the derivative of the solution to  f(u,p  0  with respect to  p  Constructor Keyword Arguments Keyword Arguments autodiff  Use automatic differentiation for constructing the Jacobian if the Jacobian needs to be constructed  Defaults to  true  chunk_size  Chunk size for forward-mode differentiation if full Jacobians are built  autojacvec=false  and  autodiff=true  Default is  0  for automatic choice of chunk size diff_type  The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with  autodiff=false  autojacvec  Calculate the vector-Jacobian product  J'*v  via automatic differentiation with special seeding The default is  true  The total set of choices are false  the Jacobian is constructed via FiniteDiff.jl true  the Jacobian is constructed via ForwardDiff.jl TrackerVJP  Uses Tracker.jl for the vjp ZygoteVJP  Uses Zygote.jl for the vjp EnzymeVJP  Uses Enzyme.jl for the vjp ReverseDiffVJP(compile=false  Uses ReverseDiff.jl for the vjp  compile  is a boolean for whether to precompile the tape which should only be done if there are no branches  if  or  while  statements in the  f  function linsolve  the linear solver used in the adjoint solve Defaults to  nothing  which uses a polyalgorithm to attempt to automatically choose an efficient algorithm For more details on the vjp choices please consult the sensitivity algorithms documentation page or the docstrings of the vjp types References Johnson S G Notes on Adjoint Methods for 18.336 Online at http://math.mit.edu/stevenj/18.336/adjoint.pdf 2007"},{"doctype":"document","id":"MethodOfLines/curvilinear_grids.md","title":"Curvilinear Grids","text":"Curvilinear Grids Curvilinear grids can be achieved via a change of variables See  this post  on StackExchange for more"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.OrnsteinUhlenbeck","title":"OrnsteinUhlenbeck","text":""},{"doctype":"documentation","id":"references/RecursiveArrayTools.recursive_one","title":"recursive_one","text":"a Calls  one  on the bottom container to get the true element one type"},{"doctype":"documentation","id":"references/SciMLOperators.AffineOperator","title":"AffineOperator","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.TricubeKernel","title":"TricubeKernel","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.BoundaryPadded3Tensor","title":"BoundaryPadded3Tensor","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.DiffEqOperatorComposition","title":"DiffEqOperatorComposition","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.iv_from_nested_difference","title":"iv_from_nested_difference","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.contrained_optimization_problem","title":"contrained_optimization_problem","text":""},{"doctype":"documentation","id":"references/PolyChaos.samplePCE","title":"samplePCE","text":"Univariate Combines  sampleMeasure  and  evaluatePCE  i.e it first draws  n  samples from the measure then evaluates the PCE for those samples Multivariate"},{"doctype":"documentation","id":"references/PolyChaos.rm_legendre","title":"rm_legendre","text":"Creates  N  recurrence coefficients for monic Legendre polynomials that are orthogonal on  1,1  relative to  w(t  1 "},{"doctype":"documentation","id":"references/ModelingToolkit.instream","title":"instream","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.RODEParamGradientWrapper","title":"RODEParamGradientWrapper","text":""},{"doctype":"documentation","id":"references/PolyChaos.lanczos","title":"lanczos","text":"Lanczos procedure---given the nodes and weights the function generates the first  N  recurrence coefficients of the corresponding discrete orthogonal polynomials Set the Boolean  removezeroweights  to  true  if zero weights should be removed The script is adapted from the routine RKPW in W.B Gragg and W.J Harrod  The numerically stable   reconstruction of Jacobi matrices from spectral data  Numer Math 44 1984 317-335"},{"doctype":"documentation","id":"references/DiffEqSensitivity.build_deriv_config","title":"build_deriv_config","text":""},{"doctype":"documentation","id":"references/PolyChaos.genLaguerreOrthoPoly","title":"genLaguerreOrthoPoly","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.affect_equations","title":"affect_equations","text":""},{"doctype":"documentation","id":"references/NeuralPDE.PhysicsInformedNN","title":"PhysicsInformedNN","text":""},{"doctype":"documentation","id":"references/MethodOfLines.count_differentials","title":"count_differentials","text":"Counts the Differential operators for given variable x This is used to determine the order of a PDE"},{"doctype":"document","id":"Surrogates/Salustowicz.md","title":"Salustowicz","text":"Salustowicz Benchmark Function The true underlying function HyGP had to approximate is the 1D Salustowicz function The function can be evaluated in the given domain  x in 0 10  The Salustowicz benchmark function is as follows f(x  e^{(-x x^3 cos(x sin(x cos(x sin^2(x  1 Let's import these two packages   Surrogates  and  Plots  Now let's define our objective function Let's sample f in 30 points between 0 and 10 using the  sample  function The sampling points are chosen using a Sobol Sample this can be done by passing  SobolSample  to the  sample  function Now let's fit Salustowicz Function with different Surrogates Not's let's see Kriging Surrogate with different hyper parameter"},{"doctype":"documentation","id":"references/Surrogates.select_evaluation_point_ND","title":"select_evaluation_point_ND","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.error_reporting","title":"error_reporting","text":""},{"doctype":"documentation","id":"references/ParameterizedFunctions.build_param_list","title":"build_param_list","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.getaxis","title":"getaxis","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.rec_remove_macro_linenums!","title":"rec_remove_macro_linenums!","text":""},{"doctype":"documentation","id":"references/Integrals.transformation_if_inf","title":"transformation_if_inf","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.reduced_echelon_nullspace","title":"reduced_echelon_nullspace","text":""},{"doctype":"documentation","id":"references/PolyChaos.gauss","title":"gauss","text":"Gauss quadrature rule also known as Golub-Welsch algorithm gauss  generates the  N  Gauss quadrature nodes and weights for a given weight function The weight function is represented by the  N  recurrence coefficients for the monic polynomials orthogonal with respect to the weight function Note The function  gauss  accepts at most  N  length(α  1  quadrature points hence providing at most an  length(α  1 point quadrature rule Note If no  N  is provided then  N  length(α  1 "},{"doctype":"documentation","id":"references/SciMLBase.AbstractReactionNetwork","title":"AbstractReactionNetwork","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/ExponentialUtilities.gebal_noalloc!","title":"gebal_noalloc!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.var_from_nested_difference","title":"var_from_nested_difference","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.jacobian!","title":"jacobian!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.ParentScope","title":"ParentScope","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.get_affect!","title":"get_affect!","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.get_timestep","title":"get_timestep","text":""},{"doctype":"documentation","id":"references/SciMLBase.reinit!","title":"reinit!","text":"The reinit function lets you restart the integration at a new value Arguments u0  Value of  u  to start at Default value is  integrator.sol.prob.u0 Keyword Arguments t0  Starting timepoint Default value is  integrator.sol.prob.tspan[1 tf  Ending timepoint Default value is  integrator.sol.prob.tspan[2 erase_sol=true  Whether to start with no other values in the solution or keep the previous solution tstops   d_discontinuities    saveat  Cache where these are stored Default is the original cache reset_dt  Set whether to reset the current value of  dt  using the automatic  dt  determination algorithm Default is  integrator.dtcache  zero(integrator.dt  integrator.opts.adaptive reinit_callbacks  Set whether to run the callback initializations again and  initialize_save  is for that Default is  true  reinit_cache  Set whether to re-run the cache initialization function i.e resetting FSAL not allocating vectors which should usually be true for correctness Default is  true  Additionally once can access  auto_dt_reset  which will run the auto  dt  initialization algorithm"},{"doctype":"documentation","id":"references/SciMLBase.get_du","title":"get_du","text":"Returns the derivative at  t "},{"doctype":"documentation","id":"references/NeuralPDE.vectorify","title":"vectorify","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.VariableBounds","title":"VariableBounds","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.is_pox","title":"is_pox","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.ControlSystem","title":"ControlSystem","text":"t x t v t u t D Differential t loss x v u eqs D x v D v u sys loss eqs t x v u DocStringExtensions.TypeDefinition A system describing an optimal control problem This contains a loss function and ordinary differential equations with control variables that describe the dynamics Fields DocStringExtensions.TypeFields(false Example"},{"doctype":"documentation","id":"references/Surrogates._calc_coeffs","title":"_calc_coeffs","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.equivalent","title":"equivalent","text":"Unitful MT γ P unit E unit τ unit MT MT MT MT P γ MT E τ γ Test unit equivalence Example of implemented behavior"},{"doctype":"documentation","id":"references/DiffEqFlux.NeuralDSDE","title":"NeuralDSDE","text":"model1 model2 tspan alg nothing args sensealg kwargs model1 model2 tspan alg nothing args sensealg kwargs Constructs a neural stochastic differential equation neural SDE with diagonal noise Arguments model1  A Chain or FastChain neural network that defines the drift function model2  A Chain or FastChain neural network that defines the diffusion function Should output a vector of the same size as the input tspan  The timespan to be solved on alg  The algorithm used to solve the ODE Defaults to  nothing  i.e the default algorithm from DifferentialEquations.jl sensealg  The choice of differentiation algorthm used in the backpropogation Defaults to using reverse-mode automatic differentiation via Tracker.jl kwargs  Additional arguments splatted to the ODE solver See the  Common Solver Arguments  documentation for more details"},{"doctype":"documentation","id":"references/DiffEqOperators.__init__","title":"__init__","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractQuadratureProblem","title":"AbstractQuadratureProblem","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.RealWienerProcess","title":"RealWienerProcess","text":"t0 W0 Z0 nothing kwargs t0 W0 Z0 nothing kwargs The  RealWienerProcess  is a Brownian motion that is forced to be real-valued While the normal  WienerProcess  becomes complex valued if  W0  is complex this verion is real valued for when you want to for example solve an SDE defined by complex numbers where the noise is in the reals"},{"doctype":"documentation","id":"references/MethodOfLines.half_offset_centered_difference","title":"half_offset_centered_difference","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timeseries_point_weighted_meancov","title":"timeseries_point_weighted_meancov","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractDDESolution","title":"AbstractDDESolution","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/ExponentialUtilities._expv_hb","title":"_expv_hb","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.isdisturbance","title":"isdisturbance","text":"Determine whether or not symbolic variable  x  is marked as a disturbance input"},{"doctype":"documentation","id":"references/PolyChaos.build_w_genhermite","title":"build_w_genhermite","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.exactdiv","title":"exactdiv","text":""},{"doctype":"documentation","id":"references/Catalyst.reactioncomplexes","title":"reactioncomplexes","text":"Calculate the reaction complexes and complex incidence matrix for the given  ReactionSystem  Notes returns a pair of a vector of  ReactionComplex s and the complex incidence matrix An empty  ReactionComplex  denotes the null ∅ state from reactions like ∅  A or A  ∅ The complex incidence matrix  B  is number of complexes by number of reactions with B_{i j  begin{cases}\n-1 text{if the i'th complex is the substrate of the j'th reaction},\\\\\n1 text{if the i'th complex is the product of the j'th reaction},\\\\\n0 text{otherwise.}\n\\end{cases Set sparse=true for a sparse matrix representation of the incidence matrix"},{"doctype":"documentation","id":"references/SciMLOperators.cache_self","title":"cache_self","text":""},{"doctype":"documentation","id":"references/Catalyst.get_networkproperties","title":"get_networkproperties","text":"Return the current network properties of  sys "},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.componentwise_meancor","title":"componentwise_meancor","text":""},{"doctype":"documentation","id":"references/NeuralPDE.v_inf","title":"v_inf","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.ExpMethodNative","title":"ExpMethodNative","text":"Matrix exponential method corresponding to calling  Base.exp "},{"doctype":"document","id":"DiffEqSensitivity/optimal_control/SDE_control.md","title":"Controlling Stochastic Differential Equations","text":"StochasticDiffEq DiffEqCallbacks Statistics LinearAlgebra Random Plots lr epochs numtraj numtrajplot dt tinterval tstart Nintervals tspan tstart tinterval Nintervals ts Array tstart dt Nintervals tinterval dt Δ Ωmax κ C1 Float32 Parameters flType intType tType lr flType epochs intType numtraj intType numtrajplot intType dt flType tinterval flType tspan tType Nintervals intType ts Vector flType Δ flType Ωmax flType κ flType C1 flType myparameters Parameters typeof dt typeof numtraj typeof tspan lr epochs numtraj numtrajplot dt tinterval tspan Nintervals ts Δ Ωmax κ C1 nn relu tanh p_nn nn prepare_initial dt n_par theta acos rand typeof dt n_par rand typeof dt n_par pi u0 cos theta sin theta cos theta sin theta sin vcat transpose u0 u0 prepare_initial myparameters dt myparameters numtraj qubit_drift! du u p t ceR cdR ceI cdI u Δ Ωmax κ p end end nn_weights p end Ω nn u nn_weights Ωmax du ceI Δ ceR κ cdI Ω du cdI Δ ceR cdI ceI cdR ceR κ ceI Ω du ceR Δ ceI κ cdR Ω du cdR Δ ceI cdI ceI cdR ceR κ ceR Ω nothing qubit_diffusion! du u p t ceR cdR ceI cdI u κ p end du du sqrt κ ceR du sqrt κ ceI nothing condition u t integrator affect! integrator integrator u integrator u norm integrator u cb condition affect! save_positions CreateGrid t W1 t W1 Zygote CreateGrid W sqrt myparameters dt randn typeof myparameters dt size myparameters ts W1 cumsum zero myparameters dt W end dims NG CreateGrid myparameters ts W1 p_all p_nn myparameters Δ myparameters Ωmax myparameters κ prob qubit_drift! qubit_diffusion! vec u0 myparameters tspan p_all callback cb noise NG g u p t ceR u cdR u ceI u cdI u p mean cdR cdI ceR cdR ceI cdI loss p u0 prob myparameters Parameters alg EM sensealg pars p myparameters Δ myparameters Ωmax myparameters κ prob_func prob i repeat u0tmp deepcopy vec u0 i W sqrt myparameters dt randn typeof myparameters dt size myparameters ts W1 cumsum zero myparameters dt W end dims NG CreateGrid myparameters ts W1 prob p pars u0 u0tmp callback cb noise NG ensembleprob prob prob_func prob_func safetycopy _sol ensembleprob alg sensealg sensealg saveat myparameters tinterval dt myparameters dt adaptive trajectories myparameters numtraj batch_size myparameters numtraj A convert Array _sol loss g A myparameters C1 nothing loss visualize p u0 prob myparameters Parameters alg EM pars p myparameters Δ myparameters Ωmax myparameters κ prob_func prob i repeat u0tmp deepcopy vec u0 i W sqrt myparameters dt randn typeof myparameters dt size myparameters ts W1 cumsum zero myparameters dt W end dims NG CreateGrid myparameters ts W1 prob p pars u0 u0tmp callback cb noise NG ensembleprob prob prob_func prob_func safetycopy u ensembleprob alg saveat myparameters tinterval dt myparameters dt adaptive trajectories myparameters numtrajplot batch_size myparameters numtrajplot ceR u cdR u ceI u cdI u infidelity cdR cdI ceR cdR ceI cdI meaninfidelity mean infidelity loss myparameters C1 meaninfidelity loss fidelity ceR ceI ceR cdR ceI cdI mf mean fidelity dims sf std fidelity dims pl1 plot myparameters Nintervals mf ribbon sf ylim xlim myparameters Nintervals c lw xlabel ylabel legend pl plot pl1 legend size pl loss opt ADAM myparameters lr list_plots losses epoch myparameters epochs println epoch myparameters epochs u0 prepare_initial myparameters dt myparameters numtraj _dy back Zygote pullback p loss p u0 prob myparameters sensealg p_nn _dy gs back one _dy push! losses _dy epoch myparameters epochs epoch u0 prepare_initial myparameters dt myparameters numtrajplot pl test_loss visualize p_nn u0 prob myparameters println epoch test_loss display pl push! list_plots pl Flux Optimise update! opt p_nn gs println Controlling Stochastic Differential Equations In this tutorial we show how to use DiffEqFlux to control the time evolution of a system described by a stochastic differential equations SDE Specifically we consider a continuously monitored qubit described by an SDE in the Ito sense with multiplicative scalar noise see 1 for a reference dψ  b(ψ(t Ω(t))ψ(t dt  σ(ψ(t))ψ(t dW_t  We use a predictive model to map the quantum state of the qubit ψ(t at each time to the control parameter Ω(t which rotates the quantum state about the  x axis of the Bloch sphere to ultimately prepare and stabilize the qubit in the excited state Copy-Pasteable Code Before getting to the explanation here's some code to start with We will follow a full explanation of the definition and training process Output Step-by-step description Load packages Parameters We define the parameters of the qubit and hyper-parameters of the training process In plain terms the quantities that were defined are lr   learning rate of the optimizer epochs   number of epochs in the training process numtraj   number of simulated trajectories in the training process numtrajplot   number of simulated trajectories to visualize the performance dt   time step for solver initial  dt  if adaptive tinterval   time spacing between checkpoints tspan   time span Nintervals   number of checkpoints ts   discretization of the entire time interval used for  NoiseGrid Δ   detuning between the qubit and the laser Ωmax   maximum frequency of the control laser κ   decay rate C1   loss function hyper-parameter Controller We use a neural network to control the parameter Ω(t Alternatively one could also e.g use  tensor layers  Initial state We prepare  n_par  initial states uniformly distributed over the Bloch sphere To avoid complex numbers in our simulations we split the state of the qubit   ψ(t  c_e(t 1,0  c_d(t 0,1 into its real and imaginary part Defining the SDE We define the drift and diffusion term of the qubit The SDE doesn't preserve the norm of the quantum state To ensure the normalization of the state we add a  DiscreteCallback  after each time step Further we use a NoiseGrid from the  DiffEqNoiseProcess  package as one possibility to simulate a 1D Brownian motion Note that the NN is placed directly into the drift function thus the control parameter Ω is continuously updated Compute loss function We'd like to prepare the excited state of the qubit An appropriate choice for the loss function is the infidelity of the state ψ(t with respect to the excited state We create a parallelized  EnsembleProblem  where the  prob_func  creates a new  NoiseGrid  for every trajectory and loops over the initial states The number of parallel trajectories and the used batch size can be tuned by the kwargs  trajectories  and  batchsize  in the  solve  call See also  the   parallel ensemble simulation docs  for a description of the available ensemble algorithms To optimize only the parameters of the neural network we use  pars  p myparameters.Δ myparameters.Ωmax myparameters.κ Visualization To visualize the performance of the controller we plot the mean value and standard deviation of the fidelity of a bunch of trajectories  myparameters.numtrajplot  as a function of the time steps at which loss values are computed Training We use the  ADAM  optimizer to optimize the parameters of the neural network In each epoch we draw new initial quantum states compute the forward evolution and subsequently the gradients of the loss function with respect to the parameters of the neural network  sensealg  allows one to switch between the different  sensitivity modes   InterpolatingAdjoint  and  BacksolveAdjoint  are the two possible continuous adjoint sensitivity methods The necessary correction between Ito and Stratonovich integrals is computed under the hood in the DiffEqSensitivity package Evolution of the fidelity as a function of time References 1 Schäfer Frank Pavel Sekatski Martin Koppenhöfer Christoph Bruder and Michal Kloc Control of stochastic quantum dynamics by differentiable programming Machine Learning Science and Technology 2 no 3 2021 035004"},{"doctype":"documentation","id":"references/SciMLBase.has_expmv!","title":"has_expmv!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.has_inequality_constraints","title":"has_inequality_constraints","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.SystemStructures.VariableType","title":"VariableType","text":""},{"doctype":"documentation","id":"references/NeuralPDE.get_number","title":"get_number","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.find_derivatives!","title":"find_derivatives!","text":""},{"doctype":"documentation","id":"references/SciMLBase.expmv","title":"expmv","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.tunable_parameters","title":"tunable_parameters","text":"Get all parameters of  sys  that are marked as  tunable  Keyword argument  default  indicates whether variables without  tunable  metadata are to be considered tunable or not Create a tunable parameter by See also  getbounds   istunable"},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.eq_derivative!","title":"eq_derivative!","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.integrate_term","title":"integrate_term","text":""},{"doctype":"documentation","id":"references/PolyChaos.evaluate","title":"evaluate","text":"Univariate Evaluate the  n th univariate basis polynomial at point(s  x  The function is multiply dispatched to facilitate its use with the composite type  AbstractOrthoPoly If several basis polynomials stored in  ns  are to be evaluated at points  x  then call If  all  basis polynomials are to be evaluated at points  x  then call which returns an Array of dimensions  length(x),op.deg+1  Note n  is the degree of the univariate basis polynomial length(x  N  where  N  is the number of points a,b  are the recursion coefficients Multivariate Evaluate the n-th p-variate basis polynomial at point(s x The function is multiply dispatched to facilitate its use with the composite type  MultiOrthoPoly If several basis polynomials are to be evaluated at points  x  then call where  ind  is a matrix of multi-indices If  all  basis polynomials are to be evaluated at points  x  then call which returns an array of dimensions  mop.dim,size(x,1  Note n  is a multi-index length(n  p  i.e a p-variate basis polynomial size(x  N,p  where  N  is the number of points size(a)==size(b)=p "},{"doctype":"documentation","id":"references/SciMLBase.DiffEqScalar","title":"DiffEqScalar","text":"Represents a time-dependent scalar/scaling operator The update function is called by  update_coefficients  and is assumed to have the following signature"},{"doctype":"documentation","id":"references/ExponentialUtilities.expT!","title":"expT!","text":"Calculate the subspace exponential  exp(t*T  for a tridiagonal subspace matrix  T  with  α  on the diagonal and  β  on the super-/subdiagonal diagonalizing via  stegr "},{"doctype":"documentation","id":"references/ModelingToolkit.get_substitutions","title":"get_substitutions","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.SteadyStateProblemExpr","title":"SteadyStateProblemExpr","text":"Generates a Julia expression for building a SteadyStateProblem from an ODESystem and allows for automatically symbolically calculating numerical enhancements"},{"doctype":"documentation","id":"references/ModelingToolkit.get_current_var","title":"get_current_var","text":""},{"doctype":"document","id":"RecursiveArrayTools/array_types.md","title":"Recursive Array Types","text":"Recursive Array Types The Recursive Array types are types which implement an  AbstractArray  interface so that recursive arrays can be handled with standard array functionality For example wrapped arrays will automatically do things like recurse broadcast define optimized mapping and iteration functions and more Abstract Types Concrete Types"},{"doctype":"documentation","id":"references/Catalyst.reactioncomplexmap","title":"reactioncomplexmap","text":"Find each  ReactionComplex  within the specified system constructing a mapping from the complex to vectors that indicate which reactions it appears in as substrates and products Notes Each  ReactionComplex  is mapped to a vector of pairs with each pair having the form  reactionidx  ± 1  where  1  indicates the complex appears as a substrate and  1  as a product in the reaction with integer label  reactionidx "},{"doctype":"documentation","id":"references/Surrogates._nonDominatedSorting","title":"_nonDominatedSorting","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractDAESolution","title":"AbstractDAESolution","text":"DocStringExtensions.TypeDefinition"},{"doctype":"document","id":"Surrogates/secondorderpoly.md","title":"Second order polynomial tutorial","text":"Second order polynomial tutorial The square polynomial model can be expressed by  y  Xβ  ϵ  Where X is the matrix of the linear model augmented by adding 2d columns containing pair by pair product of variables and variables squared Sampling Building the surrogate Optimizing The optimization method successfully found the minima"},{"doctype":"document","id":"GlobalSensitivity/tutorials/juliacon21.md","title":"Global Sensitivity Analysis of the Lotka-Volterra model","text":"OrdinaryDiffEq Statistics CairoMakie f du u p t du p u p u u du p u p u u u0 tspan p prob f u0 tspan p t collect range stop length f1 p prob1 prob p p sol prob1 Tsit5 saveat t bounds reg_sens f1 bounds fig Figure resolution ax hm CairoMakie heatmap fig reg_sens partial_correlation figure resolution axis xticksvisible yticksvisible yticklabelsvisible xticklabelsvisible title Colorbar fig hm ax hm CairoMakie heatmap fig reg_sens standard_regression figure resolution axis xticksvisible yticksvisible yticklabelsvisible xticklabelsvisible title Colorbar fig hm fig StableRNGs _rng StableRNG morris_sens f1 bounds rng _rng fig Figure resolution scatter fig morris_sens means_star color green axis xticksvisible xticklabelsvisible title scatter fig morris_sens means_star color red axis xticksvisible xticklabelsvisible title fig sobol_sens f1 bounds N efast_sens f1 bounds fig Figure resolution barplot fig sobol_sens S1 color green axis xticksvisible xticklabelsvisible title ylabel barplot fig sobol_sens ST color green axis xticksvisible xticklabelsvisible ylabel barplot fig efast_sens S1 color red axis xticksvisible xticklabelsvisible title barplot fig efast_sens ST color red axis xticksvisible xticklabelsvisible fig fig Figure resolution barplot fig sobol_sens S1 color green axis xticksvisible xticklabelsvisible title ylabel barplot fig sobol_sens ST color green axis xticksvisible xticklabelsvisible ylabel barplot fig efast_sens S1 color red axis xticksvisible xticklabelsvisible title barplot fig efast_sens ST color red axis xticksvisible xticklabelsvisible fig N lb ub sampler A B N lb ub sampler sobol_sens_desmat f1 A B f_batch p prob_func prob i repeat prob p p i ensemble_prob prob prob_func prob_func sol ensemble_prob Tsit5 saveat t trajectories size p out zeros size p i size p out i mean sol i out i maximum sol i out sobol_sens_batch f_batch A B batch f1 A B f_batch A B batch f1 p prob1 prob p p sol prob1 Tsit5 saveat t sobol_sens f1 nboot bounds N fig Figure resolution ax hm CairoMakie scatter fig sobol_sens S1 end label markersize CairoMakie scatter! fig sobol_sens S1 end label markersize ax hm CairoMakie scatter fig sobol_sens S1 end label markersize CairoMakie scatter! fig sobol_sens S1 end label markersize ax hm CairoMakie scatter fig sobol_sens S1 end label markersize CairoMakie scatter! fig sobol_sens S1 end label markersize ax hm CairoMakie scatter fig sobol_sens S1 end label markersize CairoMakie scatter! fig sobol_sens S1 end label markersize title Label fig legend Legend fig ax Global Sensitivity Analysis of the Lotka-Volterra model The tutorial covers a workflow of using GlobalSensitivity.jl on the Lotka-Volterra differential equation We showcase how to use multiple GSA methods analyse their results and leverage Julia's parallelism capabilities to perform Global Sensitivity analysis at scale heatmapreg morrisscat sobolefastprey   sobolefastpred timeseriessobollv"},{"doctype":"document","id":"Integrals/tutorials/numerical_integrals.md","title":"Numerically Solving Integrals","text":"f x p sum sin x prob f ones ones sol prob reltol abstol Cubature Base Threads f dx x p Threads i size x dx i sum sin x i prob f ones ones batch sol prob CubatureJLh reltol abstol IntegralsCuba sol prob CubaCuhre reltol abstol Numerically Solving Integrals For basic multidimensional quadrature we can construct and solve a  IntegralProblem  If we would like to parallelize the computation we can use the batch interface to compute multiple points at once For example here we do allocation-free multithreading with Cubature.jl If we would like to compare the results against Cuba.jl's  Cuhre  method then the change is a one-argument change"},{"doctype":"documentation","id":"references/ModelingToolkit.generate_tgrad","title":"generate_tgrad","text":"sys dvs sys ps sys expression Val kwargs Generates a function for the time gradient of a system Extra arguments control the arguments to the internal  build_function  call"},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.deg","title":"deg","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.SystemStructures.quick_cancel_expr","title":"quick_cancel_expr","text":""},{"doctype":"document","id":"DiffEqSensitivity/pde_fitting/pde_constrained.md","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"DelimitedFiles Plots DifferentialEquations OptimizationPolyalgorithms OptimizationOptimJL Lx x Lx dx x x Nx size x u0 exp x p xtrs dx Nx dt dx t0 tMax dt tspan t0 tMax t t0 dt tMax ddx u dx zero eltype u u end u end dx zero eltype u d2dx u dx zero eltype u u end u end u end dx zero eltype u heat u p t a0 a1 p dx Nx xtrs a0 u a1 d2dx u dx prob heat u0 tspan p sol prob Tsit5 dt dt saveat t plot x sol u lw label size plot! x sol u end lw ls dash label ps predict θ Array prob Tsit5 p θ dt dt saveat t loss θ pred predict θ l predict θ sol sum abs2 l pred l pred loss ps size pred size sol size t LOSS PRED PARS cb θ l pred display l append! PRED pred append! LOSS l append! PARS θ cb ps loss ps scatter sol end label size plot! PRED end end lw label adtype optf x p loss x adtype optprob optf ps res optprob PolyOpt cb cb res u DelimitedFiles Plots DifferentialEquations Lx x Lx dx x x Nx size x u0 exp x p xtrs dx Nx dt dx t0 tMax dt tspan t0 tMax t t0 dt tMax ddx u dx zero eltype u u end u end dx zero eltype u d2dx u dx zero eltype u u end u end u end dx zero eltype u heat u p t a0 a1 p dx Nx xtrs a0 u a1 d2dx u dx prob heat u0 tspan p sol prob Tsit5 dt dt saveat t plot x sol u lw label size plot! x sol u end lw ls dash label ps predict θ Array prob Tsit5 p θ dt dt saveat t loss θ pred predict θ l predict θ sol sum abs2 l pred l pred loss ps size pred size sol size t LOSS PRED PARS cb θ l pred display l append! PRED pred append! LOSS l append! PARS θ cb ps loss ps scatter sol end label size plot! PRED end end lw label adtype optf x p loss x adtype optprob optf ps res optprob PolyOpt cb cb res u Partial Differential Equation PDE Constrained Optimization This example uses a prediction model to optimize the one-dimensional Heat Equation Step-by-step description below Step-by-step Description Load Packages Parameters First we setup the 1-dimensional space over which our equations will be evaluated  x  spans  from 0.0 to 10.0  in steps of  0.01   t  spans  from 0.00 to 0.04  in steps of  4.0e-5  In plain terms the quantities that were defined are x  to  Lx  spans the specified 1D space dx   distance between two points Nx   total size of space u0   initial condition p   true solution xtrs   convenient grouping of  dx  and  Nx  into Array dt   time distance between two points t   t0  to  tMax  spans the specified time frame tspan   span of  t Auxiliary Functions We then define two functions to compute the derivatives numerically The  Central   Difference  is used in both the 1st and 2nd degree derivatives Heat Differential Equation Next we setup our desired set of equations in order to define our problem Solve and Plot Ground Truth We then solve and plot our partial differential equation This is the true solution which we will compare to further on Building the Prediction Model Now we start building our prediction model to try to obtain the values  p  We make an initial guess for the parameters and name it  ps  here The  predict  function is a non-linear transformation in one layer using  solve  If unfamiliar with the concept refer to  here  Train Parameters Training our model requires a  loss function  an  optimizer  and a  callback   function  to display the progress Loss We first make our predictions based on the current values of our parameters  ps  then take the difference between the predicted solution and the truth above For the loss we use the  Mean squared error  Optimizer The optimizers  ADAM  with a learning rate of 0.01 and  BFGS  are directly passed in training see below Callback The callback function displays the loss during training We also keep a history of the loss the previous predictions and the previous parameters with  LOSS   PRED  and  PARS  accumulators Plotting Prediction vs Ground Truth The scatter points plotted here are the ground truth obtained from the actual solution we solved for above The solid line represents our prediction The goal is for both to overlap almost perfectly when the PDE finishes its training and the loss is close to 0 Train The parameters are trained using  Optimization.solve  and adjoint sensitivities The resulting best parameters are stored in  res  and  res.u  returns the parameters that minimizes the cost function We successfully predict the final  ps  to be equal to  0.999999999999975   1.0000000000000213  vs the true solution of  p    1.0 1.0 Expected Output"},{"doctype":"documentation","id":"references/Surrogates._wendland","title":"_wendland","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.add_kwonly","title":"add_kwonly","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timepoint_median","title":"timepoint_median","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.SigmoidKernel","title":"SigmoidKernel","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.default_u0","title":"default_u0","text":""},{"doctype":"documentation","id":"references/PolyChaos.OrthoPoly","title":"OrthoPoly","text":""},{"doctype":"documentation","id":"references/RecursiveArrayTools.issymbollike","title":"issymbollike","text":""},{"doctype":"documentation","id":"references/Catalyst.get_rx_species","title":"get_rx_species","text":""},{"doctype":"documentation","id":"references/NeuralOperators.apply_pattern","title":"apply_pattern","text":""},{"doctype":"documentation","id":"references/MethodOfLines.DiscreteSpace","title":"DiscreteSpace","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.find_masked_pivot","title":"find_masked_pivot","text":""},{"doctype":"document","id":"SciMLBase/interfaces/Differentiation.md","title":"[Automatic Differentiation and Sensitivity Algorithms (Adjoints)]( sensealg)","text":"prob args sensealg nothing u0 nothing p nothing kwargs u0 u0 nothing u0 prob u0 p p nothing p prob p sensealg nothing haskey prob kwargs sensealg sensealg prob kwargs sensealg solve_up prob sensealg u0 p args kwargs ChainRulesCore frule typeof solve_up prob sensealg Union Nothing u0 p args kwargs _solve_forward prob sensealg u0 p args kwargs ChainRulesCore rrule typeof solve_up prob sensealg Union Nothing u0 p args kwargs _solve_adjoint prob sensealg u0 p args kwargs _concrete_solve_adjoint args kwargs error _concrete_solve_forward args kwargs error Automatic Differentiation and Sensitivity Algorithms Adjoints  sensealg Automatic differentiation control is done through the  sensealg  keyword argument Hooks exist in the high level interfaces for  solve  which shuttle the definitions of automatic differentiation overloads to dispatches defined in DiffEqSensitivity.jl should be renamed SciMLSensitivity.jl as it expands This is done by first entering a top-level  solve  definition for example solve_up  then drops down the differentiable arguments as positional arguments which is required for the  ChainRules.jl  interface Then the  ChainRules  overloads are written on the  solve_up  calls like Default definitions then exist to throw an informative error if the sensitivity mechanism is not added The sensitivity mechanism is kept in a separate package because of the high dependency and load time cost introduced by the automatic differentiation libraries Different choices of automatic differentiation are then selected by the  sensealg  keyword argument in  solve  which is made into a positional argument in the  solve_adjoint  and other functions in order to allow dispatch SensitivityADPassThrough The special sensitivity algorithm  SensitivityADPassThrough  is used to ignore the internal sensitivity dispatches and instead do automatic differentiation directly through the solver Generally this  sensealg  is only used internally Note about ForwardDiff ForwardDiff does not use ChainRules.jl and thus it completely ignores the special handling"},{"doctype":"documentation","id":"references/SciMLOperators.has_adjoint","title":"has_adjoint","text":""},{"doctype":"documentation","id":"references/RecursiveArrayTools.copyat_or_push!","title":"copyat_or_push!","text":"T a AbstractVector T i Int x If  i<length(x  it's simply a  recursivecopy  to the  i th element Otherwise it will  push  a  deepcopy "},{"doctype":"documentation","id":"references/ModelingToolkit._varmap_to_vars","title":"_varmap_to_vars","text":""},{"doctype":"documentation","id":"references/Surrogates.InverseDistanceSurrogate","title":"InverseDistanceSurrogate","text":"mutable struct InverseDistanceSurrogate  AbstractSurrogate The inverse distance weighting model is an interpolating method and the unknown points are calculated with a weighted average of the sampling points p is a positive real number called the power parameter p  1 is needed for the derivative to be continuous"},{"doctype":"documentation","id":"references/NeuralPDE.parse_equation","title":"parse_equation","text":""},{"doctype":"documentation","id":"references/NeuralOperators.FourierTransform","title":"FourierTransform","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.tearing_sub","title":"tearing_sub","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.tearing_substitution","title":"tearing_substitution","text":""},{"doctype":"documentation","id":"references/ModelingToolkit._eq_unordered","title":"_eq_unordered","text":""},{"doctype":"documentation","id":"references/NeuralOperators.SpectralConv","title":"SpectralConv","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractSteadyStateProblem","title":"AbstractSteadyStateProblem","text":""},{"doctype":"documentation","id":"references/SciMLBase.calculate_ensemble_errors","title":"calculate_ensemble_errors","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractSciMLOperator","title":"AbstractSciMLOperator","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/Catalyst.bwd_arrows","title":"bwd_arrows","text":""},{"doctype":"documentation","id":"references/SciMLOperators.expmv","title":"expmv","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.Bisection","title":"Bisection","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractSteadyStateIntegrator","title":"AbstractSteadyStateIntegrator","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.WienerProcess!","title":"WienerProcess!","text":"t0 W0 Z0 nothing kwargs t0 W0 Z0 nothing kwargs The  WienerProcess  also known as Brownian motion or the noise in the Langevin equation is the stationary process with white noise increments and a distribution  N(0,dt  The constructor is"},{"doctype":"document","id":"NeuralPDE/examples/blackscholes.md","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"d X0 repeat div d tspan r sigma f X u σᵀ∇u p t r u sum X σᵀ∇u g X sum X μ_f X p t zero X σ_f X p t Diagonal sigma X prob g f μ_f σ_f X0 tspan hls d opt Flux ADAM u0 Flux Dense d hls relu Dense hls hls relu Dense hls σᵀ∇u Flux Dense d hls relu Dense hls hls relu Dense hls hls relu Dense hls d pdealg u0 σᵀ∇u opt opt ans prob pdealg verbose maxiters trajectories alg EM dt pabstol Solving the 100-dimensional Black-Scholes-Barenblatt Equation Black Scholes equation is a model for stock option price In 1973 Black and Scholes transformed their formula on option pricing and corporate liabilities into a PDE model which is widely used in financing engineering for computing the option price over time 1 In this example we will solve a Black-Scholes-Barenblatt equation of 100 dimensions The Black-Scholes-Barenblatt equation is a nonlinear extension to the Black-Scholes equation which models uncertain volatility and interest rates derived from the Black-Scholes equation This model results in a nonlinear PDE whose dimension is the number of assets in the portfolio To solve it using the  TerminalPDEProblem  we write As described in the API docs we now need to define our  NNPDENS  algorithm by giving it the Flux.jl chains we want it to use for the neural networks  u0  needs to be a  d dimensional  1-dimensional chain while  σᵀ∇u  needs to be  d+1 dimensional to  d  dimensions Thus we define the following And now we solve the PDE Here we say we want to solve the underlying neural SDE using the Euler-Maruyama SDE solver with our chosen  dt=0.2  do at most 150 iterations of the optimizer 100 SDE solves per loss evaluation for averaging and stop if the loss ever goes below  1f-6  Reference Shinde A S and K C Takale Study of Black-Scholes model and its applications Procedia Engineering 38 2012 270-279"},{"doctype":"documentation","id":"references/DiffEqSensitivity.generate_callbacks","title":"generate_callbacks","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.convert_tspan","title":"convert_tspan","text":""},{"doctype":"documentation","id":"references/Catalyst.graph_attrs","title":"graph_attrs","text":""},{"doctype":"documentation","id":"references/PolyChaos.w_laguerre","title":"w_laguerre","text":""},{"doctype":"documentation","id":"references/Catalyst.prodstoichmat","title":"prodstoichmat","text":"Returns the product stoichiometry matrix  P  with  P_{i j  the stoichiometric coefficient of the ith product within the jth reaction Note Set sparse=true for a sparse matrix representation"},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.is_not_pox","title":"is_not_pox","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.ReverseLossCallback","title":"ReverseLossCallback","text":""},{"doctype":"documentation","id":"references/Catalyst.VariableConstantSpecies","title":"VariableConstantSpecies","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractDiffEqInterpolation","title":"AbstractDiffEqInterpolation","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/ExponentialUtilities.get_cache","title":"get_cache","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.tearing_reassemble","title":"tearing_reassemble","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.preface","title":"preface","text":""},{"doctype":"documentation","id":"references/SciMLBase.SplitODEProblem","title":"SplitODEProblem","text":"f u0 tspan p kwargs f1 f2 u0 tspan p kwargs Defines a split ordinary differential equation ODE problem Documentation Page https://diffeq.sciml.ai/stable/types/split ode types Mathematical Specification of a Split ODE Problem To define a  SplitODEProblem  you simply need to give a two functions   f_1  and  f_2  along with an initial condition  u_0  which define an ODE frac{du}{dt   f_1(u,p,t  f_2(u,p,t f  should be specified as  f(u,p,t  or in-place as  f(du,u,p,t  and  u₀  should be an AbstractArray or number whose geometry matches the desired geometry of  u  Note that we are not limited to numbers or vectors for  u₀  one is allowed to provide  u₀  as arbitrary matrices  higher dimension tensors as well Many splits are at least partially linear That is the equation frac{du}{dt   Au  f_2(u,p,t For how to define a linear function  A  see the documentation for the  DiffEqOperators  Constructors The  isinplace  parameter can be omitted and will be determined using the signature of  f2  Note that both  f1  and  f2  should support the in-place style if  isinplace  is  true  or they should both support the out-of-place style if  isinplace  is  false  You cannot mix up the two styles Parameters are optional and if not given then a  NullParameters  singleton will be used which will throw nice errors if you try to index non-existent parameters Any extra keyword arguments are passed on to the solvers For example if you set a  callback  in the problem then that  callback  will be added in every solve call Under the hood a  SplitODEProblem  is just a regular  ODEProblem  whose  f  is a  SplitFunction  Therefore you can solve a  SplitODEProblem  using the same solvers for  ODEProblem  For solvers dedicated to split problems see Split ODE Solvers  split_ode_solve For specifying Jacobians and mass matrices see the DiffEqFunctions  performance_overloads page Fields f1   f2  The functions in the ODE u0  The initial condition tspan  The timespan for the problem p  The parameters for the problem Defaults to  NullParameters kwargs  The keyword arguments passed onto the solves DocStringExtensions.MethodSignatures Define a split ODE problem from a  SplitFunction "},{"doctype":"documentation","id":"references/ModelingToolkit.check_eqs_u0","title":"check_eqs_u0","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.RODEUDerivativeWrapper","title":"RODEUDerivativeWrapper","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.get_tprev","title":"get_tprev","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_connector_type","title":"get_connector_type","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.RobinBC","title":"RobinBC","text":"l  and  r  are the BC coefficients i.e  αl βl γl  and  αl βl γl  tuples and vectors work and correspond to BCs of the form αl  u  βl  u  γl   αr  u  βr  u  γr imposed on the lower  l  and higher  r  index boundaries respectively RobinBC  implements a Robin boundary condition operator  Q  that acts on a vector to give an extended vector as a result see https://github.com/JuliaDiffEq/DiffEqOperators.jl/files/3267835/ghost_node.pdf Write vector b̄₁ as a vertical concatenation with b0 and the rest of the elements of b̄₁ denoted b̄ ₁ the same with ū into u0 and ū  b̄ ₁  b̄ 2  fill(β/Δx length(stencil)-1 Pull out the product of u0 and b0 from the dot product The stencil used to approximate u is denoted s b0  α+(β/Δx)*s[1 Rearrange terms to find a general formula for u0 b̄ ₁̇⋅ū b0  γ/b0 which is dependent on ū  the robin coefficients and Δx The non-identity part of Qa is qa b`₁/b0  β s[2:end]/(α+β s[1]/Δx The constant part is Qb  γ/(α+β*s[1]/Δx do the same at the other boundary amounts to a flip of s[2:end with the other set of boundary coefficients"},{"doctype":"documentation","id":"references/PolyChaos.integrate","title":"integrate","text":"integrate function  f  using quadrature rule specified via  nodes   weights  For example  int_0^1 6x^5  1  can be solved as follows Note function  f  is assumed to return a scalar interval of integration is hidden in  nodes "},{"doctype":"documentation","id":"references/DiffEqFlux.isgpu","title":"isgpu","text":""},{"doctype":"documentation","id":"references/MethodOfLines.generate_spherical_diffusion_rules","title":"generate_spherical_diffusion_rules","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.SchurLU","title":"SchurLU","text":""},{"doctype":"documentation","id":"references/PolyChaos.w_hermite","title":"w_hermite","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.SystemStructures.require_complete","title":"require_complete","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.EquationSolveError","title":"EquationSolveError","text":""},{"doctype":"document","id":"LabelledArrays/Example_dsl.md","title":"Example: Nice DiffEq Syntax Without A DSL","text":"OrdinaryDiffEq lorenz_f! du u p t du x p σ u y u x du y u x p ρ u z u y du z u x u y p β u z u0 x y z p σ ρ β tspan prob lorenz_f! u0 tspan p sol prob Tsit5 sol x LorenzVector x y z LorenzParameterVector σ ρ β f u p t x p σ u y u x y u x p ρ u z u y z u x u y p β u z LorenzVector x y z u0 LorenzVector p LorenzParameterVector tspan prob f u0 tspan p sol prob Tsit5 Example Nice DiffEq Syntax Without A DSL Users of the SciML ecosystem are often solving large models with complicated states and/or hundreds or thousands of parameters These models are implemented using arrays and those arrays have traditionally been indexed by integers such as  p[1  or  p[1:5  Numerical indexing is wonderful for small models but can quickly cause problems as models become bigger It is easy to forget which index corresponds to which reaction rate or which diffusion coeffient This confusion can lead to difficult to debug problems in a user's code  LabelledArrays  can make an important difference here It is much easier to build a model using parameter references such as  p.rate_nacl  or  p.probability_birth  instead of  p[26  or  p[1026  Labelled arrays make both the development and debugging of models much faster LabelledArrays.jl are a way to get DSL-like syntax without a macro In this case we can solve differential equations with labelled components by making use of labelled arrays and always refer to the components by name instead of index One key caveat is that users do not need to sacrifice performance when using labelled arrays Labelled arrays are as performant as traditional numerically indexed arrays Let's solve the Lorenz equation using an  LVector s  LVectors  are mutable hence we can use the non-allocating form of the  OrdinaryDiffEq  API In the example above we used an  LArray  to define the intial state  u0  as well as the parameter vector  p  The reminder of the ODE solution steps are are no different that the original  DifferentialEquations   tutorials  Alternatively we can use an immutable  SLVector  to implement the same equation In this case we need to use the allocating form of the  OrdinaryDiffEq  API when defining our model equation"},{"doctype":"documentation","id":"references/ModelingToolkit.same_or_inner_namespace","title":"same_or_inner_namespace","text":"Determine whether or not  var  is in the same namespace as  u  or a namespace internal to the namespace of  u  Example  sys.u  sys.inner.u  will bind  sys.inner.u  but  sys.u  remains an unbound external signal The namepsaced signal  sys.inner.u  lives in a namspace internal to  sys "},{"doctype":"documentation","id":"references/SciMLBase.LinearInterpolation","title":"LinearInterpolation","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.SelectedState","title":"SelectedState","text":""},{"doctype":"documentation","id":"references/PolyChaos.MeixnerPollaczekOrthoPoly","title":"MeixnerPollaczekOrthoPoly","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity._extract","title":"_extract","text":""},{"doctype":"document","id":"DiffEqSensitivity/training_tips/divergence.md","title":"Handling Divergent and Unstable Trajectories","text":"loss p tmp_prob prob p p tmp_sol tmp_prob Tsit5 saveat tmp_sol retcode Success sum abs2 Array tmp_sol dataset Inf DifferentialEquations OptimizationOptimJL Plots lotka_volterra! du u p t rab wol u α β γ δ p du drab α rab β rab wol du dwol γ rab wol δ wol nothing u0 tspan p prob lotka_volterra! u0 tspan p sol prob saveat plot sol dataset Array sol scatter! sol t dataset tmp_prob prob p tmp_sol tmp_prob plot tmp_sol scatter! sol t dataset loss p tmp_prob prob p p tmp_sol tmp_prob Tsit5 saveat tmp_sol retcode Success sum abs2 Array tmp_sol dataset Inf pinit adtype optf x p loss x adtype optprob optf pinit res optprob ADAM maxiters res optprob Newton loss p tmp_prob prob p p tmp_sol tmp_prob Tsit5 saveat size tmp_sol size dataset sum abs2 Array tmp_sol dataset Inf Handling Divergent and Unstable Trajectories It is not uncommon for a set of parameters in an ODE model to simply give a divergent trajectory If the rate of growth compounds and outpaces the rate of decay you will end up at infinity in finite time This it is not uncommon to see divergent trajectories in the optimization of parameters as many times an optimizer can take an excursion into a parameter regime which simply gives a model with an infinite solution This can be addressed by using the retcode system In DifferentialEquations.jl  RetCodes  detail the status of the returned solution Thus if the retcode corresponds to a failure we can use this to give an infinite loss and effectively discard the parameters This is shown in the loss function A full example making use of this trick is You might notice that  AutoZygote  default fails for the above  Optimization.solve  call with Optim's optimizers which happens because of Zygote's behaviour for zero gradients in which case it returns  nothing  To avoid such issue you can just use a different version of the same check which compares the size of the obtained solution and the data we have shown below which is easier to AD"},{"doctype":"documentation","id":"references/Catalyst.assemble_diffusion","title":"assemble_diffusion","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.Falsi","title":"Falsi","text":""},{"doctype":"documentation","id":"references/Surrogates.KrigingStructure","title":"KrigingStructure","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.Retcode","title":"Retcode","text":""},{"doctype":"document","id":"Catalyst/tutorials/dsl.md","title":"The Reaction DSL","text":"rn X Y XY XY Z1 Z2 osys convert rn t X t Y t Z t XY t Z1 t Z2 t u0 X Y XY Z1 Z2 u0 rn X Y XY Z1 Z2 oprob osys u0 tspan u0 X Y XY Z1 Z2 oprob rn u0 tspan rn p ∅ X d X ∅ p d rn X X ∅ rn X X2 rn X Y Z XY2Z2 rn X Y Z XY2Z2 rn X Y XY X Y XY XY X Y XY X Y rn X Y XY X Y XY rn X Y XY rn X Y XY X Y XY rn1 S P1 P2 rn2 S P1 S P2 rn1 S1 S2 P rn2 S1 P S2 P rn1 S1 S2 S3 P1 P2 P3 rn2 S1 P1 S2 P2 S3 P3 rn1 S P1 P2 rn2 S P1 S P2 P1 S P2 S rn k X Y ∅ k rn k X Y X k SpecialFunctions rn X X Y t gamma Y X ∅ pi X Y Y ∅ rn production_degradation p ∅ X d X ∅ p d nameof rn production_degradation rn1 X v K n ∅ X v X n X n K n ∅ X v K n rn2 X v K ∅ X v X X K ∅ X v K rn1 X v K n ∅ X v K n X n K n ∅ X v K n rn2 X v K ∅ X v K X K ∅ X v K k t A t spec A rate k A name network rn name rate B spec B spec C rn The Reaction DSL This tutorial covers some of the basic syntax for building chemical reaction network models using Catalyst's domain specific language DSL Examples showing how to both construct and solve ODE SDE and jump models are provided in  Basic   Chemical Reaction Network Examples  To learn more about the symbolic  ReactionSystem s generated by the DSL and how to use them directly see the tutorial on  Programmatic Construction of Symbolic Reaction Systems  Basic syntax The  reaction_network  macro allows the symbolic specification of reaction networks with a simple format Its input is a set of chemical reactions and from them it generates a symbolic  ReactionSystem  reaction network object The  ReactionSystem  can be used as input to ModelingToolkit  ODEProblem   NonlinearProblem   SteadyStateProblem   SDEProblem   JumpProblem  and more  ReactionSystem s can also be incrementally extended as needed allowing for programmatic construction of networks and network composition The basic syntax is where each line corresponds to a chemical reaction Each reaction consists of a reaction rate the expression on the left hand side of     a set of substrates the expression in-between    and    and a set of products the expression on the right hand side of    The substrates and the products may contain one or more reactants separated by    The naming convention for these are the same as for normal variables in Julia The chemical reaction model is generated by the  reaction_network  macro and stored in the  rn  variable a normal Julia variable which does not need to be called  rn  It corresponds to a  ReactionSystem  a symbolic representation of the chemical network  The generated  ReactionSystem  can be converted to a symbolic differential equation model via We can then convert the symbolic ODE model into a compiled optimized representation for use in the SciML ODE solvers by constructing an  ODEProblem  Creating an  ODEProblem  also requires our specifying the initial conditions for the model We do this by creating a mapping from each symbolic variable representing a chemical species to its initial value Alternatively we can create a mapping use Julia  Symbol s for each variable and then convert them to a mapping involving symbolic variables like Given the mapping we can then create an  ODEProblem  from our symbolic  ODESystem Catalyst provides a shortcut to avoid having to explicitly  convert  to an  ODESystem  and/or use  symmap_to_varmap  allowing direct construction of the  ODEProblem  like For more detailed examples see the  Basic Chemical Reaction Network   Examples  The generated differential equations use the law of mass action For the above example the ODEs are then frac{d[X]}{dt  2 X Y]\\\\\n\\frac{d[Y]}{dt  2 X Y]\\\\\n\\frac{d[XY]}{dt  2 X Y  XY]\\\\\n\\frac{d[Z1]}{dt XY]\\\\\n\\frac{d[Z2]}{dt  XY Defining parameters and species Parameter values do not need to be set when the model is created Components can be designated as symbolic parameters by declaring them at the end Parameters can only exist in the reaction rates where they can be mixed with reactants All variables not declared after  end  will be treated as a chemical species Production Destruction and Stoichiometry Sometimes reactants are produced/destroyed from/to nothing This can be designated using either  0  or  ∅  If several molecules of the same reactant are involved in a reaction the stoichiometry of a reactant in a reaction can be set using a number Here two molecules of species  X  form the dimer  X2  this corresponds to the differential equation frac{d[X]}{dt  X]^2\\\\\n\\frac{d[X2]}{dt  frac{1}{2 X]^2 Other numbers than 2 can be used and parenthesis can be used to reuse the same stoichiometry for several reactants Note one can explicitly multiply by integer coefficients too i.e Arrow variants A variety of unicode arrows are accepted by the DSL in addition to    All of these work      →   ↣   ↦   ⇾   ⟶   ⟼   ⥟   ⥟   ⇀   ⇁  Backwards arrows can also be used to write the reaction in the opposite direction For example these reactions are equivalent Bi-directional arrows for reversible reactions Bi-directional arrows including bidirectional unicode arrows like ↔ can be used to designate a reversible reaction For example these two models are equivalent If the reaction rates in the backward and forward directions are different they can be designated in the following way which is identical to Combining several reactions in one line Several similar reactions can be combined in one line by providing a tuple of reaction rates and/or substrates and/or products If several tuples are provided they must all be of identical length These pairs of reaction networks are all identical This can also be combined with bi-directional arrows in which case separate tuples can be provided for the backward and forward reaction rates These reaction networks are identical Variable reaction rates Reaction rates do not need to be a single parameter or a number but can also be expressions depending on time or the current concentration of other species when for example one species can activate the production of another For instance this is a valid notation and will have  Y  degraded at rate frac{d[Y]}{dt  k[X][Y Note this is actually equivalent to the reaction in the resulting generated ODESystem  however  the latter  Reaction  will be classified as  ismassaction  and the former will not which can impact optimizations used in generating  JumpSystem s For this reason it is recommended to use the latter representation when possible Most expressions and functions are valid reaction rates e.g where here  t  always denotes Catalyst's time variable Please note that many user-defined functions can be called directly but others will require registration with Symbolics.jl see the faq  user_functions Naming the generated  ReactionSystem ModelingToolkit uses system names to allow for compositional and hierarchical models To specify a name for the generated  ReactionSystem  via the  reaction_network  macro just place the name before  begin  Pre-defined functions Hill functions and a Michaelis-Menten function are pre-defined and can be used as rate laws Below the pair of reactions within  rn1  are equivalent as are the pair of reactions within  rn2  Repressor Hill  hillr  and Michaelis-Menten  mmr  functions are also provided Please see the API  Rate Laws  section for more details Interpolation of Julia Variables The DSL allows Julia variables to be interpolated for the network name within rate constant expressions or for species within the reaction For example gives with giving As the parameter  k  was pre-defined and appears via interpolation we did not   need to declare it at the end of the  reaction_network  macro Note when using interpolation expressions like  2$spec  won't work the multiplication symbol must be explicitly included like  2*$spec "},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.get_timepoint","title":"get_timepoint","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.Wedges","title":"Wedges","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.split_states","title":"split_states","text":""},{"doctype":"documentation","id":"references/Catalyst.make_hill_exp","title":"make_hill_exp","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timepoint_meancov","title":"timepoint_meancov","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity._adjoint_sensitivities","title":"_adjoint_sensitivities","text":""},{"doctype":"document","id":"Integrals/basics/solve.md","title":"Common Solver Options (Solve Keyword Arguments)","text":"Common Solver Options Solve Keyword Arguments reltol  Relative tolerance abstol  Absolute tolerance maxiters  The maximum number of iterations Additionally the extra keyword arguments are splatted to the library calls so see the documentation of the integrator library for all of the extra details These extra keyword arguments are not guaranteed to act uniformly"},{"doctype":"documentation","id":"references/SciMLBase.AbstractDiscreteProblem","title":"AbstractDiscreteProblem","text":"DocStringExtensions.TypeDefinition Base for types which define discrete problems"},{"doctype":"documentation","id":"references/DiffEqFlux._hamiltonian_forward","title":"_hamiltonian_forward","text":""},{"doctype":"documentation","id":"references/ModelingToolkit._check_operator_variables","title":"_check_operator_variables","text":"Check if difference/derivative operation occurs in the R.H.S of an equation"},{"doctype":"documentation","id":"references/DiffEqFlux.ffjord","title":"ffjord","text":""},{"doctype":"documentation","id":"references/Catalyst.hillar_names","title":"hillar_names","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.SystemStructures.DiffGraph","title":"DiffGraph","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.with_connector_type","title":"with_connector_type","text":""},{"doctype":"document","id":"Integrals/basics/FAQ.md","title":"Frequently Asked Questions","text":"Frequently Asked Questions Ask more questions"},{"doctype":"documentation","id":"references/ModelingToolkit.get_Wfact","title":"get_Wfact","text":""},{"doctype":"documentation","id":"references/Catalyst.assemble_jumps","title":"assemble_jumps","text":""},{"doctype":"documentation","id":"references/Catalyst.reactions","title":"reactions","text":"Given a  ReactionSystem  return a vector of all  Reactions  in the system Notes If  ModelingToolkit.get_systems(network  is not empty will allocate"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.VirtualBrownianTree","title":"VirtualBrownianTree","text":"t0 W0 Z0 nothing dist bridge kwargs W0 zeros W W0 tree_depth prob W sol prob dt A  VirtualBrownianTree  builds the noise process starting from an initial time  t0  the first value of the proces  W0  and optionally the first value  Z0  for an auxiliary pseudo-process The constructor is given as where  dist  specifies the distribution that is used to generate the end point(s  Wend   Zend  of the noise process for the final time  tend   bridge  denotes the distribution of the employed Brownian bridge  Per default  tend  is fixed to  t0+1  but can be changed by passing a custom  tend  as a keyword argument The following keyword arguments are available tend  is the end time of the noise process Wend  is the end value of the noise process Zend  is the end value of the pseudo-noise process atol  represents the absolute tolerance determining when the recursion is terminated tree_depth  allows one to store a cache of seeds noise values and times to speed up the simulation by reducing the recursion steps search_depth  maximal search depth for the tree if  atol  is not reached rng  the splittable PRNG used for generating the random numbers Default  Threefry4x  from the Random123 package VirtualBrownianTree Example In this example we define a multi-dimensional Brownian process based on a  VirtualBrownianTree  with a minimal  tree_depth=0  such that memory consumption is minimized Using a look-up cache by increasing  tree_depth  can significantly reduce the runtime Thus the  VirtualBrownianTree  allows for trading off speed for memory in a simple manner"},{"doctype":"documentation","id":"references/ModelingToolkit.variable_dependencies","title":"variable_dependencies","text":"sys variables sys variablestoids nothing odesys For each variable determine the equations that modify it and return as a  BipartiteGraph  Notes Dependencies are returned as a  BipartiteGraph  mapping variable indices to the indices of equations that modify them variables  denotes the list of variables to determine dependencies for variablestoids  denotes a  Dict  mapping  Variable s to their  Int  index in  variables  Example Continuing the example of  equation_dependencies"},{"doctype":"documentation","id":"references/ModelingToolkit.has_connections","title":"has_connections","text":""},{"doctype":"documentation","id":"references/SciMLBase.plot_indices","title":"plot_indices","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_ivs","title":"get_ivs","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.resize_stack!","title":"resize_stack!","text":""},{"doctype":"documentation","id":"references/SciMLBase","title":"SciMLBase","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.CompoundPoissonProcess!","title":"CompoundPoissonProcess!","text":"https://www.math.wisc.edu/~anderson/papers/AndPostleap.pdf Incorporating postleap checks in tau-leaping J Chem Phys 128 054103 2008 https://doi.org/10.1063/1.2819665"},{"doctype":"documentation","id":"references/GlobalSensitivity.ff_main_effects","title":"ff_main_effects","text":""},{"doctype":"documentation","id":"references/GlobalSensitivity._calculate_standard_regression_coefficients","title":"_calculate_standard_regression_coefficients","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.KEEP","title":"KEEP","text":""},{"doctype":"documentation","id":"references/LinearSolve.AbstractSolveFunction","title":"AbstractSolveFunction","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.factors","title":"factors","text":""},{"doctype":"documentation","id":"references/Surrogates.sample","title":"sample","text":""},{"doctype":"documentation","id":"references/NeuralPDE.QuadratureTraining","title":"QuadratureTraining","text":"quadrature_alg  quadrature algorithm reltol  relative tolerance abstol  absolute tolerance maxiters  the maximum number of iterations in quadrature algorithm batch  the preferred number of points to batch For more information look Quadrature.jl https://github.com/SciML/Quadrature.jl"},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.test_point","title":"test_point","text":""},{"doctype":"documentation","id":"references/Surrogates._calc_loba_coeff1D","title":"_calc_loba_coeff1D","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.EXACT_SOLUTION_LEFT","title":"EXACT_SOLUTION_LEFT","text":""},{"doctype":"document","id":"LinearSolve/advanced/custom.md","title":"Passing in a Custom Linear Solver","text":"LinearAlgebra my_linsolve A b u p newA Pl Pr solverdata verbose kwargs verbose println u A b u prob Diagonal rand rand alg my_linsolve sol prob alg my_linsolve! A b u p newA Pl Pr solverdata verbose kwargs verbose println u A b u alg my_linsolve! sol prob alg Passing in a Custom Linear Solver Julia users are building a wide variety of applications in the SciML ecosystem often requiring problem-specific handling of their linear solves As existing solvers in  LinearSolve.jl  may not be optimally suited for novel applications it is essential for the linear solve interface to be easily extendable by users To that end the linear solve algorithm  LinearSolveFunction  accepts a user-defined function for handling the solve A user can pass in their custom linear solve function say  my_linsolve  to  LinearSolveFunction  A contrived example of solving a linear system with a custom solver is below The inputs to the function are as follows A  the linear operator b  the right-hand-side u  the solution initialized as  zero(b  p  a set of parameters newA  a  Bool  which is  true  if  A  has been modified since last solve Pl  left-preconditioner Pr  right-preconditioner solverdata  solver cache set to  nothing  if solver hasn't been initialized kwargs  standard SciML keyword arguments such as  verbose   maxiters   abstol   reltol The function  my_linsolve  must accept the above specified arguments and return the solution  u  As memory for  u  is already allocated the user may choose to modify  u  in place as follows"},{"doctype":"document","id":"MethodOfLines/tutorials/heatss.md","title":"Steady State Heat Equation - No Time Dependance - NonlinearProblem","text":"Steady State Heat Equation  No Time Dependance  NonlinearProblem Sometimes it is desirable to solve an equation that has no time evolution such as the steady state heat equation"},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.candidate_sqrt","title":"candidate_sqrt","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.applychain","title":"applychain","text":""},{"doctype":"document","id":"DiffEqSensitivity/ode_fitting/optimization_ode.md","title":"Optimization of Ordinary Differential Equations","text":"DifferentialEquations OptimizationPolyalgorithms OptimizationOptimJL Plots lotka_volterra! du u p t x y u α β δ γ p du dx α x β x y du dy δ y γ x y u0 tspan tsteps p prob lotka_volterra! u0 tspan p sol prob Tsit5 Plots plot sol savefig loss p sol prob Tsit5 p p saveat tsteps loss sum abs2 sol loss sol callback p l pred display l plt plot pred ylim display plt adtype optf x p loss x adtype optprob optf p result_ode optprob PolyOpt cb callback maxiters DifferentialEquations OptimizationPolyalgorithms OptimizationOptimJL Plots lotka_volterra! du u p t x y u α β δ γ p du dx α x β x y du dy δ y γ x y u0 tspan tsteps p prob lotka_volterra! u0 tspan p sol prob Tsit5 Plots plot sol savefig loss p sol prob Tsit5 p p saveat tsteps loss sum abs2 sol loss sol callback p l pred display l plt plot pred ylim display plt adtype optf x p loss x adtype optprob optf p result_ode optprob PolyOpt cb callback maxiters remade_solution prob p result_ode u Tsit5 saveat tsteps plot remade_solution ylim result_ode optprob ADAM cb callback maxiters Optimization of Ordinary Differential Equations Copy-Paste Code If you want to just get things running try the following Explanation will follow Explanation First let's create a Lotka-Volterra ODE using DifferentialEquations.jl For more details  see the DifferentialEquations.jl documentation  The Lotka-Volterra equations have the form begin{aligned}\n\\frac{dx}{dt  alpha x  beta x y      \n\\frac{dy}{dt  delta y  gamma x y    \n\\end{aligned LV Solution Plot For this first example we do not yet include a neural network We take  AD-compatible  solve   function  function that takes the parameters and an initial condition and returns the solution of the differential equation Next we choose a loss function Our goal will be to find parameters that make the Lotka-Volterra solution constant  x(t)=1  so we define our loss as the squared distance from 1 Lastly we use the  Optimization.solve  function to train the parameters using  ADAM  to arrive at parameters which optimize for our goal  Optimization.solve  allows defining a callback that will be called at each step of our training loop It takes in the current parameter vector and the returns of the last call to the loss function We will display the current loss and make a plot of the current situation Let's optimize the model In just seconds we found parameters which give a relative loss of  1e-16  We can get the final loss with  result_ode.minimum  and get the optimal parameters with  result_ode.u  For example we can plot the final outcome and show that we solved the control problem and successfully found parameters to make the ODE solution constant Final plot Note that this was done with the default optimizer One can also pass an optimization method like  ADAM(0.1  and tweak settings like set  maxiters=100  to force at most 100 iterations of the optimization This looks like"},{"doctype":"documentation","id":"references/ModelingToolkit.retime_dvs","title":"retime_dvs","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.todict","title":"todict","text":""},{"doctype":"documentation","id":"references/Catalyst.tup_leng","title":"tup_leng","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_postprocess_fbody","title":"get_postprocess_fbody","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.istunable","title":"istunable","text":"Determine whether or not symbolic variable  x  is marked as a tunable for an automatic tuning algorithm default  indicates whether variables without  tunable  metadata are to be considered tunable or not Create a tunable parameter by See also  tunable_parameters   getbounds"},{"doctype":"documentation","id":"references/SciMLBase.add_labels!","title":"add_labels!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.compose","title":"compose","text":"DocStringExtensions.MethodSignatures compose multiple systems together The resulting system would inherit the first system's name"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.GeometricBrownianMotionProcess","title":"GeometricBrownianMotionProcess","text":"μ σ t0 W0 Z0 nothing kwargs μ σ t0 W0 Z0 nothing kwargs A  GeometricBrownianMotion  process is a Wiener process with constant drift  μ  and constant diffusion  σ  I.e this is the solution of the stochastic differential equation dX_t  mu X_t dt  sigma X_t dW_t The  GeometricBrownianMotionProcess  is distribution exact meaning not a numerical solution of the stochastic differential equation and instead follows the exact distribution properties It can be back interpolated exactly as well The constructor is"},{"doctype":"documentation","id":"references/DiffEqSensitivity.NILSSForwardSensitivityFunction","title":"NILSSForwardSensitivityFunction","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractBVProblem","title":"AbstractBVProblem","text":"DocStringExtensions.TypeDefinition Base for types which define BVP problems"},{"doctype":"documentation","id":"references/QuasiMonteCarlo.SectionSample","title":"SectionSample","text":"SectionSample(x0 sampler  where  sampler  is any sampler above and  x0  is a vector of either  NaN  for a free dimension or some scalar for a constrained dimension"},{"doctype":"documentation","id":"references/DiffEqOperators.∘","title":"∘","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.value","title":"value","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.get_caches","title":"get_caches","text":""},{"doctype":"documentation","id":"references/GlobalSensitivity.DeltaResult","title":"DeltaResult","text":""},{"doctype":"documentation","id":"references/Catalyst.double_arrows","title":"double_arrows","text":""},{"doctype":"documentation","id":"references/PolyChaos.LogisticMeasure","title":"LogisticMeasure","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.StegrCache","title":"StegrCache","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.isoperator","title":"isoperator","text":""},{"doctype":"documentation","id":"references/Catalyst.reset!","title":"reset!","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.hasbranching","title":"hasbranching","text":""},{"doctype":"documentation","id":"references/GlobalSensitivity._calculate_partial_correlation_coefficients","title":"_calculate_partial_correlation_coefficients","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.SystemStructures.SystemStructure","title":"SystemStructure","text":""},{"doctype":"document","id":"PolyChaos/type_hierarchy.md","title":"Type Hierarchy","text":"Note If you are unfamiliar with the mathematical background of orthogonal polynomials check out this tutorial  MathematicalBackground Type Hierarchy Let's look at the types  PolyChaos  provides There are four  AbstractTypes   AbstractMeasure   AbstractOrthoPoly   AbstractQuad  and  AbstractTensor   AbstractMeasure  is the core on which  AbstractOrthoPoly  builds on which  AbstractQuad  builds which is then used by  AbstractTensor  AbstractMeasure The type tree for  AbstractMeasure  looks as follows There are several canonical measures that  PolyChaos  provides all gathered in as subtypes of  AbstractCanonicalMeasure  The  Measure  type is a generic measure and  ProductMeasure  has an obvious meaning What are the relevant fields Measure It all begins with a measure more specifically absolutely continuous measures What are the fields of such a type  Measure  Field Meaning  name::String  Name of measure  w::Function  Weight function  w Omega rightarrow mathbb{R   dom::Tuple{Real,Real  Domain  Omega  symmetric::Bool  Is  w  symmetric relative to some  m in Omega  hence  w(m-x  w(m+x  for all  x in Omega   pars::Dict  Additional parameters e.g shape parameters for Beta distribution They are a  name  a weight function  w Omega rightarrow mathbb{R  with domain  Omega   dom  If the weight function is symmetric relative to some  m in Omega  the field  symmetric  should be set to  true  Symmetry relative to  m  means that forall x in Omega quad w(m-x  w(m+x For example the Gaussian probability density w(x  frac{1}{\\sqrt{2\\pi mathrm{e}^{-x^2/2 is symmetric relative to the origin  m=0  If the weight function has any parameters then they are stored in the dictionary  pars  For example the probability density of the Beta distribution on  Omega  0,1  has two positive shape parameters  alpha beta  0 w(x  frac{1}{B(\\alpha,\\beta x^{\\alpha-1 1-x)^{\\beta-1 This tutorial shows the above in action  UnivariateMonicOrthogonalPolynomials ProductMeasure So far everything was univariate the weight of the measure was mapping real numbers to real numbers  PolyChaos  can handle product measures too Let's assume the weight function is a product of two independent Gaussian densities w mathbb{R times mathbb{R rightarrow mathbb{R quad w(x  frac{1}{\\sqrt{2\\pi mathrm{e}^{x_1^2/2 frac{1}{\\sqrt{2\\pi mathrm{e}^{x_2^2/2 The type  ProductMeasure  serves this purpose with its straightforward fields Field Meaning  w::Function  Weight function  measures::Vector{<:AbstractMeasure  Vector of univariate measures This tutorial shows the above in action  MultivariateMonicOrthogonalPolynomials Canonical Measures Canonical measures are special because we know their orthogonal polynomials That is why several canonical measures are pre-defined in  PolyChaos  Some of them may require additional parameters alphabetical order Beta01Measure Field Meaning w::Function   frac{1}{B(\\alpha,\\beta t^{\\alpha-1 1-t)^{\\beta-1   dom::Tuple{<:Real,<:Real   0 1   symmetric::Bool  true if  alpha  beta   ashapeParameter::Real   alpha  0   bshapeParameter::Real   beta  0  GammaMeasure Field Meaning  w::Function   frac{\\beta^\\alpha}{\\Gamma(\\alpha t^{\\alpha-1 exp(-\\beta t   dom::Tuple{<:Real,<:Real   0 infty  symmetric::Bool  false  shapeParameter::Real   alpha  0  rateParameter::Real   1 GaussMeasure Field Meaning  w::Function   frac{1}{\\sqrt{2 pi  exp left  frac{t^2}{2 right  dom::Tuple{<:Real,<:Real   infty infty  symmetric::Bool  true HermiteMeasure Field Meaning  w::Function   exp\\left  t^2 right   dom::Tuple{<:Real,<:Real  infty infty   symmetric::Bool  true JacobiMeasure Field Meaning  dom::Tuple{<:Real,<:Real   1 1  symmetric::Bool  true if  alpha  beta   ashapeParameter::Real   alpha  1   bshapeParameter::Real   beta  1  LaguerreMeasure Field Meaning  w::Function   exp(-t   dom::Tuple{<:Real,<:Real   0 infty   symmetric::Bool  true LegendreMeasure Field Meaning  w::Function   1  dom::Tuple{<:Real,<:Real   1 1  symmetric::Bool  true LogisticMeasure Field Meaning  w::Function   frac{\\exp(-t)}{(1+\\exp(-t))^2   dom::Tuple{<:Real,<:Real   infty infty  symmetric::Bool  true MeixnerPollaczekMeasure Field Meaning  w::Function   frac{1}{2 pi exp((2\\phi-\\pi)t lvert\\Gamma(\\lambda  mathrm{i}t)\\rvert^2   dom::Tuple{<:Real,<:Real   infty,\\infty  symmetric::Bool  false  λParameter::Real   lambda  0  ϕParameter::Real   0  phi  pi Uniform01Measure Field Meaning  w::Function   1  dom::Tuple{<:Real,<:Real  0 1   symmetric::Bool  true Uniform_11Measure Field Meaning  w::Function   0.5  dom::Tuple{<:Real,<:Real  1 1   symmetric::Bool  true genHermiteMeasure Field Meaning  w::Function   lvert t rvert^{2 mu}\\exp left  t^2 right   dom::Tuple{<:Real,<:Real   infty infty   symmetric::Bool  true  muParameter::Real   mu  0.5  genLaguerreMeasure Field Meaning  w::Function   t^{\\alpha}\\exp(-t   dom::Tuple{<:Real,<:Real   0,\\infty  symmetric::Bool  false  shapeParameter::Bool   alpha>-1  AbstractOrthoPoly Orthogonal polynomials are at the heart of  PolyChaos  The type tree for  AbstractOrthoPoly  looks as follows It mirrors the type tree from  AbstractMeasure  there is a generica univariate type  OrthoPoly  a multivariate extension  MultiOrthoPoly  for product measures and several univariate canonical orthogonal polynomials OrthoPoly Given an absolutely continuous measure we are wondering what are the monic polynomials  phi_i Omega rightarrow mathbb{R  that are orthogonal relative to this very measure Mathematically this reads langle phi_i phi_j rangle  int_{\\Omega phi_i(t phi_j(t w(t mathrm{d}t \n\\begin{cases}\n 0  i=j \n 0  i\\neq j.\n\\end{cases They can be constructed using the type  OrthoPoly  which has the fields Name Meaning  name::String Name  deg::Int  Maximum degree  α::Vector{<:Real  Vector of recurrence coefficients α  β::Vector{<:Real  Vector of recurrence coefficients β  meas::AbstractMeasure  Underlying measure The purpose of  name  is obvious The integer  deg  stands for the maxium degree of the polynomials Rather than storing the polynomials  phi_i  themselves we store the recurrence coefficients  α   β  that characterize the system of orthogonal polynomials These recurrence coefficients are the single most important piece of information for the orthogonal polynomials For several common measures there exist analytic formulae These are built-in to  PolyChaos  and should be used whenever possible This tutorial shows the above in action  UnivariateMonicOrthogonalPolynomials MultiOrthoPoly Just as we did in the univariate case we use  ProductMeasure  as a building block for multivariate orthogonal polynomials The type  MultiOrthoPoly  combines product measures with the respective orthogonal polynomials and their quadrature rules Its fields are Name Meaning  name::Vector{String  Vector of names  deg::Int  Maximum degree  dim::Int  Dimension  ind::Matrix{<:Int  Array of multi-indices  measure::ProductMeasure  Underlying product measure The names of the univariate bases are stored in  names  the maximum degree of the basis is  deg  the overall dimension of the multivariate basis is  dim  the multi-index  ind  maps the  j th multivariate basis to the elements of the univariate bases the product measure is stored in  meas  finally all univariate bases are collected in  uni  This tutorial shows the above in action  MultivariateMonicOrthogonalPolynomials AbstractCanonicalOrthoPoly These are the bread-and-butter polynomials polynomials for which we know analytic formulae for the recursion coefficients The following canonical orthogonal polynomials are implemented Their fields follow Name Meaning  deg::Int  Maximum degree  α::Vector{<:Real  Vector of recurrence coefficients  β::Vector{<:Real  Vector of recurrence coefficients  measure::CanonicalMeasure Underlying canonical measure  quad::AbstractQuad  Quadrature rule Quad Quadrature rules are intricately related to orthogonal polynomials An  n point quadrature rule is a pair of so-called nodes  t_k  and weights  w_k  for  k=1,\\dots,n  that allow to solve integrals relative to the measure int_\\Omega f(t w(t mathrm{d t approx sum_{k=1}^n w_k f(t_k If the integrand  f  is polynomial then the specific Gauss quadrature rules possess the remarkable property that an  n point quadrature rule can integrate polynomial integrands  f  of degree at most  2n-1   exactly  no approximation error is made The fields of  Quad  are Name Meaning  name::String  Name  Nquad::Int  Number  n  of quadrature points  nodes::Vector{<:Real  Nodes  weights::Vector{<:Real  Weights with obvious meanings PolyChaos  provides the type  EmptyQuad  that is added in case no quadrature rule is desired This tutorial shows the above in action  NumericalIntegration Tensor The last type we need to address is  Tensor  It is used to store the results of scalar products Its fields are Name Meaning  dim  Dimension   m  of tensor  langle phi_{i_1 phi_{i_2 cdots phi_{i_{m-1 phi_{i_m rangle   T::SparseVector{Float64,Int Entries of tensor  get::Function Function to get entries from  T   op::AbstractOrthoPoly Underlying univariate orthogonal polynomials The  dimension   m  of the tensor is the number of terms that appear in the scalar product Let's assume we set  m  3  hence have  langle phi_{i phi_{j phi_{k rangle  then the concrete entry is obtained as  Tensor.get([i,j,k  This tutorial shows the above in action  ComputationOfScalarProducts"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.generate_wedges","title":"generate_wedges","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.generate_rootfinding_callback","title":"generate_rootfinding_callback","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.has_equality_constraints","title":"has_equality_constraints","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.collect_ivs","title":"collect_ivs","text":"Get all the independent variables with respect to which differentials  op  are taken"},{"doctype":"documentation","id":"references/ModelingToolkit.generate_connection_set!","title":"generate_connection_set!","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractQuadratureAlgorithm","title":"AbstractQuadratureAlgorithm","text":""},{"doctype":"documentation","id":"references/QuasiMonteCarlo","title":"QuasiMonteCarlo","text":""},{"doctype":"documentation","id":"references/PolyChaos.convert2affinePCE","title":"convert2affinePCE","text":"Computes the affine PCE coefficients  x_0  and  x_1  from X  a_1  a_2 Xi  x_0  x_1 phi_1(\\Xi where  phi_1(t  t-\\alpha_0  is the first-order monic basis polynomial Works for subtypes of AbstractCanonicalOrthoPoly The keyword  kind in lbub μσ  specifies whether  p1  and  p2  have the meaning of lower/upper bounds or mean/standard deviation"},{"doctype":"documentation","id":"references/PolyChaos.numberPolynomials","title":"numberPolynomials","text":""},{"doctype":"documentation","id":"references/SciMLBase.SDEFunction","title":"SDEFunction","text":"iip recompile f g mass_matrix I analytic nothing tgrad nothing jac nothing jvp nothing vjp nothing ggprime nothing jac_prototype nothing sparsity jac_prototype paramjac nothing syms nothing indepsym nothing colorvec nothing SDEFunction  AbstractSDEFunction A representation of an SDE function  f  defined by M du  f(u,p,t)dt  g(u,p,t dW and all of its related functions such as the Jacobian of  f  its gradient with respect to time and more For all cases  u0  is the initial condition  p  are the parameters and  t  is the independent variable Constructor Note that only the function  f  itself is required This function should be given as  f!(du,u,p,t  or  du  f(u,p,t  See the section on  iip  for more details on in-place vs out-of-place handling All of the remaining functions are optional for improving or accelerating  the usage of  f  These include mass_matrix  the mass matrix  M  represented in the ODE function Can be used to determine that the equation is actually a differential-algebraic equation DAE if  M  is singular Note that in this case special solvers are required see the DAE solver page for more details https://diffeq.sciml.ai/stable/solvers/dae_solve Must be an AbstractArray or an AbstractSciMLOperator analytic(u0,p,t  used to pass an analytical solution function for the analytical  solution of the ODE Generally only used for testing and development of the solvers tgrad(dT,u,p,t  or dT=tgrad(u,p,t returns  frac{\\partial f(u,p,t)}{\\partial t jac(J,u,p,t  or  J=jac(u,p,t  returns  frac{df}{du jvp(Jv,v,u,p,t  or  Jv=jvp(v,u,p,t  returns the directional derivative frac{df}{du v vjp(Jv,v,u,p,t  or  Jv=vjp(v,u,p,t  returns the adjoint derivative frac{df}{du}^\\ast v ggprime(J,u,p,t  or  J  ggprime(u,p,t  returns the Milstein derivative   frac{dg(u,p,t)}{du g(u,p,t jac_prototype  a prototype matrix matching the type that matches the Jacobian For example if the Jacobian is tridiagonal then an appropriately sized  Tridiagonal  matrix can be used as the prototype and integrators will specialize on this structure where possible Non-structured sparsity patterns should use a  SparseMatrixCSC  with a correct sparsity pattern for the Jacobian The default is  nothing  which means a dense Jacobian paramjac(pJ,u,p,t  returns the parameter Jacobian  frac{df}{dp  syms  the symbol names for the elements of the equation This should match  u0  in size For example if  u0  0.0,1.0  and  syms  x y  this will apply a canonical naming to the values allowing  sol[:x  in the solution and automatically naming values in plots indepsym  the canonical naming for the independent variable Defaults to nothing which internally uses  t  as the representation in any plots colorvec  a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the  jac_prototype  This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern Defaults to  nothing  which means a color vector will be internally computed on demand when required The cost of this operation is highly dependent on the sparsity pattern iip In-Place vs Out-Of-Place For more details on this argument see the ODEFunction documentation recompile Controlling Compilation and Specialization For more details on this argument see the ODEFunction documentation Fields The fields of the ODEFunction type directly match the names of the inputs"},{"doctype":"documentation","id":"references/ModelingToolkit.has_states","title":"has_states","text":""},{"doctype":"documentation","id":"references/LinearSolve.SciMLSolution","title":"SciMLSolution","text":""},{"doctype":"document","id":"DiffEqOperators/operator_tutorials/kdv.md","title":"Solving KdV Solitons with Upwinding Operators","text":"Test OrdinaryDiffEq LinearAlgebra N Δx N c x Δx ϕ x t sech x t u0 ϕ x du zeros size x A Float64 Δx length x KdV du u p t bc ϕ t ϕ t Δx mul! du A bc u single_solition KdV u0 soln single_solition Tsit5 abstol reltol Plots plot ϕ x title yaxis label plot! ϕ x label plot! soln label ls dash plot! soln label ls dash Solving KdV Solitons with Upwinding Operators The KdV equation is of the form  uₜ  αuuₓ  βuₓₓₓ  0  Here we'll use  α  6   β  1  for simplicity of the true solution expression 1-Soliton solution using Upwind Difference The analytical expression for the single soliton case takes the form  u(x,t  c/2)/cosh²(√c  ξ/2  c  0  wave speed   ξ   x  c*t  moving coordinate solution_plot"},{"doctype":"documentation","id":"references/LinearSolve.set_A","title":"set_A","text":"DocStringExtensions.MethodSignatures"},{"doctype":"documentation","id":"references/DiffEqOperators.AbstractDerivativeOperator","title":"AbstractDerivativeOperator","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.INPLACE_WHITE_NOISE_DIST","title":"INPLACE_WHITE_NOISE_DIST","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.adjoint_sensitivities","title":"adjoint_sensitivities","text":"du0 dp sol alg dg ts sensealg checkpoints sol t kwargs dg out u p t i g u p t dgdu out u p t dgdp out u p t du0 dp sol alg g nothing kwargs f du u p t du dx p u p u u du dy p u u u p prob f p sol prob Vern9 abstol reltol dg out u p t i out u ts res sol Vern9 dg ts abstol reltol ForwardDiff Calculus Tracker G p tmp_prob prob u0 convert eltype p prob u0 p p sol tmp_prob Vern9 abstol reltol saveat ts sensealg SensitivityADPassThrough A convert Array sol sum A G res2 ForwardDiff gradient G res3 Calculus gradient G res4 Tracker gradient G res5 ReverseDiff gradient G ts sol prob Vern9 saveat ts res sol Vern9 dg ts sensealg checkpointing res sol Vern9 dg ts sensealg checkpointing checkpoints g u p t sum u dg out u p t out u u out u u res sol Vern9 g nothing dg abstol reltol iabstol ireltol QuadGK G p tmp_prob prob p p sol tmp_prob Vern9 abstol reltol res err quadgk t sum sol t atol rtol res res2 ForwardDiff gradient G res3 Calculus gradient G adjoint sensitivities(sol,alg,g,t=nothing,dg=nothing                             abstol=1e-6,reltol=1e-3                             checkpoints=sol.t                             corfunc analytical=nothing                             callback  nothing                             sensealg=InterpolatingAdjoint                             kwargs Adjoint sensitivity analysis is used to find the gradient of the solution with respect to some functional of the solution In many cases this is used in an optimization problem to return the gradient with respect to some cost function It is equivalent to backpropagation or reverse-mode automatic differentiation of a differential equation Using  adjoint_sensitivities  directly let's you do three things One it can allow you to be more efficient since the sensitivity calculation can be done directly on a cost function avoiding the overhead of building the derivative of the full concretized solution It can also allow you to be more efficient by directly controlling the forward solve that is then reversed over Lastly it allows one to define a continuous cost function on the continuous solution instead of just at discrete data points Warning Adjoint sensitivity analysis functionality requires being able to solve   a differential equation defined by the parameter struct  p  Thus while   DifferentialEquations.jl can support any parameter struct type usage   with adjoint sensitivity analysis requires that  p  could be a valid   type for being the initial condition  u0  of an array This means that   many simple types such as  Tuple s and  NamedTuple s will work as   parameters in normal contexts but will fail during adjoint differentiation   To work around this issue for complicated cases like nested structs look   into defining  p  using  AbstractArray  libraries such as RecursiveArrayTools.jl    or ComponentArrays.jl so that  p  is an  AbstractArray  with a concrete element type Warning Non-checkpointed InterpolatingAdjoint and QuadratureAdjoint sensealgs   require that the forward solution  sol(t  has an accurate dense   solution unless checkpointing is used This means that you should   not use  solve(prob,alg,saveat=ts  unless checkpointing If specific   saving is required one should solve dense  solve(prob,alg  use the   solution in the adjoint and then  sol(ts  interpolate Syntax There are two forms For discrete adjoints the form is where  alg  is the ODE algorithm to solve the adjoint problem  dg  is the jump function  sensealg  is the sensitivity algorithm and  ts  is the time points for data  dg  is given by which is the in-place gradient of the cost functional  g  at time point  ts[i  with  u=u(t  For continuous functionals the form is for the cost functional with in-place gradient If the gradient is omitted i.e then we assume  dgdp  is zero and  dgdu  will be computed automatically using ForwardDiff or finite differencing depending on the  autodiff  setting in the  AbstractSensitivityAlgorithm  Note that the keyword arguments are passed to the internal ODE solver for solving the adjoint problem Example discrete adjoints on a cost function In this example we will show solving for the adjoint sensitivities of a discrete cost functional First let's solve the ODE and get a high quality continuous solution Now let's calculate the sensitivity of the  ell_2  error against 1 at evenly spaced points in time that is L(u,p,t)=\\sum_{i=1}^{n}\\frac{\\Vert1-u(t_{i},p)\\Vert^{2}}{2 for  t_i  0.5i  This is the assumption that the data is  data[i]=1.0  For this function notice we have that begin{aligned}\ndg_{1}&=1-u_{1 \ndg_{2}&=1-u_{2 \n quad vdots\n\\end{aligned and thus Also we can omit  dgdp  because the cost function doesn't dependent on  p  If we had data we'd just replace  1.0  with  data[i  To get the adjoint sensitivities call This is super high accuracy As always there's a tradeoff between accuracy and computation time We can check this almost exactly matches the autodifferentiation and numerical differentiation results and see this gives the same values Example controlling adjoint method choices and checkpointing In the previous examples all calculations were done using the interpolating method This maximizes speed but at a cost of requiring a dense  sol  If it is not possible to hold a dense forward solution in memory then one can use checkpointing For example Creates a non-dense solution with checkpoints at  0.0,0.2,0.5,0.7  Now we can do When grabbing a Jacobian value during the backwards solution it will no longer interpolate to get the value Instead it will start a forward solution at the nearest checkpoint to build local interpolants in a way that conserves memory By default the checkpoints are at  sol.t  but we can override this Example continuous adjoints on an energy functional In this case we'd like to calculate the adjoint sensitivity of the scalar energy functional G(u,p)=\\int_{0}^{T}\\frac{\\sum_{i=1}^{n}u_{i}^{2}(t)}{2}dt which is Notice that the gradient of this function with respect to the state  u  is To get the adjoint sensitivities we call Notice that we can check this against autodifferentiation and numerical differentiation as follows"},{"doctype":"documentation","id":"references/RecursiveArrayTools.AbstractDiffEqArray","title":"AbstractDiffEqArray","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.NoMetadata","title":"NoMetadata","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.JumpSystem","title":"JumpSystem","text":"β γ t S t I t R t rate₁ β S I affect₁ S S I I rate₂ γ I affect₂ I I R R j₁ ConstantRateJump rate₁ affect₁ j₂ ConstantRateJump rate₂ affect₂ j₃ MassActionJump β γ R S R js j₁ j₂ j₃ t S I R β γ DocStringExtensions.TypeDefinition A system of jump processes Fields DocStringExtensions.TypeFields(false Example"},{"doctype":"documentation","id":"references/Optimization.MinSense","title":"MinSense","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.reset_p","title":"reset_p","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.Fragment","title":"Fragment","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.generate_control_function","title":"generate_control_function","text":"For a system  sys  that has unbound inputs as determined by  unbound_inputs  generate a function with additional input argument  in The return values also include the remaining states and parameters in the order they appear as arguments to  f  Example"},{"doctype":"documentation","id":"references/DiffEqSensitivity.BacksolveAdjoint","title":"BacksolveAdjoint","text":"chunk_size autodiff Val central autojacvec autodiff checkpointing noise noisemixing g_mixing! du u p t du p u p u du p u p u nothing Sundials lorenz du u p t du u u du u u u du u u u u0 tspan prob lorenz u0 tspan sol prob Tsit5 reltol abstol prob2 lorenz sol end sol prob Tsit5 reltol abstol sol end u0 BacksolveAdjoint  AbstractAdjointSensitivityAlgorithm An implementation of adjoint sensitivity analysis using a backwards solution of the ODE By default this algorithm will use the values from the forward pass to perturb the backwards solution to the correct spot allowing reduced memory O(1 memory Checkpointing stabilization is included for additional numerical stability over the naive implementation Constructor Keyword Arguments autodiff  Use automatic differentiation for constructing the Jacobian if the Jacobian needs to be constructed  Defaults to  true  chunk_size  Chunk size for forward-mode differentiation if full Jacobians are built  autojacvec=false  and  autodiff=true  Default is  0  for automatic choice of chunk size diff_type  The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with  autodiff=false  autojacvec  Calculate the vector-Jacobian product  J'*v  via automatic differentiation with special seeding The default is  true  The total set of choices are false  the Jacobian is constructed via FiniteDiff.jl true  the Jacobian is constructed via ForwardDiff.jl TrackerVJP  Uses Tracker.jl for the vjp ZygoteVJP  Uses Zygote.jl for the vjp EnzymeVJP  Uses Enzyme.jl for the vjp ReverseDiffVJP(compile=false  Uses ReverseDiff.jl for the vjp  compile  is a boolean for whether to precompile the tape which should only be done if there are no branches  if  or  while  statements in the  f  function checkpointing  whether checkpointing is enabled for the reverse pass Defaults to  true  noise  Calculate the vector-Jacobian product  J'*v  of the diffusion term of an SDE via automatic differentiation with special seeding The default is  true  The total set of choices are false  the Jacobian is constructed via FiniteDiff.jl true  the Jacobian is constructed via ForwardDiff.jl DiffEqSensitivity.ZygoteNoise  Uses Zygote.jl for the vjp DiffEqSensitivity.ReverseDiffNoise(compile=false  Uses ReverseDiff.jl for the vjp  compile  is a boolean for whether to precompile the tape which should only be done if there are no branches  if  or  while  statements in the  f  function noisemixing  Handle noise processes that are not of the form  du[i  f(u[i  For example to compute the sensitivities of an SDE with diagonal diffusion correctly  noisemixing=true  must be enabled The default is  false  For more details on the vjp choices please consult the sensitivity algorithms documentation page or the docstrings of the vjp types Applicability of Backsolve and Caution When  BacksolveAdjoint  is applicable it is a fast method and requires the least memory However one must be cautious because not all ODEs are stable under backwards integration by the majority of ODE solvers An example of such an equation is the Lorenz equation Notice that if one solves the Lorenz equation forward and then in reverse with any adaptive time step and non-reversible integrator then the backwards solution diverges from the forward solution As a quick demonstration Thus one should check the stability of the backsolve on their type of problem before enabling this method Additionally using checkpointing with backsolve can be a low memory way to stabilize it For more details on this topic see  Stiff Neural Ordinary Differential Equations  Checkpointing To improve the numerical stability of the reverse pass  BacksolveAdjoint  includes a checkpointing feature If  sol.u  is a time series then whenever a time  sol.t  is hit while reversing a callback will replace the reversing ODE portion with  sol.u[i  This nudges the solution back onto the appropriate trajectory and reduces the numerical caused by drift SciMLProblem Support This  sensealg  only supports  ODEProblem s  SDEProblem s and  RODEProblem s This  sensealg  supports callback functions events References ODE Rackauckas C and Ma Y and Martensen J and Warner C and Zubov K and Supekar R and Skinner D and Ramadhana A and Edelman A Universal Differential Equations for Scientific Machine Learning,\tarXiv:2001.04385 Hindmarsh A C and Brown P N and Grant K E and Lee S L and Serban R and Shumaker D E and Woodward C S SUNDIALS Suite of nonlinear and differential/algebraic equation solvers ACM Transactions on Mathematical Software TOMS 31 pp:363–396 2005 Chen R.T.Q and Rubanova Y and Bettencourt J and Duvenaud D K Neural ordinary differential equations In Advances in neural information processing systems pp 6571–6583 2018 Pontryagin L S and Mishchenko E.F and Boltyanskii V.G and Gamkrelidze R.V The mathematical theory of optimal processes Routledge 1962 Rackauckas C and Ma Y and Dixit V and Guo X and Innes M and Revels J and Nyberg J and Ivaturi V A comparison of automatic differentiation and continuous sensitivity analysis for derivatives of differential equation solutions arXiv:1812.01892 DAE Cao Y and Li S and Petzold L and Serban R Adjoint sensitivity analysis for differential-algebraic equations The adjoint DAE system and its numerical solution SIAM journal on scientific computing 24 pp 1076-1089 2003 SDE Gobet E and Munos R Sensitivity Analysis Using Ito-Malliavin Calculus and Martingales and Application to Stochastic Optimal Control SIAM Journal on control and optimization 43 pp 1676-1713 2005 Li X and Wong T.-K L.and Chen R T Q and Duvenaud D Scalable Gradients for Stochastic Differential Equations PMLR 108 pp 3870-3882 2020 http://proceedings.mlr.press/v108/li20i.html"},{"doctype":"documentation","id":"references/Optimization.AutoForwardDiff","title":"AutoForwardDiff","text":"f kwargs AutoForwardDiff  AbstractADType An AbstractADType choice for use in OptimizationFunction for automatically generating the unspecified derivative functions Usage This uses the  ForwardDiff.jl  package It is the fastest choice for small systems especially with heavy scalar interactions It is easy to use and compatible with most pure is Julia functions which have loose type restrictions However because it's forward-mode it scales poorly in comparison to other AD choices Hessian construction is suboptimal as it uses the forward-over-forward approach Compatible with GPUs Compatible with Hessian-based optimization Compatible with Hv-based optimization Compatible with constraints Note that only the unspecified derivative functions are defined For example if a  hess  function is supplied to the  OptimizationFunction  then the Hessian is not defined via ForwardDiff"},{"doctype":"documentation","id":"references/ModelingToolkit.has_defaults","title":"has_defaults","text":""},{"doctype":"documentation","id":"references/SciMLOperators.ComposedOperator","title":"ComposedOperator","text":""},{"doctype":"documentation","id":"references/Catalyst.push_reactions!","title":"push_reactions!","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.CollocationKernel","title":"CollocationKernel","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.LIBLAPACK","title":"LIBLAPACK","text":""},{"doctype":"documentation","id":"references/Integrals.ZygoteVJP","title":"ZygoteVJP","text":""},{"doctype":"documentation","id":"references/MethodOfLines.center_align","title":"center_align","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.enqueue_expr!","title":"enqueue_expr!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.isautonomous","title":"isautonomous","text":""},{"doctype":"documentation","id":"references/SciMLBase.rand_cache","title":"rand_cache","text":""},{"doctype":"document","id":"GlobalSensitivity/index.md","title":"Global Sensitivity Analysis","text":"effects f method param_range N batch effects f method A B batch Global Sensitivity Analysis Global Sensitivity Analysis GSA methods are used to quantify the uncertainty in output of a model w.r.t the parameters These methods allow practitioners to measure both parameter's individual contributions and the contribution of their interactions to the output uncertainity Installation To use this functionality you must install GlobalSensitivity.jl Note GlobalSensitivity.jl is unrelated to the GlobalSensitivityAnalysis.jl package General Interface The general interface for calling a global sensitivity analysis is either where y=f(x  is a function that takes in a single vector and spits out a single vector or scalar If  batch=true  then  f  takes in a matrix where each row is a set of parameters and returns a matrix where each row is a the output for the corresponding row of parameters method  is one of the GSA methods below param_range  is a vector of tuples for the upper and lower bound for the given parameter  i  N  is a required keyword argument for the number of samples to take in the trajectories/design Note that for some methods there is a second interface where one can directly pass the design matrices where  A  and  B  are design matrices with each row being a set of parameters Note that  generate_design_matrices  from  QuasiMonteCarlo.jl  can be used to generate the design matrices The descriptions of the available methods can be found in the Methods section The GSA interface allows for utilizing batched functions with the  batch  kwarg discussed above for parallel computation of GSA results"},{"doctype":"documentation","id":"references/Surrogates.phi_njND","title":"phi_njND","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.assemble_maj","title":"assemble_maj","text":""},{"doctype":"documentation","id":"references/LinearSolve.QRFactorization","title":"QRFactorization","text":""},{"doctype":"documentation","id":"references/Catalyst.recursive_expand_functions!","title":"recursive_expand_functions!","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.construct_t2","title":"construct_t2","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractOptimizationAlgorithm","title":"AbstractOptimizationAlgorithm","text":"DocStringExtensions.TypeDefinition"},{"doctype":"document","id":"PolyChaos/functions.md","title":"Functions","text":"Functions Note The core interface of all essential functions are  not  dependent on specialized types such as  AbstractOrthoPoly  Having said that for exactly those essential functions there exist overloaded functions that accept specialized types such as  AbstractOrthoPoly  as arguments Too abstract For example the function  evaluate  that evaluates a polynomial of degree  n  at points  x  has the core interface where  a  and  b  are the vectors of recurrence coefficients For simplicity there also exists the interface So fret not upon the encounter of multiply-dispatched versions of the same thing It's there to simplify your life The idea of this approach is to make it simpler for others to copy and paste code snippets and use them in their own work List of all functions in  PolyChaos  Recurrence Coefficients for Monic Orthogonal Polynomials The functions below provide analytic expressions for the recurrence coefficients of common orthogonal polynomials All of these provide  monic orthogonal polynomials  relative to the weights Note The number  N  of recurrence coefficients has to be positive for all functions below Show Orthogonal Polynomials To get a human-readable output of the orthognoal polynomials there is the function  showpoly In case you want to see the entire basis just use  showbasis Both of these functions make excessive use of Evaluate Orthogonal Polynomials Scalar Products Quadrature Rules Polynomial Chaos Auxiliary Functions"},{"doctype":"documentation","id":"references/NonlinearSolve.AbstractImmutableNonlinearSolver","title":"AbstractImmutableNonlinearSolver","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.is_pos","title":"is_pos","text":""},{"doctype":"document","id":"GlobalSensitivity/methods/easi.md","title":"EASI Method","text":"max_harmonic Int Test ishi_batch X A B sin X A sin X B X sin X ishi X A B sin X A sin X B X sin X lb ones π ub ones π res1 ishi lb i ub i i N res2 ishi_batch lb i ub i i N batch EASI Method EASI  has the following keyword arguments max_harmonic  Maximum harmonic of the input frequency for which the output power spectrum is analyzed for Defaults to  10  Method Details The EASI method is a Fourier-based technique for performing variance-based methods of global sensitivity analysis for the computation of first order effects Sobol’ indices hence belonging into the same class of algorithms as FAST and RBD It is a computationally cheap method for which existing data can be used Unlike the FAST and RBD methods which use a specially generated sample set that contains suitable frequency data for the input factors in EASI these frequencies are introduced by sorting and shuffling the available input samples API Example"},{"doctype":"documentation","id":"references/ExponentialUtilities.phiv","title":"phiv","text":"Compute the matrix-phi-vector products using Krylov  k   1 The phi functions are defined as varphi_0(z  exp(z),\\quad varphi_{k+1}(z  frac{\\varphi_k(z  1}{z A Krylov subspace is constructed using  arnoldi  and  phiv_dense  is called on the Hessenberg matrix If  correct=true  then phi_0 through phi_k-1 are updated using the last Arnoldi vector v_m+1   If  errest=true  then an additional error estimate for the second-to-last phi is also returned For the additional keyword arguments consult  arnoldi  phiv(t,Ks,k;correct,kwargs  phi_0(tA)b phi_1(tA)b  phi_k(tA)b errest Compute the matrix-phi-vector products using a pre-constructed Krylov subspace Niesen J  Wright W 2009 A Krylov subspace algorithm for evaluating the φ-functions in exponential integrators arXiv preprint arXiv:0907.4631 Formula 10"},{"doctype":"documentation","id":"references/SciMLBase.__has_Wfact_t","title":"__has_Wfact_t","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.Laplacian","title":"Laplacian","text":""},{"doctype":"documentation","id":"references/Catalyst.numspecies","title":"numspecies","text":"Return the total number of species within the given  ReactionSystem  and subsystems that are  ReactionSystem s Notes If there are no subsystems this will be fast As this calls  species  it can be slow and will allocate if there are any subsystems"},{"doctype":"documentation","id":"references/DiffEqFlux.forward_ffjord","title":"forward_ffjord","text":""},{"doctype":"documentation","id":"references/Optimization.AutoReverseDiff","title":"AutoReverseDiff","text":"f kwargs compile AutoReverseDiff  AbstractADType An AbstractADType choice for use in OptimizationFunction for automatically generating the unspecified derivative functions Usage This uses the  ReverseDiff.jl  package  AutoReverseDiff  has a default argument  compile  which denotes whether the reverse pass should be compiled  compile  should only   be set to  true  if  f  contains no branches if statements while loops   otherwise it can produce incorrect derivatives  AutoReverseDiff  is generally applicable to many pure Julia codes and with  compile=true  it is one of the fastest options on code with heavy scalar interactions Hessian calculations are fast by mixing ForwardDiff with ReverseDiff for forward-over-reverse However its performance can falter when  compile=false  Not compatible with GPUs Compatible with Hessian-based optimization by mixing with ForwardDiff Compatible with Hv-based optimization by mixing with ForwardDiff Not compatible with constraint functions Note that only the unspecified derivative functions are defined For example if a  hess  function is supplied to the  OptimizationFunction  then the Hessian is not defined via ReverseDiff Constructor Note currently compilation is not defined/used"},{"doctype":"documentation","id":"references/DiffEqSensitivity.AbstractAdjointSensitivityAlgorithm","title":"AbstractAdjointSensitivityAlgorithm","text":""},{"doctype":"documentation","id":"references/PolyChaos.InconsistencyError","title":"InconsistencyError","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.exp_pade_p!","title":"exp_pade_p!","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractDiscreteFunction","title":"AbstractDiscreteFunction","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/Catalyst.paramsmap","title":"paramsmap","text":"Given a  ReactionSystem  return a Dictionary mapping from all parameters that appear within the system to their index within  parameters(network "},{"doctype":"document","id":"Optimization/API/modelingtoolkit.md","title":"ModelingToolkit Integration","text":"ModelingToolkit Integration Optimization.jl is heavily integrated with the ModelingToolkit.jl symbolic system for symbolic-numeric optimizations It provides a front-end for automating the construction parallelization and optimization of code Optimizers can better interface with the extra symbolic information provided by the system There are two ways that the user interacts with ModelingToolkit.jl One can use  OptimizationFunction  with  AutoModelingToolkit  for automatically transforming numerical codes into symbolic codes See the OptimizationFunction documentation  optfunction for more details Secondly one can generate  OptimizationProblem s for use in Optimization.jl from purely a symbolic front-end This is the form users will encounter when using ModelingToolkit.jl directly and its also the form supplied by domain-specific languages For more information see the  OptimizationSystem documentation "},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.ascend_dg_all","title":"ascend_dg_all","text":""},{"doctype":"documentation","id":"references/RecursiveArrayTools.common_number","title":"common_number","text":""},{"doctype":"document","id":"PolyChaos/chi_squared_k1.md","title":"Chi-squared Distribution (k=1)","text":"Chi-squared Distribution  k=1  Theory Given a standard random variable  X sim mathcal{N}(0,1  we would like to find the random variable  Y  X^2  The analytic solution is known  Y  follows a chi-squared distribution with  k=1  degree of freedom Using polynomial chaos expansion PCE the problem can be solved using Galerkin projection Let  phi_k k=0}^{n  be the monic orthogonal basis relative to the probability density of  X  namely f_X(x  frac{1}{\\sqrt{2 pi exp left  frac{x^2}{2 right Then the PCE of  X  is given by X  sum_{k=0}^n x_k phi_k with x_0  0 quad x_1  1 quad x_i  0 quad forall i 2,\\dots,n To find the PCE coefficients  y_k  for  Y  sum_{k=}^n y_k phi_k  we apply Galerkin projection which leads to y_m langle phi_m phi_m rangle  sum_{i=0}^n sum_{j=0}^n x_i x_j langle phi_i phi_j phi_m rangle quad forall m  0 dots n Hence knowing the scalars  langle phi_m phi_m rangle  and  langle phi_i phi_j phi_m rangle  the PCE coefficients  y_k  can be obtained immediately From the PCE coefficients we can get the moments and compare them to the closed-form expressions Notice  A maximum degree of 2 suffices to get the  exact  solution with PCE In other words increasing the maximum degree to values greater than 2 introduces nothing but computational overhead and numerical errors possibly Practice First we create a orthogonal basis relative to  f_X(x  of degree at most  d=2   deg  below Notice that we consider a total of  Nrec  recursion coefficients and that we also add a quadrature rule by setting  addQuadrature  true  What are the basis polynomials Note that the command  showbasis  is based on the more general  showpoly  Next we define the PCE for  X  With the orthogonal basis and the quadrature at hand we can compute the tensors  t2  and  t3  that store the entries  langle phi_m phi_m rangle  and  langle phi_i phi_j phi_m rangle  respectively With the tensors at hand we can compute the Galerkin projection Let's compare the moments via PCE to the closed-form expressions Let's plot the probability density function to compare results We first draw samples from the measure with the help of  sampleMeasure  and then evaluate the basis at these samples and multiply times the PCE coefficients The latter stop is done using  evaluatePCE  Finally we compare the result agains the analytical PDF  rho(t  frac{\\mathrm{e}^{-0.5t}}{\\sqrt{2 t  Gamma(0.5  of the chi-squared distribution with one degree of freedom"},{"doctype":"documentation","id":"references/PolyChaos.fejer","title":"fejer","text":"Fejer's first quadrature rule"},{"doctype":"documentation","id":"references/SciMLBase.DynamicalDDEProblem","title":"DynamicalDDEProblem","text":"DocStringExtensions.TypeDefinition Define a dynamical DDE problem from a  DynamicalDDEFunction  Define a dynamical DDE problem from the two functions  f1  and  f2  Arguments f1  and  f2  The functions in the DDE v0  and  u0  The initial conditions h  The initial history function tspan  The timespan for the problem p  Parameter values for  f1  and  f2  callback  A callback to be applied to every solver which uses the problem Defaults to nothing isinplace  optionally sets whether the function is inplace or not This is determined automatically but not inferred"},{"doctype":"document","id":"Surrogates/ackley.md","title":"Ackley function","text":"Ackley function The Ackley function is defined as  f(x  a*exp(-b\\sqrt{\\frac{1}{d}\\sum_{i=1}^d x_i^2  exp(\\frac{1}{d sum_{i=1}^d cos(cx_i  a  exp(1  Usually the recommended values are  a  20   b  0.2  and  c  2\\pi Let's see the 1D case Now let's define the  Ackley  function The fit looks good Let's now see if we are able to find the minimum value using optimization methods The DYCORS methods successfully finds the minimum"},{"doctype":"documentation","id":"references/Surrogates.remove_tracker","title":"remove_tracker","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.varvar_dependencies","title":"varvar_dependencies","text":"eqdeps T vardeps T T Integer vardeps eqdeps varvardep odesys odesys Calculate a  LightGraph.SimpleDiGraph  that maps each variable to variables they depend on Notes The  fadjlist  of the  SimpleDiGraph  maps from a variable to the variables that depend on it The  badjlist  of the  SimpleDiGraph  maps from a variable to variables on which it depends Example Continuing the example of  equation_dependencies"},{"doctype":"documentation","id":"references/ModelingToolkit.ValidationError","title":"ValidationError","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.ForwardSensitivityParameterCompatibilityError","title":"ForwardSensitivityParameterCompatibilityError","text":""},{"doctype":"documentation","id":"references/LabelledArrays.LAStyle","title":"LAStyle","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.Tail6","title":"Tail6","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.__init__","title":"__init__","text":""},{"doctype":"document","id":"Surrogates/multi_objective_opt.md","title":"Multi objective optimization benchmarks","text":"Multi objective optimization benchmarks Case 1 Non colliding objective functions Case 2 objective functions with conflicting minima"},{"doctype":"documentation","id":"references/DiffEqOperators.DivergenceOperator","title":"DivergenceOperator","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.choose_dt","title":"choose_dt","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.ExpMethodHigham2005","title":"ExpMethodHigham2005","text":"Computes the matrix exponential using the algorithm Higham N J 2005 The scaling and squaring method for the matrix exponential revisited SIAM J Matrix Anal Appl.Vol 26 No 4 pp 1179–1193 based on generated code If a matrix is specified balancing is determined automatically"},{"doctype":"document","id":"NeuralPDE/examples/ode.md","title":"Solving ODEs with Neural Networks","text":"Flux GalacticFlux GalacticOptimJL linear u p t cos pi t tspan u0 prob linear u0 tspan Flux Dense σ Dense opt Flux ADAM sol prob opt dt verbose abstol maxiters Solving ODEs with Neural Networks The following is an example of solving a DifferentialEquations.jl  ODEProblem  with a neural network using the physics-informed neural networks approach specialized to 1-dimensional PDEs ODEs"},{"doctype":"document","id":"Surrogates/cantilever.md","title":"Cantilever beam function","text":"Cantilever beam function The Cantilever Beam function is defined as  f(w,t  frac{4L^3}{Ewt}*\\sqrt frac{Y}{t^2})^2  frac{X}{w^2})^2   With parameters L,E,X and Y given Let's import Surrogates and Plots Define the objective function Let's plot it Fitting different Surrogates Plotting"},{"doctype":"documentation","id":"references/DiffEqSensitivity.gradient!","title":"gradient!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.nonzerosmap","title":"nonzerosmap","text":""},{"doctype":"documentation","id":"references/SciMLOperators","title":"SciMLOperators","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.ChebyshevBasis","title":"ChebyshevBasis","text":"n Constructs a Chebyshev basis of the form T_{0}(x T_{1}(x  T_(x where T_j is the j-th Chebyshev polynomial of the first kind Arguments n  number of terms in the polynomial expansion"},{"doctype":"document","id":"Integrals/index.md","title":"Integrals.jl: Unified Integral Approximation Interface","text":"Pkg Pkg add Integrals.jl Unified Integral Approximation Interface Integrals.jl is a unified interface for the numerical approximation of integrals quadrature in Julia It interfaces with other packages of the Julia ecosystem to make it easy to test alternative solver packages and pass small types to control algorithm swapping Installation To install Integrals.jl use the Julia package manager Contributing Please refer to the  SciML ColPrac Contributor's Guide on Collaborative Practices for Community Packages  for guidance on PRs issues and other matters relating to contributing to ModelingToolkit There are a few community forums the diffeq-bridged channel in the  Julia Slack JuliaDiffEq  on Gitter on the  Julia Discourse forums see also  SciML Community page"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.generate_boxes2","title":"generate_boxes2","text":""},{"doctype":"documentation","id":"references/MethodOfLines._split_terms","title":"_split_terms","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.is_neg_half","title":"is_neg_half","text":""},{"doctype":"documentation","id":"references/Catalyst.reactionrates","title":"reactionrates","text":"Given a  ReactionSystem  returns a vector of the symbolic reaction rates for each reaction"},{"doctype":"documentation","id":"references/NeuralPDE.GradientScaleAdaptiveLoss","title":"GradientScaleAdaptiveLoss","text":"A way of adaptively reweighting the components of the loss function in the total sum such that BC_i loss weights are scaled by the exponential moving average of max(|∇pde_loss|)/mean(|∇bc_i_loss  reweight_every  how often to reweight the BC loss functions measured in iterations  reweighting is somewhat expensive since it involves evaluating the gradient of each component loss function weight_change_inertia  a real number that represents the inertia of the exponential moving average of the BC weight changes pde_loss_weights  either a scalar which will be broadcast or vector the size of the number of PDE equations which describes the weight the respective PDE loss has in the full loss sum bc_loss_weights  either a scalar which will be broadcast or vector the size of the number of BC equations which describes the initial weight the respective BC loss has in the full loss sum additional_loss_weights  a scalar which describes the weight the additional loss function has in the full loss sum this is currently not adaptive and will be constant with this adaptive loss from paper Understanding and mitigating gradient pathologies in physics-informed neural networks Sifan Wang Yujun Teng Paris Perdikaris https://arxiv.org/abs/2001.04536v1 with code reference https://github.com/PredictiveIntelligenceLab/GradientPathologiesPINNs"},{"doctype":"documentation","id":"references/PolyChaos.w_genhermite","title":"w_genhermite","text":""},{"doctype":"documentation","id":"references/NeuralPDE.NNKolmogorov","title":"NNKolmogorov","text":"opt sdealg ensemblealg Algorithm for solving Backward Kolmogorov Equations Arguments chain  A Chain neural network with a d-dimensional output opt  The optimizer to train the neural network Defaults to  ADAM(0.1  sdealg  The algorithm used to solve the discretized SDE according to the process that X follows Defaults to  EM  ensemblealg  The algorithm used to solve the Ensemble Problem that performs Ensemble simulations for the SDE Defaults to  EnsembleThreads  See the  Ensemble Algorithms  documentation for more details kwargs  Additional arguments splatted to the SDE solver See the  Common Solver Arguments  documentation for more details 1]Beck Christian et al Solving stochastic differential equations and Kolmogorov equations by means of deep learning arXiv preprint arXiv:1806.00421 2018"},{"doctype":"document","id":"DiffEqFlux/examples/multiple_shooting.md","title":"Multiple Shooting","text":"Lux OptimizationPolyalgorithms DifferentialEquations Random rng Random default_rng datasize u0 Float32 tspan tsteps range tspan tspan length datasize trueODEfunc du u p t true_A du u true_A prob_trueode trueODEfunc u0 tspan ode_data Array prob_trueode Tsit5 saveat tsteps nn Lux ActivationFunction x x Lux Dense tanh Lux Dense p_init st Lux setup rng nn neuralode nn tspan Tsit5 saveat tsteps prob_node u p t nn u p st u0 tspan Lux ComponentArray p_init plot_multiple_shoot plt preds group_size step group_size ranges datasize group_size i rg enumerate ranges plot! plt tsteps rg preds i markershape circle label i anim Animation callback p l preds doplot display l doplot plt scatter tsteps ode_data label plot_multiple_shoot plt preds group_size frame anim display plot plt group_size continuity_term loss_function data pred sum abs2 data pred loss_multiple_shooting p p ode_data tsteps prob_node loss_function Tsit5 group_size continuity_term adtype optf x p loss_multiple_shooting x adtype optprob optf Lux ComponentArray p_init res_ms optprob PolyOpt cb callback gif anim fps Multiple Shooting In Multiple Shooting the training data is split into overlapping intervals The solver is then trained on individual intervals If the end conditions of any interval coincide with the initial conditions of the next immediate interval then the joined/combined solution is same as solving on the whole dataset without splitting To ensure that the overlapping part of two consecutive intervals coincide we add a penalizing term  continuity_term  absolute_value_of(prediction of last point of group i  prediction of first point of group i+1  to the loss Note that the  continuity_term  should have a large positive value to add high penalties in case the solver predicts discontinuous values The following is a working demo using Multiple Shooting Here's the animation that we get from above pic  The connected lines show the predictions of each group Notice that there are overlapping points as well These are the points we are trying to coincide Here is an output with  group_size  30  which is same as solving on the whole interval without splitting also called single shooting pic_single_shoot3 It is clear from the above picture a single shoot doesn't perform very well with the ODE Problem we have and gets stuck in a local minima"},{"doctype":"documentation","id":"references/LabelledArrays.PrintWrapper","title":"PrintWrapper","text":""},{"doctype":"documentation","id":"references/PolyChaos.build_w_jacobi","title":"build_w_jacobi","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.phiv!","title":"phiv!","text":"Non-allocating version of phiv that uses precomputed Krylov subspace  Ks "},{"doctype":"document","id":"Integrals/tutorials/differentiating_integrals.md","title":"Differentiating Integrals","text":"ForwardDiff FiniteDiff Zygote Cuba f x p sum sin x p lb ones ub ones p testf p prob f lb ub p sin prob CubaCuhre reltol abstol dp1 Zygote gradient testf p dp2 FiniteDiff finite_difference_gradient testf p dp3 ForwardDiff gradient testf p dp1 dp2 dp3 Differentiating Integrals Integrals.jl is a fully differentiable quadrature library Thus it adds the ability to perform automatic differentiation over any of the libraries that it calls It integrates with ForwardDiff.jl for forward-mode automatic differentiation and Zygote.jl for reverse-mode automatic differentiation For example"},{"doctype":"documentation","id":"references/Catalyst.__unpacksys","title":"__unpacksys","text":""},{"doctype":"documentation","id":"references/NeuralPDE.NNParamKolmogorov","title":"NNParamKolmogorov","text":""},{"doctype":"documentation","id":"references/SciMLBase.has_ldiv!","title":"has_ldiv!","text":""},{"doctype":"documentation","id":"references/SciMLBase.solution_new_tslocation","title":"solution_new_tslocation","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.calculate_jacobian","title":"calculate_jacobian","text":"sys Calculate the jacobian matrix of a system Returns a matrix of  Num  instances The result from the first call will be cached in the system object"},{"doctype":"documentation","id":"references/ModelingToolkit.filter_kwargs","title":"filter_kwargs","text":""},{"doctype":"documentation","id":"references/Surrogates.InverseDistanceStructure","title":"InverseDistanceStructure","text":""},{"doctype":"documentation","id":"references/Surrogates.LobachevskyStructure","title":"LobachevskyStructure","text":""},{"doctype":"documentation","id":"references/SciMLBase.set_abstol!","title":"set_abstol!","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.isdependent","title":"isdependent","text":""},{"doctype":"documentation","id":"references/GlobalSensitivity.DGSMResult","title":"DGSMResult","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.construct_augmenting_path!","title":"construct_augmenting_path!","text":"Try to construct an augmenting path in matching and if such a path is found update the matching accordingly"},{"doctype":"documentation","id":"references/PolyChaos.rm_hermite_prob","title":"rm_hermite_prob","text":"Creates  N  recurrence coefficients for monic probabilists Hermite polynomials that are orthogonal on  infty,\\infty  relative to  w(t  mathrm{e}^{-0.5t^2"},{"doctype":"document","id":"Surrogates/BraninFunction.md","title":"Branin Function","text":"Branin Function The Branin Function is commonly used as a test function for metamodelling in computer experiments especially in the context of optimization The expression of the Branin Function is given as  f(x  x_2  frac{5.1}{4\\pi^2}x_1^{2  frac{5}{\\pi}x_1  6)^2  10(1-\\frac{1}{8\\pi})\\cos(x_1  10 where  x  x_1 x_2  with  5\\leq x_1 leq 10 0 leq x_2 leq 15 First of all we will import these two packages  Surrogates  and  Plots  Now let's define our objective function Now let's plot it Now it's time to fitting different surrogates and then we will plot them We will have a look on  Kriging Surrogate  Now we will have a look on  Inverse Distance Surrogate  Now let's talk about  Lobachevsky Surrogate "},{"doctype":"documentation","id":"references/PoissonRandom.log1pmx_kernel","title":"log1pmx_kernel","text":""},{"doctype":"documentation","id":"references/PolyChaos.showpoly","title":"showpoly","text":"Show the monic polynomial with coefficients  coeffs  in a human readable way They keyword  sym  sets the name of the variable and  digits  controls the number of shown digits Show the monic polynomial of degree/range  d  that has the recurrence coefficients  α   β  Tailored to types from  PolyChaos.jl Show the monic polynomial of degree/range  d  of an  AbstractOrthoPoly  Thanks   for providing this functionality"},{"doctype":"documentation","id":"references/SciMLBase.interpolation!","title":"interpolation!","text":"DocStringExtensions.MethodSignatures Get the value at tvals where the solution is known at the times t sorted with values u and derivatives ks DocStringExtensions.MethodSignatures Get the value at tval where the solution is known at the times t sorted with values u and derivatives ks"},{"doctype":"documentation","id":"references/PolyChaos.rm_hermite","title":"rm_hermite","text":"Creates  N  recurrence coefficients for monic generalized Hermite polynomials that are orthogonal on  infty,\\infty  relative to  w(t  t|^{2 mu mathrm{e}^{-t^2 The call  rm_hermite(N  is the same as  rm_hermite(N,0 "},{"doctype":"documentation","id":"references/PoissonRandom.procf","title":"procf","text":""},{"doctype":"documentation","id":"references/QuasiMonteCarlo.SamplingAlgorithm","title":"SamplingAlgorithm","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.print_message","title":"print_message","text":""},{"doctype":"document","id":"Optimization/tutorials/symbolic.md","title":"Symbolic Problem Building with ModelingToolkit","text":"OptimizationOptimJL x y a b loss a x b y x sys loss x y a b u0 x y p a b prob sys u0 p grad hess prob Newton Symbolic Problem Building with ModelingToolkit Note This example uses the OptimizationOptimJL.jl package See the Optim.jl page  optim for details on the installation and usage Needs text but it's super cool and auto-parallelizes and sparsifies too Plus you can hierarchically nest systems to have it generate huge optimization problems Check out the  ModelingToolkit.jl OptimizationSystem documentation  for more information"},{"doctype":"document","id":"ModelingToolkit/basics/Validation.md","title":"[Model Validation and Units]( units)","text":"Unitful t unit x t unit g t w t unit t unit x t unit g t w t unit t unit x t unit g t w t unit x unit x unit Unitful τ unit t unit E t unit P t unit D Differential t eqs eqs D E P E τ P eqs eqs eqs rhs Unitful τ unit t unit E t unit P t unit D Differential t eqs eqs D E P E τ P eqs t D Differential t NewType f dummycomplex complex Num scalar dummycomplex complex scalar complex f scalar c NewType MT x NewType MT x f MT op typeof dummycomplex args argunits MT args MT args sts a t unit ps s unit c c unit eqs D a dummycomplex c s sys eqs t sts ps name sys sys_simple sys Unitful t unit E t unit P t unit D Differential t eqs D E P E eqs myfunc E E eqs D E P myfunc E eqs Unitful τ unit t unit E t unit P t unit D Differential t eqs D E P E τ eqs myfunc E τ E τ eqs D E P myfunc E τ eqs remove_units p Dict Dict k Unitful ustrip k v k v p add_units p Dict Dict k v k k v p pars τ unit p Dict τ sys remove_units u0 tspan remove_units p Model Validation and Units  units ModelingToolkit.jl provides extensive functionality for model validation and unit checking This is done by providing metadata to the variable types and then running the validation functions which identify malformed systems and non-physical equations This approach provides high performance and compatibility with numerical solvers Assigning Units Units may assigned with the following syntax Do not use  quantities  such as   1u\"s   1/u\"s  or  u\"1/s  as these will result in errors instead use  u\"s   u\"s^-1  or  u\"s\"^-1  Unit Validation  Inspection Unit validation of equations happens automatically when creating a system However for debugging purposes one may wish to validate the equations directly using  validate  Inside  validate  uses  get_unit  which may be directly applied to any term Note that  validate  will not throw an error in the event of incompatible units but  get_unit  will If you would rather receive a warning instead of an error use  safe_get_unit  which will yield  nothing  in the event of an error Unit agreement is tested with  ModelingToolkit.equivalent(u1,u2  Example usage below Note that  ModelingToolkit  does not force unit conversions to preferred units in the event of nonstandard combinations  it merely checks that the equations are consistent An example of an inconsistent system at present  ModelingToolkit  requires that the units of all terms in an equation or sum to be equal-valued  ModelingToolkit.equivalent(u1,u2  rather that simply dimensionally consistent In the future the validation stage may be upgraded to support the insertion of conversion factors into the equations User-Defined Registered Functions and Types In order to validate user-defined types and  register ed functions specialize  get_unit   Single-parameter calls to  get_unit  expect an object type while two-parameter calls expect a function type as the first argument and a vector of arguments as the second argument Unitful  Literals In order for a function to work correctly during both validation  execution the function must be unit-agnostic That is no unitful literals may be used Any unitful quantity must either be a  parameter  or  variable  For example these equations will not validate successfully Instead they should be parameterized It is recommended  not  to circumvent unit validation by specializing user-defined functions on  Unitful  arguments vs  Numbers  This both fails to take advantage of  validate  for ensuring correctness and may cause in errors in the future when  ModelingToolkit  is extended to support eliminating  Unitful  literals from functions Other Restrictions Unitful  provides non-scalar units such as  dBm   °C  etc At this time  ModelingToolkit  only supports scalar quantities Additionally angular degrees  °  are not supported because trigonometric functions will treat plain numerical values as radians which would lead systems validated using degrees to behave erroneously when being solved Troubleshooting  Gotchas If a system fails to validate due to unit issues at least one warning message will appear including a line number as well as the unit types and expressions that were in conflict Some system constructors re-order equations before the unit checking can be done in which case the equation numbers may be inaccurate The printed expression that the problem resides in is always correctly shown Symbolic exponents for unitful variables  are  supported ex  P^γ  in thermodynamics However this means that  ModelingToolkit  cannot reduce such expressions to  Unitful.Unitlike  subtypes at validation time because the exponent value is not available In this case  ModelingToolkit.get_unit  is type-unstable yielding a symbolic result which can still be checked for symbolic equality with  ModelingToolkit.equivalent  Parameter  Initial Condition Values Parameter and initial condition values are supplied to problem constructors as plain numbers with the understanding that they have been converted to the appropriate units This is done for simplicity of interfacing with optimization solvers Some helper function for dealing with value maps Recommended usage"},{"doctype":"documentation","id":"references/NonlinearSolve.DESolution","title":"DESolution","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.InterpolatingAdjoint","title":"InterpolatingAdjoint","text":"g_mixing! du u p t du p u p u du p u p u nothing InterpolatingAdjoint  AbstractAdjointSensitivityAlgorithm An implementation of adjoint sensitivity analysis which uses the interpolation of the forward solution for the reverse solve vector-Jacobian products By default it requires a dense solution of the forward pass and will internally ignore saving arguments during the gradient calculation When checkpointing is enabled it will only require the memory to interpolate between checkpoints Constructor Keyword Arguments autodiff  Use automatic differentiation for constructing the Jacobian if the Jacobian needs to be constructed  Defaults to  true  chunk_size  Chunk size for forward-mode differentiation if full Jacobians are built  autojacvec=false  and  autodiff=true  Default is  0  for automatic choice of chunk size diff_type  The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with  autodiff=false  autojacvec  Calculate the vector-Jacobian product  J'*v  via automatic differentiation with special seeding The default is  true  The total set of choices are false  the Jacobian is constructed via FiniteDiff.jl true  the Jacobian is constructed via ForwardDiff.jl TrackerVJP  Uses Tracker.jl for the vjp ZygoteVJP  Uses Zygote.jl for the vjp EnzymeVJP  Uses Enzyme.jl for the vjp ReverseDiffVJP(compile=false  Uses ReverseDiff.jl for the vjp  compile  is a boolean for whether to precompile the tape which should only be done if there are no branches  if  or  while  statements in the  f  function checkpointing  whether checkpointing is enabled for the reverse pass Defaults to  true  noise  Calculate the vector-Jacobian product  J'*v  of the diffusion term of an SDE via automatic differentiation with special seeding The default is  true  The total set of choices are false  the Jacobian is constructed via FiniteDiff.jl true  the Jacobian is constructed via ForwardDiff.jl DiffEqSensitivity.ZygoteNoise  Uses Zygote.jl for the vjp DiffEqSensitivity.ReverseDiffNoise(compile=false  Uses ReverseDiff.jl for the vjp  compile  is a boolean for whether to precompile the tape which should only be done if there are no branches  if  or  while  statements in the  f  function noisemixing  Handle noise processes that are not of the form  du[i  f(u[i  For example to compute the sensitivities of an SDE with diagonal diffusion correctly  noisemixing=true  must be enabled The default is  false  For more details on the vjp choices please consult the sensitivity algorithms documentation page or the docstrings of the vjp types Checkpointing To reduce the memory usage of the reverse pass  InterpolatingAdjoint  includes a checkpointing feature If  sol  is  dense  checkpointing is ignored and the continuous solution is used for calculating  u(t  at arbitrary time points If  checkpointing=true  and  sol  is not  dense  then dense intervals between  sol.t[i  and  sol.t[i+1  are reconstructed on-demand for calculating  u(t  at arbitrary time points This reduces the total memory requirement to only the cost of holding the dense solution over the largest time interval in terms of number of required steps The total compute cost is no more than double the original forward compute cost SciMLProblem Support This  sensealg  only supports  ODEProblem s  SDEProblem s and  RODEProblem s This  sensealg  supports callbacks events References Rackauckas C and Ma Y and Martensen J and Warner C and Zubov K and Supekar R and Skinner D and Ramadhana A and Edelman A Universal Differential Equations for Scientific Machine Learning,\tarXiv:2001.04385 Hindmarsh A C and Brown P N and Grant K E and Lee S L and Serban R and Shumaker D E and Woodward C S SUNDIALS Suite of nonlinear and differential/algebraic equation solvers ACM Transactions on Mathematical Software TOMS 31 pp:363–396 2005 Rackauckas C and Ma Y and Dixit V and Guo X and Innes M and Revels J and Nyberg J and Ivaturi V A comparison of automatic differentiation and continuous sensitivity analysis for derivatives of differential equation solutions arXiv:1812.01892"},{"doctype":"documentation","id":"references/ExponentialUtilities.ExpMethodHigham2005Base","title":"ExpMethodHigham2005Base","text":"The same as  ExpMethodHigham2005  but follows  Base.exp  closer"},{"doctype":"document","id":"SymbolicNumericIntegration/index.md","title":"SymbolicNumericIntegration.jl","text":"Symbolics x julia x x x x x julia x log x julia x x log x log x y x log x log x julia x x x atan x julia x sqrt x x x log x sqrt x julia x log x log x x x julia x exp x exp x exp x x x exp x julia tan x log cos x julia sec x tan x cos x julia cosh x exp x exp x sinh x exp x cosh x julia cosh x sin x sin x sinh x cos x cosh x julia cosh x sin x sinh x sin x cosh x cos x julia log log x x log x log log x log x julia exp x exp x Inf include L convert_axiom Apostle test_axiom L bypass verbose homotopy SymbolicNumericIntegration.jl SymbolicNumericIntegration.jl  is a hybrid symbolic/numerical integration package that works on the  Julia Symbolics  expressions SymbolicNumericIntegration.jl  uses a randomized algorithm based on a hybrid of the  method of undetermined coefficients  and  sparse regression  and is able to solve a large subset of basic standard integrals polynomials exponential/logarithmic trigonometric and hyperbolic inverse trigonometric and hyperbolic rational and square root The basis of how it works and the theory of integration using the Symbolic-Numeric methods refer to  Basis of Symbolic-Numeric Integration  Function  integrate  returns the integral of a univariate expression with  constant  real or complex coefficients  integrate  returns a tuple with three values The first one is the solved integral the second one is the sum of the unsolved terms and the third value is the residual error If  integrate  is successful the unsolved portion is reported as 0 integrate  has the form  integrate(y kw  or  integrate(y x kw  where  y  is the integrand and the optional  x  is the variable of integration The keyword parameters are abstol  default  1e-6  the error tolerance to accept a solution symbolic  default  true  if true pure symbolic integration is attempted first bypass  default  false  if true the whole expression is considered at once and not per term num_steps  default  2  one plus the number of expanded basis to check if  num_steps  is 1 only the main basis is checked num_trials  default  5  the number of attempts to solve the integration numerically for each basis set show_basis  default  false  print the basis set useful for debugging Only works if  verbose  is also set homotopy  default  true  as of version 0.7.0 uses the continuous Homotopy operators to generate the integration candidates verbose  default  false  if true prints extra and voluminous debugging information radius  default  1.0  the starting radius to generate random test points opt  default  STLSQ(exp.(-10:1:0  the optimizer passed to  sparse_regression  max_basis  default  110  the maximum number of expression in the basis complex_plane  default  true  random test points are generated on the complex plane only over the real axis if  complex_plane  is  false  Testing test/runtests.jl  contains a test suite of 160 easy to moderate test integrals can be run by calling  test_integrals  Currently  SymbolicNumericIntegration.jl  solves more than 90 of its test suite Additionally 12 test suites from the  Rule-based Integrator   Rubi  are included in the  test  directory For example we can test the first one as below  Axiom  refers to the format of the test files The test suites description based on the header of the files in the Rubi directory are name id comment Apostle 1 Tom M Apostol  Calculus Volume I Second Edition 1967 Bondarenko 2 Vladimir Bondarenko Integration Problems Bronstein 3 Manuel Bronstein  Symbolic Integration Tutorial 1998 Charlwood 4 Kevin Charlwood  Integration on Computer Algebra Systems 2008 Hearn 5 Anthony Hearn  Reduce Integration Test Package Hebisch 6 Waldek Hebisch  email May 2013 Jeffrey 7 David Jeffrey  Rectifying Transformations for Trig Integration 1997 Moses 8 Joel Moses  Symbolic Integration Ph.D Thesis 1967 Stewart 9 James Stewart  Calculus 1987 Timofeev 10 A F Timofeev  Integration of Functions 1948 Welz 11 Martin Welz  posts on Sci.Math.Symbolic Webster 12 Michael Wester Citation If you use  SymbolicNumericIntegration.jl  please cite  Symbolic-Numeric Integration of Univariate Expressions based on Sparse Regression "},{"doctype":"document","id":"ModelingToolkit/basics/ContextualVariables.md","title":"Contextual Variable Types","text":"x y x x connect unit Contextual Variable Types ModelingToolkit.jl has a system of contextual variable types which allows for helping the system transformation machinery do complex manipulations and automatic detection The standard variable definition in ModelingToolkit.jl is the  variable  which is defined by  Symbolics.jl  For example This is used for the normal variable of a given system like the states of a differential equation or objective function All of the macros below support the same syntax as  variables  Parameters All modeling projects have some form of parameters  parameters  marks a variable as being the parameter of some system which allows automatic detection algorithms to ignore such variables when attempting to find the states of a system Variable metadata Experimental/TODO In many engineering systems some variables act like flows while others do not For example in circuit models you have current which flows and the related voltage which does not Or in thermal models you have heat flows In these cases the  connect  statement enforces conservation of flow between all of the connected components For example the following specifies that  x  is a 2x2 matrix of flow variables with the unit m^3/s ModelingToolkit defines  connect   unit   noise  and  description  keys for the metadata One can get and set metadata by"},{"doctype":"documentation","id":"references/ModelingToolkit.assemble_vrj","title":"assemble_vrj","text":""},{"doctype":"document","id":"Optimization/optimization_packages/mathoptinterface.md","title":"MathOptInterface.jl","text":"Pkg Pkg add sol prob Ipopt Optimizer ForwardDiff rosenbrock x p p x p x x x0 zeros _p f rosenbrock prob f x0 _p Juniper Ipopt optimizer Juniper Optimizer nl_solver MOI OptimizerWithAttributes Ipopt Optimizer opt MOI OptimizerWithAttributes optimizer nl_solver sol prob opt MathOptInterface.jl MathOptInterface  is Julia abstration layer to interface with variety of mathematical optimization solvers Installation OptimizationMOI.jl To use this package install the OptimizationMOI package Details As of now the  Optimization  interface to  MathOptInterface  implents only the  maxtime  common keyword arguments An optimizer which is implemented in the  MathOptInterface  is can be called be called directly if no optimizer options have to be defined For example using the  Ipopt.jl  optimizer The optimizer options are handled in one of two ways They can either be set via  Optimization.MOI.OptimizerWithAttributes  or as keyword argument to  solve  For example using the  Ipopt.jl  optimizer Local Optimizer Local constraint Ipopt.jl MathOptInterface Ipopt.Optimizer Ipopt is a MathOptInterface optimizer and thus its options are handled via  Optimization.MOI.OptimizerWithAttributes(Ipopt.Optimizer option_name  option_value  The full list of optimizer options can be found in the  Ipopt Documentation KNITRO.jl MathOptInterface KNITRO.Optimizer KNITRO is a MathOptInterface optimizer and thus its options are handled via  Optimization.MOI.OptimizerWithAttributes(KNITRO.Optimizer option_name  option_value  The full list of optimizer options can be found in the  KNITRO Documentation AmplNLWriter.jl MathOptInterface AmplNLWriter.Optimizer AmplNLWriter is a MathOptInterface optimizer and thus its options are handled via  Optimization.MOI.OptimizerWithAttributes(AmplNLWriter.Optimizer(algname option_name  option_value  Possible  algname s are Bonmin_jll.amplexe Couenne_jll.amplexe Ipopt_jll.amplexe SHOT_jll.amplexe To use one of the JLLs they must be added first For example  Pkg.add(\"Bonmin_jll  Juniper.jl MathOptInterface Juniper.Optimizer Juniper is a MathOptInterface optimizer and thus its options are handled via  Optimization.MOI.OptimizerWithAttributes(Ipopt.Optimizer option_name  option_value  Juniper requires the choice of a relaxation method  nl_solver  which must be a MathOptInterface-based optimizer BARON.jl MathOptInterface BARON.Optimizer BARON is a MathOptInterface optimizer and thus its options are handled via  Optimization.MOI.OptimizerWithAttributes(BARON.Optimizer option_name  option_value  The full list of optimizer options can be found in the  BARON Documentation Gradient-Based Ipopt.jl MathOptInterface Ipopt.Optimizer Ipopt is a MathOptInterface optimizer and thus its options are handled via  Optimization.MOI.OptimizerWithAttributes(Ipopt.Optimizer option_name  option_value  The full list of optimizer options can be found in the  Ipopt Documentation Global Optimizer With Constraint Equations Alpine.jl MathOptInterface Alpine.Optimizer Alpine is a MathOptInterface optimizer and thus its options are handled via  Optimization.MOI.OptimizerWithAttributes(Alpine.Optimizer option_name  option_value  The full list of optimizer options can be found in the  Alpine Documentation"},{"doctype":"documentation","id":"references/ModelingToolkit.generate_hessian","title":"generate_hessian","text":"sys dvs sys ps sys expression Val sparse kwargs Generates a function for the hessian matrix matrix of a system Extra arguments control the arguments to the internal  build_function  call"},{"doctype":"documentation","id":"references/SciMLBase.__has_analytic","title":"__has_analytic","text":""},{"doctype":"documentation","id":"references/SciMLBase.SDDEFunction","title":"SDDEFunction","text":"iip recompile f g mass_matrix I analytic nothing tgrad nothing jac nothing jvp nothing vjp nothing jac_prototype nothing sparsity jac_prototype paramjac nothing syms nothing indepsym nothing colorvec nothing SDDEFunction  AbstractSDDEFunction A representation of a SDDE function  f  defined by M du  f(u,h,p,t dt  g(u,h,p,t dW_t and all of its related functions such as the Jacobian of  f  its gradient with respect to time and more For all cases  u0  is the initial condition  p  are the parameters and  t  is the independent variable Constructor Note that only the function  f  itself is required This function should be given as  f!(du,u,h,p,t  or  du  f(u,h,p,t  See the section on  iip  for more details on in-place vs out-of-place handling The histroy function  h  acts as an interpolator over time i.e  h(t  with options matching the solution interface i.e  h(t save_idxs  2  All of the remaining functions are optional for improving or accelerating  the usage of  f  These include mass_matrix  the mass matrix  M  represented in the ODE function Can be used to determine that the equation is actually a differential-algebraic equation DAE if  M  is singular Note that in this case special solvers are required see the DAE solver page for more details https://diffeq.sciml.ai/stable/solvers/dae_solve Must be an AbstractArray or an AbstractSciMLOperator analytic(u0,p,t  used to pass an analytical solution function for the analytical  solution of the ODE Generally only used for testing and development of the solvers tgrad(dT,u,h,p,t  or dT=tgrad(u,p,t returns  frac{\\partial f(u,p,t)}{\\partial t jac(J,u,h,p,t  or  J=jac(u,p,t  returns  frac{df}{du jvp(Jv,v,h,u,p,t  or  Jv=jvp(v,u,p,t  returns the directional derivative frac{df}{du v vjp(Jv,v,h,u,p,t  or  Jv=vjp(v,u,p,t  returns the adjoint derivative frac{df}{du}^\\ast v jac_prototype  a prototype matrix matching the type that matches the Jacobian For example if the Jacobian is tridiagonal then an appropriately sized  Tridiagonal  matrix can be used as the prototype and integrators will specialize on this structure where possible Non-structured sparsity patterns should use a  SparseMatrixCSC  with a correct sparsity pattern for the Jacobian The default is  nothing  which means a dense Jacobian paramjac(pJ,h,u,p,t  returns the parameter Jacobian  frac{df}{dp  syms  the symbol names for the elements of the equation This should match  u0  in size For example if  u0  0.0,1.0  and  syms  x y  this will apply a canonical naming to the values allowing  sol[:x  in the solution and automatically naming values in plots indepsym  the canonical naming for the independent variable Defaults to nothing which internally uses  t  as the representation in any plots colorvec  a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the  jac_prototype  This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern Defaults to  nothing  which means a color vector will be internally computed on demand when required The cost of this operation is highly dependent on the sparsity pattern iip In-Place vs Out-Of-Place For more details on this argument see the ODEFunction documentation recompile Controlling Compilation and Specialization For more details on this argument see the ODEFunction documentation Fields The fields of the DDEFunction type directly match the names of the inputs"},{"doctype":"documentation","id":"references/LinearSolve.set_u","title":"set_u","text":"DocStringExtensions.MethodSignatures"},{"doctype":"documentation","id":"references/Catalyst.Expression","title":"Expression","text":""},{"doctype":"document","id":"MethodOfLines/tutorials/brusselator.md","title":"[Tutorial] ( brusselator)","text":"OrdinaryDiffEq DomainSets x y t u v Dt Differential t Dx Differential x Dy Differential y Dxx Differential x Dyy Differential y ∇² u Dxx u Dyy u brusselator_f x y t x y t x_min y_min t_min x_max y_max t_max α u0 x y t y y v0 x y t x x eq Dt u x y t v x y t u x y t u x y t α ∇² u x y t brusselator_f x y t Dt v x y t u x y t v x y t u x y t α ∇² v x y t domains x Interval x_min x_max y Interval y_min y_max t Interval t_min t_max bcs u x y u0 x y u y t u y t u x t u x t v x y v0 x y v y t v y t v x t v x t pdesys eq bcs domains x y t u x y t v x y t N dx x_max x_min N dy y_max y_min N order discretization x dx y dy t approx_order order grid_align println prob pdesys discretization println sol prob TRBDF2 saveat grid pdesys discretization discrete_x grid x discrete_y grid y discrete_t sol t solu map d sol d i grid u x y t i length sol t solv map d sol d i grid v x y t i length sol t Plots anim k length discrete_t heatmap solu k end end title discrete_t k gif anim fps anim k length discrete_t heatmap solv k end end title discrete_t k gif anim fps Tutorial   brusselator Using the Brusselator PDE as an example The Brusselator PDE is defined as follows begin{align}\n\\frac{\\partial u}{\\partial t  1  u^2v  4.4u  alpha(\\frac{\\partial^2 u}{\\partial x^2  frac{\\partial^2 u}{\\partial y^2  f(x y t)\\\\\n\\frac{\\partial v}{\\partial t  3.4u  u^2v  alpha(\\frac{\\partial^2 v}{\\partial x^2  frac{\\partial^2 v}{\\partial y^2})\n\\end{align where f(x y t  begin{cases}\n5  quad text{if  x-0.3)^2+(y-0.6)^2 ≤ 0.1^2 text and  t ≥ 1.1 \n0  quad text{else}\n\\end{cases and the initial conditions are begin{align}\nu(x y 0  22\\cdot y(1-y))^{3/2 \nv(x y 0  27\\cdot x(1-x))^{3/2}\n\\end{align with the periodic boundary condition begin{align}\nu(x+1,y,t  u(x,y,t \nu(x,y+1,t  u(x,y,t)\n\\end{align on a timespan of  t in 0,11.5  Solving with MethodOfLines With  ModelingToolkit.jl  we first symbolicaly define the system see also the docs for  PDESystem  For a list of limitations constraining which systems will work see here  limitations Method of lines discretization Then we create the discretization leaving the time dimension undiscretized by supplying  t  as an argument Optionally all dimensions can be discretized in this step just remove the argument  t  and supply  t=>dt  in the  dxs  See here  molfd for more information on the  MOLFiniteDifference  constructor arguments and options Next we discretize the system converting the  PDESystem  in to an  ODEProblem  or  NonlinearProblem  Solving the problem Now your problem can be solved with an appropriate ODE solver or Nonlinear solver if you have not supplied a time dimension in the  MOLFiniteDifference  constructor Include these solvers with  using OrdinaryDiffEq  or  using NonlinearSolve  then call  sol  solve(prob AppropriateSolver  or  sol  NonlinearSolve.solve(prob AppropriateSolver  For more information on the available solvers see the docs for  DifferentialEquations.jl   NonlinearSolve.jl  and  SteadyStateDiffEq.jl   Tsit5  is a good first choice of solver for many problems Extracting results To retrieve your solution for example for  u  use  sol[u  To get the time axis use  sol.t  Due to current limitations in the  sol  interface above 1 discretized dimension the result must be manually reshaped to correctly display the result best done with the help of the  get_discrete  helper function Here is an example of how to do this The result after plotting an animation For  u  Brusselator2Dsol_u For  v  Brusselator2Dsol_v"},{"doctype":"documentation","id":"references/DiffEqFlux.getfunc","title":"getfunc","text":""},{"doctype":"documentation","id":"references/SciMLOperators.AdjointedOperator","title":"AdjointedOperator","text":""},{"doctype":"documentation","id":"references/NeuralOperators.FourierNeuralOperator","title":"FourierNeuralOperator","text":"Fourier neural operator learns a neural operator with Dirichlet kernel to form a Fourier transformation It performs Fourier transformation across infinite-dimensional function spaces and learns better than neural operator"},{"doctype":"documentation","id":"references/NeuralOperators.OperatorKernel","title":"OperatorKernel","text":"Arguments ch  Input and output channel size for spectral convolution e.g  64=>64  modes  The Fourier modes to be preserved for spectral convolution σ  Activation function permuted  Whether the dim is permuted If  permuted=true  layer accepts data in the order of  ch  batch  otherwise the order is   ch batch  Example"},{"doctype":"documentation","id":"references/Catalyst.subnetworkmapping","title":"subnetworkmapping","text":""},{"doctype":"documentation","id":"references/SciMLBase.has_jvp","title":"has_jvp","text":""},{"doctype":"documentation","id":"references/DiffEqOperators._concretize","title":"_concretize","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.is_neg","title":"is_neg","text":""},{"doctype":"documentation","id":"references/Surrogates._construct_rbf_interp_matrix","title":"_construct_rbf_interp_matrix","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.num_types_in_tuple","title":"num_types_in_tuple","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_equality_constraints","title":"get_equality_constraints","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.inverse","title":"inverse","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleSerial","title":"EnsembleSerial","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/Optimization.get_maxiters","title":"get_maxiters","text":""},{"doctype":"documentation","id":"references/PolyChaos._hasfield","title":"_hasfield","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.AbstractBoundaryPaddedMatrix","title":"AbstractBoundaryPaddedMatrix","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.dudt_g_dgdu!","title":"dudt_g_dgdu!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.compile_affect","title":"compile_affect","text":"Returns a function that takes an integrator as argument and modifies the state with the affect"},{"doctype":"documentation","id":"references/Integrals.scale_x!","title":"scale_x!","text":""},{"doctype":"documentation","id":"references/Catalyst.DEFAULT_IV","title":"DEFAULT_IV","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractDDEFunction","title":"AbstractDDEFunction","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/Catalyst.make_mm_exp","title":"make_mm_exp","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.REAL_INPLACE_WHITE_NOISE_DIST","title":"REAL_INPLACE_WHITE_NOISE_DIST","text":""},{"doctype":"documentation","id":"references/SciMLBase.TimeGradientWrapper","title":"TimeGradientWrapper","text":""},{"doctype":"documentation","id":"references/Catalyst.ReactionComplex","title":"ReactionComplex","text":"DocStringExtensions.TypeDefinition One reaction complex Fields DocStringExtensions.TypeFields(false"},{"doctype":"documentation","id":"references/Catalyst.savegraph","title":"savegraph","text":"Given a  Graph  generated by  Graph  save the graph to the file with name  fname  and extension  fmt  Notes fmt=\"png  is the default output format Requires the Graphviz jll to be installed or Graphviz to be installed and commandline accessible"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.ou_bridge!","title":"ou_bridge!","text":""},{"doctype":"documentation","id":"references/MethodOfLines.CenterAlignedGrid","title":"CenterAlignedGrid","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.DiscreteProblemExpr","title":"DiscreteProblemExpr","text":"DiffEqBase DiffEqJump u₀map S I R parammap β γ tspan dprob js u₀map tspan parammap Generates a blank DiscreteProblem for a JumpSystem to utilize as its solving  prob.prob  This is used in the case where there are no ODEs and no SDEs associated with the system Continuing the example from the  JumpSystem  definition"},{"doctype":"documentation","id":"references/Catalyst.ReactionComplexElement","title":"ReactionComplexElement","text":"DocStringExtensions.TypeDefinition One reaction complex element Fields DocStringExtensions.TypeFields(false"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.RSWM","title":"RSWM","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.VBT_BRIDGE","title":"VBT_BRIDGE","text":""},{"doctype":"documentation","id":"references/Catalyst.get_rxexprs","title":"get_rxexprs","text":""},{"doctype":"document","id":"Optimization/index.md","title":"Optimization.jl","text":"Pkg Pkg add Optimization.jl Optimization.jl is a package with a scope that is beyond your normal global optimization package Optimization.jl seeks to bring together all of the optimization packages it can find local and global into one unified Julia interface This means you learn one package and you learn them all Optimization.jl adds a few high-level features such as integrating with automatic differentiation to make its usage fairly simple for most cases while allowing all of the options in a single unified interface Installation Assuming that you already have Julia correctly installed it suffices to import Optimization.jl in the standard way The packages relevant to the core functionality of Optimization.jl will be imported accordingly and in most cases you do not have to worry about the manual installation of dependencies However you will need to add the specific optimizer packages Overview of the Optimizers Package Local Gradient-Based Local Hessian-Based Local Derivative-Free Local Constrained Global Unconstrained Global Constrained BlackBoxOptim ❌ ❌ ❌ ❌ ✅ ❌ CMAEvolutionaryStrategy ❌ ❌ ❌ ❌ ✅ ❌ Evolutionary ❌ ❌ ❌ ❌ ✅ 🟡 Flux ✅ ❌ ❌ ❌ ❌ ❌ GCMAES ❌ ❌ ❌ ❌ ✅ ❌ MathOptInterface ✅ ✅ ✅ ✅ ✅ 🟡 MultistartOptimization ❌ ❌ ❌ ❌ ✅ ❌ Metaheuristics ❌ ❌ ❌ ❌ ✅ 🟡 NOMAD ❌ ❌ ❌ ❌ ✅ 🟡 NLopt ✅ ❌ ✅ 🟡 ✅ 🟡 Nonconvex ✅ ✅ ✅ 🟡 ✅ 🟡 Optim ✅ ✅ ✅ ✅ ✅ ✅ QuadDIRECT ❌ ❌ ❌ ❌ ✅ ❌ ✅  supported 🟡  supported in downstream library but not yet implemented in  Optimization  PR to add this functionality are welcome ❌  not supported"},{"doctype":"documentation","id":"references/ExponentialUtilities.ExpvCache","title":"ExpvCache","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.calculate_gradient","title":"calculate_gradient","text":"sys Calculate the gradient of a scalar system Returns a vector of  Num  instances The result from the first call will be cached in the system object"},{"doctype":"documentation","id":"references/Integrals.VEGAS","title":"VEGAS","text":""},{"doctype":"documentation","id":"references/Surrogates._calc_coeffs_wend","title":"_calc_coeffs_wend","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.unnamespace","title":"unnamespace","text":""},{"doctype":"documentation","id":"references/Catalyst.isweaklyreversible","title":"isweaklyreversible","text":"sir SIR β S I I ν I R β ν rcs sir subnets rn rn subnets Determine if the reaction network with the given subnetworks is weakly reversible or not Notes Requires the  incidencemat  to already be cached in  rn  by a previous call to  reactioncomplexes  For example"},{"doctype":"documentation","id":"references/DiffEqSensitivity.FORWARD_SENSITIVITY_PARAMETER_COMPATABILITY_MESSAGE","title":"FORWARD_SENSITIVITY_PARAMETER_COMPATABILITY_MESSAGE","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_continuous_events","title":"get_continuous_events","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.search_VBT","title":"search_VBT","text":""},{"doctype":"documentation","id":"references/Surrogates.RTEA","title":"RTEA","text":""},{"doctype":"documentation","id":"references/PolyChaos","title":"PolyChaos","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.isarray","title":"isarray","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_dvs","title":"get_dvs","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.bareiss_update_virtual_colswap!","title":"bareiss_update_virtual_colswap!","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.lanczos_step!","title":"lanczos_step!","text":"Take the  j th step of the Lanczos iteration"},{"doctype":"documentation","id":"references/Surrogates.RadialBasis","title":"RadialBasis","text":"Constructor for RadialBasis surrogate RadialBasis(x,y,lb,ub,rad::RadialFunction scale_factor::Float  1.0 Constructor for RadialBasis surrogate Calculates current estimate of value val with respect to the RadialBasis object"},{"doctype":"documentation","id":"references/MethodOfLines._boundary_rules","title":"_boundary_rules","text":""},{"doctype":"document","id":"SciMLBase/interfaces/Init_Solve.md","title":"The SciML init and solve Functions","text":"args kwargs args kwargs ProblemType args kwargs IteratorType SolverType SolutionType AbstractVector AlgorithmType The SciML init and solve Functions solve  function has the default definition The interface for the three functions is as follows where  ProblemType   IteratorType  and  SolutionType  are the types defined in your package To avoid method ambiguity the first argument of  solve   solve  and  init   must  be dispatched on the type defined in your package  For example do  not  define a method such as init  and the Iterator Interface init s return gives an  IteratorType  which is designed to allow the user to have more direct handling over the internal solving process Because of this internal nature the  IteratorType  has a less unified interface across problem types than other portions like  ProblemType  and  SolutionType  For example for differential equations this is the  Integrator Interface  designed for mutating solutions in a manner for callback implementation which is distinctly different from the  LinearSolve init interface  which is designed for caching efficiency with reusing factorizations solve and High-Level Handling While  init  and  solve  are the common entry point for users solver packages will mostly define dispatches on  SciMLBase.__init  and  SciMLBase.__solve  The reason is because this allows for  SciMLBase.init  and  SciMLBase.solve  to have common implementations across all solvers for doing things such as checking for common errors and throwing high level messages Solvers can opt-out of the high level error handling by directly defining  SciMLBase.init  and  SciMLBase.solve  instead though this is not recommended in order to allow for uniformity of the error messages"},{"doctype":"documentation","id":"references/Surrogates.SRBF","title":"SRBF","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.exp_generic!","title":"exp_generic!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.bareiss_update!","title":"bareiss_update!","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.construct_w","title":"construct_w","text":""},{"doctype":"documentation","id":"references/ParameterizedFunctions.build_component_funcs","title":"build_component_funcs","text":""},{"doctype":"documentation","id":"references/MethodOfLines.generate_corner_eqs!","title":"generate_corner_eqs!","text":""},{"doctype":"documentation","id":"references/LinearSolve.SciMLLinearSolveAlgorithm","title":"SciMLLinearSolveAlgorithm","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.collect_differential_variables","title":"collect_differential_variables","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.Tail8","title":"Tail8","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.namespace_parameters","title":"namespace_parameters","text":""},{"doctype":"documentation","id":"references/Catalyst.print_rxside","title":"print_rxside","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.sync_residuals!","title":"sync_residuals!","text":""},{"doctype":"document","id":"DiffEqSensitivity/ode_fitting/data_parallel.md","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"Lux DifferentialEquations Random rng Random default_rng dudt Lux Lux Dense tanh Lux Dense p st Lux setup rng dudt f u p t dudt u p st u0 Float32 prob f u0 p prob Tsit5 u0 Float32 prob f u0 p prob Tsit5 xs Float32 prob f gpu u0 gpu p prob Tsit5 DifferentialEquations OptimizationOptimJL OptimizationFlux pa u0 θ u0 pa model1 θ ensemble prob u p t u p θ θ prob_func prob i repeat prob u0 i prob u0 ensemble_prob prob prob_func prob_func sim ensemble_prob Tsit5 ensemble saveat trajectories loss_serial θ sum abs2 Array model1 θ loss_threaded θ sum abs2 Array model1 θ cb θ l l opt ADAM l1 loss_serial θ adtype optf x p loss_serial x adtype optprob optf θ res_serial optprob opt cb cb maxiters optf2 x p loss_threaded x adtype optprob2 optf2 θ res_threads optprob2 opt cb cb maxiters prob u p t u p θ θ prob_func prob i repeat prob u0 i prob u0 ensemble_prob prob prob_func prob_func sim ensemble_prob Tsit5 saveat trajectories sim ensemble_prob Tsit5 saveat trajectories Threads nthreads sim ensemble_prob Tsit5 saveat trajectories Distributed addprocs DifferentialEquations OptimizationOptimJL f u p t u p pa u0 θ u0 pa model1 θ ensemble prob f θ θ prob_func prob i repeat prob u0 i prob u0 ensemble_prob prob prob_func prob_func sim ensemble_prob Tsit5 ensemble saveat trajectories cb θ l l opt ADAM loss_distributed θ sum abs2 Array model1 θ l1 loss_distributed θ adtype optf x p loss_distributed x adtype optprob optf θ res_distributed optprob opt cb cb maxiters DifferentialEquations OptimizationOptimJL f du u p t du u p p pa u0 θ u0 pa model1 θ ensemble prob f θ θ prob_func prob i repeat prob u0 i prob u0 ensemble_prob prob prob_func prob_func sim ensemble_prob Tsit5 ensemble saveat trajectories cb θ l l opt ADAM loss_gpu θ sum abs2 Array model1 θ EnsembleGPUArray l1 loss_gpu θ adtype optf x p loss_gpu x adtype optprob optf θ res_gpu optprob opt cb cb maxiters Data-Parallel Multithreaded Distributed and Multi-GPU Batching DiffEqFlux.jl allows for data-parallel batching optimally on one computer across an entire compute cluster and batching along GPUs This can be done by parallelizing within an ODE solve or between the ODE solves The automatic differentiation tooling is compatible with the parallelism The following examples demonstrate training over a few different modes of parallelism These examples are not exhaustive Within-ODE Multithreaded and GPU Batching We end by noting that there is an alternative way of batching which can be more efficient in some cases like neural ODEs With a neural networks columns are treated independently by the properties of matrix multiplication Thus for example with  Chain  we can define an ODE and we can solve this ODE where the initial condition is a vector or we can solve this ODE where the initial condition is a matrix where each column is an independent system On the CPU this will multithread across the system due to BLAS and on GPUs this will parallelize the operations across the GPU To GPU this you'd simply move the parameters and the initial condition to the GPU This method of parallelism is optimal if all of the operations are linear algebra operations such as a neural ODE Thus this method of parallelism is demonstrated in the MNIST tutorial  mnist However this method of parallelism has many limitations First of all the ODE function is required to be written in a way that is independent across the columns Not all ODEs are written like this so one needs to be careful But additionally this method is ineffective if the ODE function has many serial operations like  u[1]*u[2  u[3  In such a case this indexing behavior will dominate the runtime and cause the parallelism to sometimes even be detrimental Out of ODE Parallelism Instead of parallelizing within an ODE solve one can parallelize the solves to the ODE itself While this will be less effective on very large ODEs like big neural ODE image classifiers this method be effective even if the ODE is small or the  f  function is not well-parallelized This kind of parallelism is done via the  DifferentialEquations.jl ensemble interface  The following examples showcase multithreaded cluster and multi)GPU parallelism through this interface Multithreaded Batching At a Glance The following is a full copy-paste example for the multithreading Distributed and GPU minibatching are described below Multithreaded Batching In-Depth In order to make use of the ensemble interface we need to build an  EnsembleProblem  The  prob_func  is the function for determining the different  DEProblem s to solve This is the place where we can randomly sample initial conditions or pull initial conditions from an array of batches in order to perform our study To do this we first define a prototype  DEProblem  Here we use the following  ODEProblem  as our base In the  prob_func  we define how to build a new problem based on the base problem In this case we want to change  u0  by a constant i.e  0.5  i/100  prob.u0  for different trajectories labelled by  i  Thus we use the  remake function from the problem interface  to do so We now build the  EnsembleProblem  with this basis Now to solve an ensemble problem we need to choose an ensembling algorithm and choose the number of trajectories to solve Here let's solve this in serial with 100 trajectories Note that  i  will thus run from  1:100  and thus running in multithreading would be This whole mechanism is differentiable so we then put it in a training loop and it soars Note that you need to make sure that  Julia's multithreading  is enabled which you can do via Distributed Batching Across a Cluster Changing to distributed computing is very simple as well The setup is all the same except you utilize  EnsembleDistributed  as the ensembler Note that for this to work you need to ensure that your processes are already started For more information on setting up processes and utilizing a compute cluster see  the official distributed documentation  The key feature to recognize is that due to the message passing required for cluster compute one needs to ensure that all of the required functions are defined on the worker processes The following is a full example of a distributed batching setup And note that only  addprocs(4  needs to be changed in order to make this demo run across a cluster For more information on adding processes to a cluster check out  ClusterManagers.jl  Minibatching Across GPUs with DiffEqGPU DiffEqGPU.jl allows for generating code parallelizes an ensemble on generated CUDA kernels This method is efficient for sufficiently small 100 ODE problems where the significant computational cost is due to the large number of batch trajectories that need to be solved This kernel-building process adds a few restrictions to the function such as requiring it has no boundschecking or allocations The following is an example of minibatch ensemble parallelism across a GPU Multi-GPU Batching DiffEqGPU supports batching across multiple GPUs See  its README  for details on setting it up"},{"doctype":"documentation","id":"references/ExponentialUtilities.alloc_mem","title":"alloc_mem","text":"Pre-allocates memory associated with matrix exponential function  method  and matrix  A  To be used in combination with  exponential "},{"doctype":"document","id":"MethodOfLines/generated/bruss_code.md","title":"[Generated Code for the Brusselator Equation] ( brusscode)","text":"Generated Code for the Brusselator Equation   brusscode Here's the generated julia code for the Brusselator  brusselator with  dx  dy  1/4"},{"doctype":"documentation","id":"references/MethodOfLines.generate_code","title":"generate_code","text":""},{"doctype":"documentation","id":"references/Catalyst.get_sexprs","title":"get_sexprs","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timepoint_quantile","title":"timepoint_quantile","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.ndsts","title":"ndsts","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.try_symbolic","title":"try_symbolic","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.contract_variables","title":"contract_variables","text":""},{"doctype":"document","id":"DiffEqFlux/examples/collocation.md","title":"Smoothed Collocation for Fast Two-Stage Training","text":"Lux OrdinaryDiffEq OptimizationFlux Plots Random rng Random default_rng u0 Float32 datasize tspan tsteps range tspan tspan length datasize trueODEfunc du u p t true_A du u true_A prob_trueode trueODEfunc u0 tspan data Array prob_trueode Tsit5 saveat tsteps randn du u data tsteps scatter tsteps data plot! tsteps u lw savefig plot tsteps du savefig dudt2 Lux ActivationFunction x x Lux Dense tanh Lux Dense loss p cost zero first p i size du _du _ dudt2 u i p st dui du i cost sum abs2 dui _du sqrt cost pinit st Lux setup rng dudt2 callback p l adtype optf x p loss x adtype optprob optf Lux ComponentArray pinit result_neuralode optprob ADAM callback callback maxiters prob_neuralode dudt2 tspan Tsit5 saveat tsteps nn_sol st prob_neuralode u0 result_neuralode u st scatter tsteps data plot! nn_sol savefig predict_neuralode p Array prob_neuralode u0 p st loss_neuralode p pred predict_neuralode p loss sum abs2 data pred loss adtype optf x p loss_neuralode x adtype optprob optf Lux ComponentArray pinit numerical_neuralode optprob ADAM cb callback maxiters nn_sol st prob_neuralode u0 numerical_neuralode u st scatter tsteps data plot! nn_sol lw savefig Flux OptimizationFlux DifferentialEquations Plots u0 Float32 datasize tspan tsteps range tspan tspan length datasize trueODEfunc du u p t true_A du u true_A prob_trueode trueODEfunc u0 tspan data Array prob_trueode Tsit5 saveat tsteps randn du u data tsteps scatter tsteps data plot! tsteps u lw plot tsteps du dudt2 Lux ActivationFunction x x Lux Dense tanh Lux Dense loss p cost zero first p i size du _du _ dudt2 u i p st dui du i cost sum abs2 dui _du sqrt cost pinit st Lux setup rng dudt2 callback p l adtype optf x p loss x adtype optprob optf Lux ComponentArray pinit result_neuralode optprob ADAM callback callback maxiters prob_neuralode dudt2 tspan Tsit5 saveat tsteps nn_sol st prob_neuralode u0 result_neuralode u st scatter tsteps data plot! nn_sol predict_neuralode p Array prob_neuralode u0 p st loss_neuralode p pred predict_neuralode p loss sum abs2 data pred loss adtype optf x p loss_neuralode x adtype optprob optf Lux ComponentArray pinit numerical_neuralode optprob ADAM cb callback maxiters nn_sol st prob_neuralode u0 numerical_neuralode u st scatter tsteps data plot! nn_sol lw Smoothed Collocation for Fast Two-Stage Training One can avoid a lot of the computational cost of the ODE solver by pretraining the neural network against a smoothed collocation of the data First the example and then an explanation Generating the Collocation The smoothed collocation is a spline fit of the datapoints which allows us to get a an estimate of the approximate noiseless dynamics We can then differentiate the smoothed function to get estimates of the derivative at each datapoint Because we have  u',u  pairs we can write a loss function that calculates the squared difference between  f(u,p,t  and  u  at each point and find the parameters which minimize this difference While this doesn't look great it has the characteristics of the full solution all throughout the timeseries but it does have a drift We can continue to optimize like this or we can use this as the initial condition to the next phase of our fitting This method then has a good global starting position making it less prone to local minima and is thus a great method to mix in with other fitting methods for neural ODEs"},{"doctype":"documentation","id":"references/ModelingToolkit.SystemStructures.algvars_range","title":"algvars_range","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.VariableOutput","title":"VariableOutput","text":""},{"doctype":"documentation","id":"references/PolyChaos.sampleMeasure","title":"sampleMeasure","text":"Univariate Draw  n  samples from the measure  m  described by its name weight function  w  domain  dom  symmetry property  symm  and if applicable parameters stored in the dictionary  d  By default an adaptive rejection sampling method is used from  AdaptiveRejectionSampling.jl  unless it is a common random variable for which  Distributions.jl  is used The function is dispatched to accept  AbstractOrthoPoly  Multivariate Multivariate extension which provides array of samples with  n  rows and as many columns as the multimeasure has univariate measures"},{"doctype":"documentation","id":"references/ModelingToolkit.VariableInput","title":"VariableInput","text":""},{"doctype":"documentation","id":"references/NeuralPDE.NonAdaptiveLoss","title":"NonAdaptiveLoss","text":"A way of weighting the components of the loss function in the total sum that does not change during optimization pde_loss_weights  either a scalar which will be broadcast or vector the size of the number of PDE equations which describes the weight the respective PDE loss has in the full loss sum bc_loss_weights  either a scalar which will be broadcast or vector the size of the number of BC equations which describes the weight the respective BC loss has in the full loss sum additional_loss_weights  a scalar which describes the weight the additional loss function has in the full loss sum"},{"doctype":"documentation","id":"references/SciMLBase.CallbackSet","title":"CallbackSet","text":"DocStringExtensions.TypeDefinition Multiple callbacks can be chained together to form a  CallbackSet  A  CallbackSet  is constructed by passing the constructor  ContinuousCallback   DiscreteCallback   VectorContinuousCallback  or other  CallbackSet  instances You can pass as many callbacks as you like When the solvers encounter multiple callbacks the following rules apply ContinuousCallback s and  VectorContinuousCallback s are applied before  DiscreteCallback s This is because they often implement event-finding that will backtrack the timestep to smaller than  dt  For  ContinuousCallback s and  VectorContinuousCallback s the event times are found by rootfinding and only the first  ContinuousCallback  or  VectorContinuousCallback  affect is applied The  DiscreteCallback s are then applied in order Note that the ordering only matters for the conditions if a previous callback modifies  u  in such a way that the next callback no longer evaluates condition to  true  its  affect  will not be applied"},{"doctype":"documentation","id":"references/DiffEqSensitivity.CallbackSensitivityFunction","title":"CallbackSensitivityFunction","text":""},{"doctype":"documentation","id":"references/LinearSolve.GenericLUFactorization","title":"GenericLUFactorization","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.is_delay_var","title":"is_delay_var","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.eqeq_dependencies","title":"eqeq_dependencies","text":"eqdeps T vardeps T T Integer eqeqdep odesys odesys Calculate a  LightGraph.SimpleDiGraph  that maps each equation to equations they depend on Notes The  fadjlist  of the  SimpleDiGraph  maps from an equation to the equations that modify variables it depends on The  badjlist  of the  SimpleDiGraph  maps from an equation to equations that depend on variables it modifies Example Continuing the example of  equation_dependencies"},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timestep_weighted_meancov","title":"timestep_weighted_meancov","text":""},{"doctype":"documentation","id":"references/Catalyst.isequal_ignore_names","title":"isequal_ignore_names","text":"Tests whether the underlying species parameters and reactions are the same in the two  ReactionSystem s Ignores the names of the systems in testing equality Notes Does not  currently simplify rates so a rate of  A^2+2*A+1  would be considered different than  A+1)^2  Does not include  defaults  in determining equality"},{"doctype":"documentation","id":"references/SciMLOperators.cache_operator","title":"cache_operator","text":"Allocate caches for a SciMLOperator for fast evaluation arguments L  AbstractSciMLOperator u  AbstractVector argument to L"},{"doctype":"documentation","id":"references/ModelingToolkit.MTKParameterCtx","title":"MTKParameterCtx","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.asdigraph","title":"asdigraph","text":"g sys variables sys equationsfirst dg digr Convert a  BipartiteGraph  to a  LightGraph.SimpleDiGraph  Notes The resulting  SimpleDiGraph  unifies the two sets of vertices equations and then states in the case it comes from  asgraph  producing one ordered set of integer vertices  SimpleDiGraph  does not support two distinct collections of vertices so they must be merged variables  gives the variables that  g  is associated with usually the  states  of a system equationsfirst  default is  true  gives whether the  BipartiteGraph  gives a mapping from equations to variables they depend on  true  as calculated by  asgraph  or whether it gives a mapping from variables to the equations that modify them as calculated by  variable_dependencies  Example Continuing the example in  asgraph"},{"doctype":"documentation","id":"references/SciMLBase.reeval_internals_due_to_modification!","title":"reeval_internals_due_to_modification!","text":"Recalculate interpolation data and update ODE integrator after changes by callbacks"},{"doctype":"documentation","id":"references/ExponentialUtilities.phiv_timestep!","title":"phiv_timestep!","text":"Non-allocating version of  phiv_timestep "},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.𝑑edges","title":"𝑑edges","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.SystemStructures.isdervar","title":"isdervar","text":""},{"doctype":"documentation","id":"references/Integrals.DESolution","title":"DESolution","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.kernel","title":"kernel","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.coeff","title":"coeff","text":"Helper functions that returns the real part if that is what is required for Hermitian matrices otherwise returns the value as-is"},{"doctype":"documentation","id":"references/ModelingToolkit.independent_variable","title":"independent_variable","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleProblem","title":"EnsembleProblem","text":"DocStringExtensions.TypeDefinition"},{"doctype":"document","id":"Integrals/basics/IntegralProblem.md","title":"Integral Problems","text":"Integral Problems"},{"doctype":"documentation","id":"references/LinearSolve.init_cacheval","title":"init_cacheval","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.determine_chunksize","title":"determine_chunksize","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.getcoeff","title":"getcoeff","text":""},{"doctype":"documentation","id":"references/RecursiveArrayTools.recursivecopy","title":"recursivecopy","text":"b AbstractArray T N a AbstractArray T N A recursive  copy  function Acts like a  deepcopy  on arrays of arrays but like  copy  on arrays of scalars"},{"doctype":"documentation","id":"references/ExponentialUtilities.arnoldi","title":"arnoldi","text":"Performs  m  anoldi iterations to obtain the Krylov subspace K_m(A,b The n x m  1 basis vectors  getV(Ks  and the m  1 x m upper Hessenberg matrix  getH(Ks  are related by the recurrence formula iop  determines the length of the incomplete orthogonalization procedure   The default value of 0 indicates full Arnoldi For symmetric/Hermitian  A   iop  will be ignored and the Lanczos algorithm will be used instead Refer to  KrylovSubspace  for more information regarding the output Happy-breakdown occurs whenver  norm(v_j  tol  opnorm  in this case the dimension of  Ks  is smaller than  m  Koskela A 2015 Approximating the matrix exponential of an advection-diffusion operator using the incomplete orthogonalization method In Numerical Mathematics and Advanced Applications-ENUMATH 2013 pp 345-353 Springer Cham"},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timeseries_point_meanvar","title":"timeseries_point_meanvar","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.is_bound","title":"is_bound","text":"Determine whether or not input/output variable  u  is bound within the system i.e if it's to be considered internal to  sys  A variable/signal is considered bound if it appears in an equation together with variables from other subsystems The typical usecase for this function is to determine whether the input to an IO component is connected to another component or if it remains an external input that the user has to supply before simulating the system See also  bound_inputs   unbound_inputs   bound_outputs   unbound_outputs"},{"doctype":"documentation","id":"references/DiffEqSensitivity.RODEParamJacobianWrapper","title":"RODEParamJacobianWrapper","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.NULL_AFFECT","title":"NULL_AFFECT","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.ParamNonDiagNoiseGradientWrapper","title":"ParamNonDiagNoiseGradientWrapper","text":""},{"doctype":"document","id":"Surrogates/radials.md","title":"radials","text":"Radial Surrogates The Radial Basis Surrogate model represents the interpolating function as a linear combination of basis functions one for each training point Let's start with something easy to get our hands dirty I want to build a surrogate for f(x  log(x)*x^2+x^3 Let's choose the Radial Basis Surrogate for 1D First of all we have to import these two packages  Surrogates  and  Plots  We choose to sample f in 30 points between 5 to 25 using  sample  function The sampling points are chosen using a Sobol sequence this can be done by passing  SobolSample  to the  sample  function Building Surrogate With our sampled points we can build the  Radial Surrogate  using the  RadialBasis  function We can simply calculate  radial_surrogate  for any value We can also use cubic radial basis functions Currently available radial basis functions are  linearRadial  the default  cubicRadial   multiquadricRadial  and  thinplateRadial  Now we will simply plot  radial_surrogate  Optimizing Having built a surrogate we can now use it to search for minimas in our original function  f  To optimize using our surrogate we call  surrogate_optimize  method We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique Radial Basis Surrogate tutorial ND First of all we will define the  Booth  function we are going to build surrogate for f(x  x_1  2*x_2  7)^2  2*x_1  x_2  5)^2 Notice one how its argument is a vector of numbers one for each coordinate and its output is a scalar Sampling Let's define our bounds this time we are working in two dimensions In particular we want our first dimension  x  to have bounds  5 10  and  0 15  for the second dimension We are taking 80 samples of the space using Sobol Sequences We then evaluate our function on all of the sampling points Building a surrogate Using the sampled points we build the surrogate the steps are analogous to the 1-dimensional case Optimizing With our surrogate we can now search for the minimas of the function Notice how the new sampled points which were created during the optimization process are appended to the  xys  array This is why its size changes"},{"doctype":"documentation","id":"references/PolyChaos.genHermiteOrthoPoly","title":"genHermiteOrthoPoly","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.numericrstoich","title":"numericrstoich","text":""},{"doctype":"documentation","id":"references/PolyChaos.calculateMultiIndices","title":"calculateMultiIndices","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.tearEquations!","title":"tearEquations!","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractDAEAlgorithm","title":"AbstractDAEAlgorithm","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/NonlinearSolve.NewtonImmutableSolver","title":"NewtonImmutableSolver","text":""},{"doctype":"documentation","id":"references/MethodOfLines.unitindex","title":"unitindex","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.DiffEqOperatorCombination","title":"DiffEqOperatorCombination","text":""},{"doctype":"documentation","id":"references/DiffEqFlux._default_continuity_loss","title":"_default_continuity_loss","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.SDEAdjointProblem","title":"SDEAdjointProblem","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractNoiseProcess","title":"AbstractNoiseProcess","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAlgorithm","title":"EnsembleAlgorithm","text":"DocStringExtensions.TypeDefinition"},{"doctype":"document","id":"DiffEqOperators/operators/vector_jacobian_product.md","title":"Vector-Jacobian Product Operators","text":"T f u AbstractArray p nothing t Union Nothing Number nothing autodiff ishermitian opnorm Vector-Jacobian Product Operators The  VecJacOperator  is a linear operator  J'*v  where  J  acts like  df/du  for some function  f(u,p,t  For in-place operations  mul!(w,J,v   f  is an in-place function  f(du,u,p,t  Note This operator is available when  Zygote  is imported"},{"doctype":"document","id":"NeuralPDE/pinn/poisson.md","title":"Poisson Equation","text":"Flux GalacticOptimJL Interval infimum supremum x y u Dxx Differential x Dyy Differential y eq Dxx u x y Dyy u x y sin pi x sin pi y bcs u y u y sin pi sin pi y u x u x sin pi x sin pi domains x Interval y Interval Flux σ Flux σ initθ Float64 dx discretization dx init_params initθ pde_system eq bcs domains x y u x y prob pde_system discretization opt GalacticOptimJL BFGS callback p l println l res prob opt callback callback maxiters discretization Plots xs ys infimum d domain dx supremum d domain d domains analytic_sol_func x y sin pi x sin pi y pi u_predict reshape first x y res minimizer x xs y ys length xs length ys u_real reshape analytic_sol_func x y x xs y ys length xs length ys diff_u abs u_predict u_real p1 plot xs ys u_real linetype contourf title p2 plot xs ys u_predict linetype contourf title p3 plot xs ys diff_u linetype contourf title plot p1 p2 p3 Flux GalacticOptimJL Interval infimum supremum x y u Dxx x Dyy y eq Dxx u x y Dyy u x y sin pi x sin pi y bcs u y u y sin pi sin pi y u x u x sin pi x sin pi domains x Interval y Interval Flux σ Flux σ initθ Float64 dx discretization dx init_params initθ pde_system eq bcs domains x y u x y prob pde_system discretization opt GalacticOptimJL BFGS callback p l println l res prob opt callback callback maxiters discretization xs ys infimum d domain dx supremum d domain d domains analytic_sol_func x y sin pi x sin pi y pi u_predict reshape first x y res minimizer x xs y ys length xs length ys u_real reshape analytic_sol_func x y x xs y ys length xs length ys diff_u abs u_predict u_real p1 plot xs ys u_real linetype contourf title p2 plot xs ys u_predict linetype contourf title p3 plot xs ys diff_u linetype contourf title plot p1 p2 p3 Poisson Equation In this example we will solve a Poisson equation ∂^2_x u(x y  ∂^2_y u(x y   sin(\\pi x sin(\\pi y   with the boundary conditions begin{align*}\nu(0 y  0  \nu(1 y   sin(\\pi sin(\\pi y  \nu(x 0  0  \nu(x 1    sin(\\pi x sin(\\pi  \n\\end{align on the space domain x in 0 1    y in 0 1   with grid discretization  dx  0.1  We will use physics-informed neural networks Copy-Pastable Code Detailed Description The ModelingToolkit PDE interface for this example looks like this Here we define the neural network where the input of NN equals the number of dimensions and output equals the number of equations in the system Convert weights of neural network from Float32 to Float64 in order to all inner calculation will be with Float64 Here we build PhysicsInformedNN algorithm where  dx  is the step of discretization  strategy  stores information for choosing a training strategy and  init_params initθ  initial parameters of neural network As described in the API docs we now need to define the  PDESystem  and create PINNs problem using the  discretize  method Here we define the callback function and the optimizer And now we can solve the PDE using PINNs with the number of epochs  maxiters=1000  We can plot the predicted solution of the PDE and compare it with the analytical solution in order to plot the relative error poissonplot"},{"doctype":"documentation","id":"references/CommonSolve","title":"CommonSolve","text":""},{"doctype":"document","id":"GlobalSensitivity/methods/delta.md","title":"Delta Moment-Independent Method","text":"T nboot Int conf_level Float64 Ygrid_length Int num_classes T Test ishi X A B sin X A sin X B X sin X lb ones π ub ones π m ishi fill lb ub N Delta Moment-Independent Method DeltaMoment  has the following keyword arguments nboot  number of bootstrap repetions Defaults to  500  conf_level  the level used for confidence interval calculation with bootstrap Default value of  0.95  Ygrid_length  number of quadrature points to consider when performing the kernel density estimation and the integration steps Should be a power of 2 for efficient FFT in kernel density estimates Defaults to  2048  num_classes  Determine how many classes to split each factor into to when generating distributions of model output conditioned on class Method Details The Delta moment-independent method relies on new estimators for density-based statistics  It allows for the estimation of both distribution-based sensitivity measures and of sensitivity measures that look at contributions to a specific moment One of the primary advantage of this method is the independence of computation cost from the number of parameters Note DeltaMoment  only works for scalar output API Example"},{"doctype":"documentation","id":"references/SciMLBase.AbstractDDEAlgorithm","title":"AbstractDDEAlgorithm","text":"DocStringExtensions.TypeDefinition"},{"doctype":"document","id":"Catalyst/tutorials/compositional_modeling.md","title":"Compositional Modeling of Reaction Systems","text":"rn rn Plots GraphRecipes plot TreePlot rn method tree fontsize nodeshape ellipse plot TreePlot repressilator method tree fontsize nodeshape ellipse model Compositional Modeling of Reaction Systems Catalyst supports the construction of models in a compositional fashion based on ModelingToolkit's subsystem functionality In this tutorial we'll see how we can construct the earlier repressilator model by composing together three identically repressed genes and how to use compositional modeling to create compartments Compositional Modeling Tooling Catalyst supports two ModelingToolkit interfaces for composing multiple  ReactionSystem s together into a full model The first mechanism for extending a system is the  extend  command with reactions Here we extended  basern  with  newrn  giving a system with all the reactions Note if a name is not specified via  named  or the  name  keyword then  rn  will have the same name as  newrn  The second main compositional modeling tool is the use of subsystems Suppose we now add to  basern  two subsystems  newrn  and  newestrn  we get a different result with reactions Here we have created a new  ReactionSystem  that adds  newrn  and  newestrn  as subsystems of  basern  The variables and parameters in the sub-systems are considered distinct from those in other systems and so are namespaced i.e prefaced by the name of the system they come from We can see the subsystems of a given system by They naturally form a tree-like structure rn network with subsystems We could also have directly constructed  rn  using the same reaction as in  basern  as with reactions Catalyst provides several different accessors for getting information from a single system or all systems in the tree To get the species parameters and equations  only  within a given system i.e ignoring subsystems we can use To see all the species parameters and reactions in the tree we can use If we want to collapse  rn  down to a single system with no subsystems we can use where but More about ModelingToolkit's interface for compositional modeling can be found in the  ModelingToolkit docs  Compositional Model of the Repressilator Let's apply the tooling we've just seen to create the repressilator in a more modular fashion We start by defining a function that creates a negatively repressed gene taking the repressor as input Here we assume the user will pass in the repressor species as a ModelingToolkit variable and specify a name for the network We use Catalyst's interpolation ability to substitute the value of these variables into the DSL see  Interpolation of Julia Variables  To make the repressilator we now make three genes and then compose them together with Notice in this system each gene is a child node in the system graph of the repressilator repressilator tree plot In building the repressilator we needed to use two new features First we needed to create a symbolic variable that corresponds to the protein produced by the third gene before we created the corresponding system We did this via  variables t G3₊P(t  We also needed to set the scope where each repressor lived Here  ParentScope(G3₊P   ParentScope(G1.P  and  ParentScope(G2.P  signal Catalyst that these variables will come from parallel systems in the tree that have the same parent as the system being constructed in this case the top-level  repressilator  system Compartment-based Models Finally let's see how we can make a compartment-based model Let's create a simple eukaryotic gene expression model with negative feedback by protein dimers Transcription and gene inhibition by the protein dimer occur in the nucleus translation and dimerization occur in the cytosol and nuclear import and export reactions couple the two compartments We'll include volume parameters for the nucleus and cytosol and assume we are working with species having units of number of molecules Rate constants will have their common concentration units i.e if  V  denotes the volume of a compartment then Reaction Type Example Rate Constant Units Effective rate constant units of per time Zero order  varnothing overset{\\alpha}{\\to A  concentration  time  alpha V  First order  A overset{\\beta}{\\to B  time)⁻¹  beta  Second order  A  B overset{\\gamma}{\\to C  concentration × time)⁻¹  gamma/V  In our model we'll therefore add the conversions of the last column to properly account for compartment volumes A graph of the resulting network is graph of gene regulation model"},{"doctype":"documentation","id":"references/RecursiveArrayTools.recursive_unitless_eltype","title":"recursive_unitless_eltype","text":"a Grabs the unitless element type For example if ones has a  Array{Array{Float64,N},N  this will return  Array{Float64,N "},{"doctype":"documentation","id":"references/ModelingToolkit.AliasGraphKeySet","title":"AliasGraphKeySet","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.UniformKernel","title":"UniformKernel","text":""},{"doctype":"documentation","id":"references/Catalyst.VariableBCSpecies","title":"VariableBCSpecies","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractEnsembleProblem","title":"AbstractEnsembleProblem","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/SciMLBase.__has_colorvec","title":"__has_colorvec","text":""},{"doctype":"document","id":"DiffEqSensitivity/training_tips/multiple_nn.md","title":"Simultaneous Fitting of Multiple Neural Networks","text":"Lux Optimizaton OptimizationOptimJL DifferentialEquations Random rng Random default_rng fitz du u p t v w u a b τinv l p du v v w l du τinv v a b w p_ Float32 u0 tspan prob fitz u0 tspan p_ sol prob Tsit5 saveat X Array sol Xₙ X Float32 randn eltype X size X NN_1 Lux Lux Dense tanh Lux Dense p1 st1 Lux setup rng NN_1 NN_2 Lux Lux Dense tanh Lux Dense p2 Lux setup rng NN_2 scaling_factor p1 Lux ComponentArray p1 p2 Lux ComponentArray p2 p Lux ComponentArray p1 p1 p Lux ComponentArray p p2 dudt_ u p t v w u z1 NN_1 v w p p1 st1 z2 NN_2 v w t p p2 st2 z1 scaling_factor z2 prob_nn dudt_ u0 tspan p sol_nn prob_nn Tsit5 saveat sol t predict θ Array prob_nn Vern7 p θ saveat sol t abstol reltol sensealg autojacvec loss θ pred predict θ sum abs2 Xₙ pred pred loss p losses callback θ l pred push! losses l length losses println losses end adtype optf x p loss x adtype optprob optf p res1_uode optprob ADAM cb callback maxiters optprob2 optf res1_uode u res2_uode optprob2 BFGS maxiters Simultaneous Fitting of Multiple Neural Networks In many cases users are interested in fitting multiple neural networks or parameters simultaneously This tutorial addresses how to perform this kind of study The following is a fully working demo on the Fitzhugh-Nagumo ODE The key is that  Optimization.solve  acts on a single parameter vector  p  Thus what we do here is concatenate all of the parameters into a single vector  p  p1;p2;scaling_factor  and then train on this parameter vector Whenever we need to evaluate the neural networks we cut the vector and grab the portion that corresponds to the neural network For example the  p1  portion is  p[1:length(p1  which is why the first neural network's evolution is written like  NN_1([v,w p[1:length(p1  This method is flexible to use with many optimizers and in fairly optimized ways The allocations can be reduced by using  view p[1:length(p1  We can also see with the  scaling_factor  that we can grab parameters directly out of the vector and use them as needed"},{"doctype":"documentation","id":"references/SciMLBase.has_Wfact_t","title":"has_Wfact_t","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.INPLACE_VBT_BRIDGE","title":"INPLACE_VBT_BRIDGE","text":""},{"doctype":"documentation","id":"references/Surrogates.lobachevsky_integral","title":"lobachevsky_integral","text":"lobachevsky_integral(loba::LobachevskySurrogate,lb,ub Calculates the integral of the Lobachevsky surrogate which has a closed form"},{"doctype":"documentation","id":"references/NeuralPDE.get_ε","title":"get_ε","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.topsort_equations","title":"topsort_equations","text":"DocStringExtensions.MethodSignatures Use Kahn's algorithm to topologically sort observed equations Example"},{"doctype":"documentation","id":"references/Catalyst.hill","title":"hill","text":"A Hill rate function"},{"doctype":"documentation","id":"references/PolyChaos._throwError","title":"_throwError","text":""},{"doctype":"documentation","id":"references/PolyChaos.Measure","title":"Measure","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.check_for_g","title":"check_for_g","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.AbstractCosWindowing","title":"AbstractCosWindowing","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.LocalScope","title":"LocalScope","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractLinearAlgorithm","title":"AbstractLinearAlgorithm","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/NonlinearSolve.alg_autodiff","title":"alg_autodiff","text":""},{"doctype":"documentation","id":"references/Integrals.HCubatureJL","title":"HCubatureJL","text":""},{"doctype":"documentation","id":"references/Surrogates._l","title":"_l","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.MAXITERS_EXCEED","title":"MAXITERS_EXCEED","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.calc_J","title":"calc_J","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.ischeckpointing","title":"ischeckpointing","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.Logger","title":"Logger","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.constructRadauIIA5","title":"constructRadauIIA5","text":""},{"doctype":"documentation","id":"references/Catalyst.get_combinatoric_ratelaws","title":"get_combinatoric_ratelaws","text":"Returns true if the default for the system is to rescale ratelaws see https://catalyst.sciml.ai/dev/tutorials/using_catalyst/#Reaction-rate-laws-used-in-simulations for details Can be overriden via passing  combinatoric_ratelaws  to  convert  or the  Problem  functions"},{"doctype":"documentation","id":"references/SciMLOperators.TransposedOperator","title":"TransposedOperator","text":""},{"doctype":"documentation","id":"references/ParameterizedFunctions.ode_symbol_findreplace","title":"ode_symbol_findreplace","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.componentwise_vectors_timestep","title":"componentwise_vectors_timestep","text":""},{"doctype":"documentation","id":"references/PolyChaos.HermiteOrthoPoly","title":"HermiteOrthoPoly","text":""},{"doctype":"document","id":"DiffEqSensitivity/sde_fitting/optimization_sde.md","title":"Optimization of Stochastic Differential Equations","text":"DifferentialEquations Plots lotka_volterra! du u p t x y u α β γ δ p du dx α x β x y du dy δ x y γ y u0 tspan multiplicative_noise! du u p t x y u du p x du p y p prob lotka_volterra! multiplicative_noise! u0 tspan p sol prob plot sol Statistics ensembleprob prob sol ensembleprob SOSRI saveat trajectories truemean mean sol dims truevar sol dims loss p tmp_prob prob p p ensembleprob tmp_prob tmp_sol ensembleprob SOSRI saveat trajectories arrsol Array tmp_sol sum abs2 truemean mean arrsol dims sum abs2 truevar arrsol dims arrsol cb2 p l arrsol p l means mean arrsol dims arrsol dims p1 plot sol t means lw scatter! p1 sol t truemean p2 plot sol t lw scatter! p2 sol t truevar p plot p1 p2 layout display p OptimizationOptimJL pinit adtype optf x p loss x adtype optprob optf pinit res optprob ADAM cb cb2 maxiters p l DifferentialEquations OptimizationJL Plots lotka_volterra! du u p t x y u α β δ γ p du dx α x β x y du dy δ y γ x y lotka_volterra_noise! du u p t du u du u u0 tspan p prob_sde lotka_volterra! lotka_volterra_noise! u0 tspan predict_sde p Array prob_sde SOSRI p p sensealg saveat loss_sde p sum abs2 x x predict_sde p callback p l display l remade_solution prob_sde p p SOSRI saveat plt plot remade_solution ylim display plt adtype optf x p loss_sde x adtype optprob optf p result_sde optprob ADAM cb callback maxiters Optimization of Stochastic Differential Equations Here we demonstrate  sensealg  ForwardDiffSensitivity  provided by DiffEqSensitivity.jl for forward-mode automatic differentiation of a small stochastic differential equation For large parameter equations like neural stochastic differential equations you should use reverse-mode automatic differentiation However forward-mode can be more efficient for low numbers of parameters 100 Note the default is reverse-mode AD which is more suitable for things like neural SDEs Example 1 Fitting Data with SDEs via Method of Moments and Parallelism Let's do the most common scenario fitting data Let's say our ecological system is a stochastic process Each time we solve this equation we get a different solution so we need a sensible data source Let's assume that we are observing the seasonal behavior of this system and have 10,000 years of data corresponding to 10,000 observations of this timeseries We can utilize this to get the seasonal means and variances To simulate that scenario we will generate 10,000 trajectories from the SDE to build our dataset From here we wish to utilize the method of moments to fit the SDE's parameters Thus our loss function will be to solve the SDE a bunch of times and compute moment equations and use these as our loss against the original series We then plot the evolution of the means and variances to verify the fit For example We can then use  Optimization.solve  to fit the SDE The final print out was Notice that  both the parameters of the deterministic drift equations and the   stochastic portion the diffusion equation are fit through this process  Also notice that the final fit of the moment equations is close The time for the full fitting process was approximately 4 minutes Example 2 Fitting SDEs via Bayesian Quasi-Likelihood Approaches An inference method which can be much more efficient in many cases is the quasi-likelihood approach This approach matches the random likelihood of the SDE output with the random sampling of a Bayesian inference problem to more efficiently directly estimate the posterior distribution For more information please see  the Turing.jl Bayesian Differential Equations tutorial Example 3 Controlling SDEs to an objective In this example we will find the parameters of the SDE that force the solution to be close to the constant 1 For this training process because the loss function is stochastic we will use the  ADAM  optimizer from Flux.jl The  Optimization.solve  function is the same as before However to speed up the training process we will use a global counter so that way we only plot the current results every 10 iterations This looks like Let's optimize"},{"doctype":"document","id":"DiffEqOperators/operators/operator_overview.md","title":"Operator Overview","text":"Operator Overview The operators in DiffEqOperators.jl are instantiations of the  AbstractSciMLOperator  interface This is documented in  SciMLBase  Thus each of the operators have the functions and traits as defined for the operator interface In addition the DiffEqOperators.jl operators satisfy the following properties Derivative  Boundary gives a GhostDerivative operator representing a derivative operator which respects boundary conditions Boundary conditions generate extended vectors in a non-allocating form Operators can be concretized into matrices Operator Compositions Multiplying two DiffEqOperators will build a  DiffEqOperatorComposition  while adding two DiffEqOperators will build a  DiffEqOperatorCombination  Multiplying a DiffEqOperator by a scalar will produce a  DiffEqScaledOperator  All will inherit the appropriate action Efficiency of Composed Operator Actions Composed operator actions utilize NNLib.jl in order to do cache-efficient convolution operations in higher-dimensional combinations"},{"doctype":"documentation","id":"references/DiffEqFlux.DeterministicCNF","title":"DeterministicCNF","text":"model tspan basedist nothing monte_carlo args kwargs Constructs a continuous-time recurrent neural network also known as a neural ordinary differential equation neural ODE with fast gradient calculation via adjoints 1 and specialized for density estimation based on continuous normalizing flows CNF 2 with a direct computation of the trace of the dynamics jacobian At a high level this corresponds to the following steps Parameterize the variable of interest x(t as a function f(z θ t of a base variable z(t with known density p_z Use the transformation of variables formula to predict the density p_x as a function of the density p_z and the trace of the Jacobian of f Choose the parameter θ to minimize a loss function of p_x usually the negative likelihood of the data note This layer has been deprecated in favour of  FFJORD  Use FFJORD with  monte_carlo=false  instead After these steps one may use the NN model and the learned θ to predict the density p_x for new values of x Arguments model  A Chain neural network that defines the dynamics of the model basedist  Distribution of the base variable Set to the unit normal by default tspan  The timespan to be solved on kwargs  Additional arguments splatted to the ODE solver See the  Common Solver Arguments  documentation for more details References 1 Pontryagin Lev Semenovich Mathematical theory of optimal processes CRC press 1987 2 Chen Ricky TQ Yulia Rubanova Jesse Bettencourt and David Duvenaud Neural ordinary differential equations In Proceedings of the 32nd International Conference on Neural Information Processing Systems pp 6572-6583 2018 3 Grathwohl Will Ricky TQ Chen Jesse Bettencourt Ilya Sutskever and David Duvenaud Ffjord Free-form continuous dynamics for scalable reversible generative models arXiv preprint arXiv:1810.01367 2018"},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.closure","title":"closure","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.wBcorrect!","title":"wBcorrect!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.AbstractODESystem","title":"AbstractODESystem","text":""},{"doctype":"documentation","id":"references/NeuralPDE.get_indvars_ex","title":"get_indvars_ex","text":""},{"doctype":"documentation","id":"references/SciMLBase.set_ut!","title":"set_ut!","text":"Set current state of the  integrator  to  u  and  t"},{"doctype":"documentation","id":"references/NeuralPDE.discretize_inner_functions","title":"discretize_inner_functions","text":""},{"doctype":"documentation","id":"references/Catalyst.Html","title":"Html","text":"AST type for Graphviz's HTML-like node labels For now the HTML content is just a string"},{"doctype":"documentation","id":"references/SciMLBase.batch_func","title":"batch_func","text":""},{"doctype":"documentation","id":"references/Catalyst.Reaction","title":"Reaction","text":"k t A t B t C t D t rxs k nothing A k B nothing k A C k C A B k C A k A B C k B A k A B A C k A B C D k A C D k A A B k A B C C D k A B nothing nothing k nothing A nothing k A A A nothing only_use_rate k A B only_use_rate k A exp B C D k B nothing B nothing k t A B k t A B C D DocStringExtensions.TypeDefinition One chemical reaction Fields DocStringExtensions.TypeFields(false Examples Notes nothing  can be used to indicate a reaction that has no reactants or no products In this case the corresponding stoichiometry vector should also be set to  nothing  The three-argument form assumes all reactant and product stoichiometric coefficients are one"},{"doctype":"documentation","id":"references/SciMLBase.AbstractHistoryFunction","title":"AbstractHistoryFunction","text":"DocStringExtensions.TypeDefinition Base for types which define the history of a delay differential equation"},{"doctype":"documentation","id":"references/SciMLBase.struct_as_namedtuple","title":"struct_as_namedtuple","text":""},{"doctype":"documentation","id":"references/LinearSolve.DESolution","title":"DESolution","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.EpanechnikovKernel","title":"EpanechnikovKernel","text":""},{"doctype":"documentation","id":"references/LinearSolve.SimpleLUFactorization","title":"SimpleLUFactorization","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.has_observed","title":"has_observed","text":""},{"doctype":"documentation","id":"references/MethodOfLines","title":"MethodOfLines","text":""},{"doctype":"documentation","id":"references/NeuralPDE.get_dict_vars","title":"get_dict_vars","text":"Create dictionary variable  unique number for variable Example 1 Dict with 3 entries y  2 t  3 x  1 Example 2 Dict with 2 entries u1  1 u2  2"},{"doctype":"documentation","id":"references/DiffEqFlux.GaussianKernel","title":"GaussianKernel","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.componentwise_vectors_timepoint","title":"componentwise_vectors_timepoint","text":""},{"doctype":"documentation","id":"references/Catalyst.processmult","title":"processmult","text":""},{"doctype":"documentation","id":"references/SciMLBase.__has_paramjac","title":"__has_paramjac","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.CompoundPoissonBridge","title":"CompoundPoissonBridge","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.perpsize","title":"perpsize","text":"the size of A perpendicular to dim"},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.require_complete","title":"require_complete","text":""},{"doctype":"documentation","id":"references/QuasiMonteCarlo.GoldenSample","title":"GoldenSample","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractSDEFunction","title":"AbstractSDEFunction","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/Catalyst.complexstoichmat","title":"complexstoichmat","text":"Given a  ReactionSystem  and vector of reaction complexes return a matrix with positive entries of size number of species by number of complexes where the non-zero positive entries in the kth column denote stoichiometric coefficients of the species participating in the kth reaction complex Notes Set sparse=true for a sparse matrix representation"},{"doctype":"documentation","id":"references/DiffEqFlux.StaticDense","title":"StaticDense","text":"StaticDense(in,out,activation=identity initW  Flux.glorot_uniform initb  Flux.zeros32 A Dense layer  activation.(W*x  b  with input size  in  and output size  out  The  activation  function defaults to  identity  meaning the layer is an affine function Initial parameters are taken to match  Flux.Dense  The internal calculations are done with  StaticArrays  for extra speed for small linear algebra operations Should only be used for input/output sizes of approximately 16 or less bias represents bias(b in the dense layer and it defaults to true Note that this function has specializations on  tanh  for a slightly faster adjoint with Zygote"},{"doctype":"documentation","id":"references/DiffEqSensitivity.ODEQuadratureAdjointSensitivityFunction","title":"ODEQuadratureAdjointSensitivityFunction","text":""},{"doctype":"documentation","id":"references/GlobalSensitivity.eFAST","title":"eFAST","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.continuous_events","title":"continuous_events","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.isvariable","title":"isvariable","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.index","title":"index","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.FORWARDDIFF_SENSITIVITY_PARAMETER_COMPATABILITY_MESSAGE","title":"FORWARDDIFF_SENSITIVITY_PARAMETER_COMPATABILITY_MESSAGE","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.AbstractBoundaryPaddedArray","title":"AbstractBoundaryPaddedArray","text":""},{"doctype":"document","id":"ParameterizedFunctions/index.md","title":"ParameterizedFunctions.jl: Simple High Level ODE Definitions","text":"Pkg Pkg add DifferentialEquations lotka_volterra d🐁 α 🐁 β 🐁 🐈 d🐈 γ 🐈 δ 🐁 🐈 α β γ δ p u0 prob lotka_volterra u0 p sol prob Tsit5 reltol abstol rober dy₁ k₁ y₁ k₃ y₂ y₃ dy₂ k₁ y₁ k₂ y₂ k₃ y₂ y₃ dy₃ k₂ y₂ k₁ k₂ k₃ prob rober sol prob ParameterizedFunctions.jl Simple High Level ODE Definitions ParameterizedFunctions.jl is a component of the SciML ecosystem which allows for easily defining parameterized ODE models in a simple syntax Installation To install ParameterizedFunctions.jl use the Julia package manager Example Contributing Please refer to the  SciML ColPrac Contributor's Guide on Collaborative Practices for Community Packages  for guidance on PRs issues and other matters relating to contributing to SciML There are a few community forums the diffeq-bridged channel in the  Julia Slack JuliaDiffEq  on Gitter on the  Julia Discourse forums see also  SciML Community page"},{"doctype":"documentation","id":"references/Catalyst.hillr_names","title":"hillr_names","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.FourierBasis","title":"FourierBasis","text":"n Constructs a Fourier basis of the form F_j(x  j is even  cos((j÷2)*x  sin((j÷2)*x  F_0(x F_1(x  F_n(x Arguments n  number of terms in the Fourier expansion"},{"doctype":"documentation","id":"references/ModelingToolkit.difference_vars!","title":"difference_vars!","text":""},{"doctype":"documentation","id":"references/RecursiveArrayTools.npartitions","title":"npartitions","text":"Retrieve number of partitions of  ArrayPartitions  in  A  or throw an error if there are  ArrayPartitions  with a different number of partitions"},{"doctype":"documentation","id":"references/SciMLBase.interpret_vars","title":"interpret_vars","text":""},{"doctype":"documentation","id":"references/SciMLBase.get_colorizers","title":"get_colorizers","text":""},{"doctype":"document","id":"ExponentialUtilities/expv.md","title":"Expv: Matrix Exponentials Times Vectors","text":"Expv Matrix Exponentials Times Vectors The main functionality of ExponentialUtilities is the computation of matrix-phi-vector products The phi functions are defined as In exponential algorithms products in the form of  ϕ_m(tA)b  is frequently encountered Instead of computing the matrix function first and then computing the matrix-vector product the common alternative is to construct a  Krylov subspace   K_m(A,b  and then approximate the matrix-phi-vector product Support for matrix-free operators You can use any object as the matrix  A  as long as it implements the following linear operator interface Base.eltype(A Base.size(A dim LinearAlgebra.mul!(y A x  for computing  y  A  x  in place LinearAlgebra.opnorm(A p=Inf  If this is not implemented or the default implementation can be slow you can manually pass in the operator norm a rough estimate is fine using the keyword argument  opnorm  LinearAlgebra.ishermitian(A  If this is not implemented or the default implementation can be slow you can manually pass in the value using the keyword argument  ishermitian  Core API Caches"},{"doctype":"documentation","id":"references/SciMLBase.done","title":"done","text":""},{"doctype":"documentation","id":"references/MethodOfLines.InteriorMap","title":"InteriorMap","text":""},{"doctype":"documentation","id":"references/SciMLBase.has_jac","title":"has_jac","text":""},{"doctype":"documentation","id":"references/NeuralPDE.get_limits","title":"get_limits","text":""},{"doctype":"documentation","id":"references/SciMLOperators.has_mul","title":"has_mul","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.UNonDiagNoiseJacobianWrapper","title":"UNonDiagNoiseJacobianWrapper","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.NeuralDAE","title":"NeuralDAE","text":"model constraints_model tspan alg nothing args sensealg kwargs model constraints_model tspan alg nothing args sensealg kwargs Constructs a neural differential-algebraic equation neural DAE Arguments model  A Chain or FastChain neural network that defines the derivative function Should take an input of size  x  and produce the residual of  f(dx,x,t  for only the differential variables constraints_model  A function  constraints_model(u,p,t  for the fixed constaints to impose on the algebraic equations tspan  The timespan to be solved on alg  The algorithm used to solve the ODE Defaults to  nothing  i.e the default algorithm from DifferentialEquations.jl sensealg  The choice of differentiation algorthm used in the backpropogation Defaults to using reverse-mode automatic differentiation via Tracker.jl kwargs  Additional arguments splatted to the ODE solver See the  Common Solver Arguments  documentation for more details"},{"doctype":"documentation","id":"references/Surrogates.lobachevsky_integrate_dimension","title":"lobachevsky_integrate_dimension","text":"lobachevsky_integrate_dimension(loba::LobachevskySurrogate,lb,ub,dimension Integrating the surrogate on selected dimension dim"},{"doctype":"documentation","id":"references/ModelingToolkit.check_units","title":"check_units","text":"Throws error if units of equations are invalid"},{"doctype":"documentation","id":"references/SciMLBase.DiffEqScaledOperator","title":"DiffEqScaledOperator","text":""},{"doctype":"documentation","id":"references/NeuralPDE.dottable_","title":"dottable_","text":"julia e sin x sin x julia Broadcast __dot__ e sin x julia e sin x Override  Broadcast.__dot  with  Broadcast.dottable(x::Function  true Example"},{"doctype":"documentation","id":"references/ModelingToolkit.get_structure","title":"get_structure","text":""},{"doctype":"document","id":"DiffEqOperators/operators/matrix_free_operators.md","title":"Matrix-Free Operators","text":"f F args N size nothing opnorm ishermitian F N Matrix-Free Operators A  MatrixFreeOperator  is a linear operator  A*u  where the action of  A  is explicitly defined by an in-place function  f(du u p t "},{"doctype":"documentation","id":"references/ModelingToolkit.isparameter","title":"isparameter","text":""},{"doctype":"documentation","id":"references/SciMLBase.EnsembleAnalysis.timepoint_meancor","title":"timepoint_meancor","text":""},{"doctype":"documentation","id":"references/Catalyst.make_reaction","title":"make_reaction","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.is_neg_int","title":"is_neg_int","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.bareiss_zero!","title":"bareiss_zero!","text":""},{"doctype":"documentation","id":"references/SciMLBase.DECache","title":"DECache","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/ModelingToolkit.round_trip_expr","title":"round_trip_expr","text":""},{"doctype":"documentation","id":"references/LinearSolve.KrylovKitJL_CG","title":"KrylovKitJL_CG","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.generate_boxes3","title":"generate_boxes3","text":""},{"doctype":"documentation","id":"references/Catalyst.chemical_arrows","title":"chemical_arrows","text":""},{"doctype":"documentation","id":"references/QuasiMonteCarlo.SobolSample","title":"SobolSample","text":"Samples using the Sobol sequence"},{"doctype":"documentation","id":"references/DiffEqFlux.jacobian_fn","title":"jacobian_fn","text":""},{"doctype":"document","id":"Integrals/solvers/IntegralSolvers.md","title":"Integral Solver Algorithms","text":"Integral Solver Algorithms The following algorithms are available QuadGKJL  Uses QuadGK.jl Requires  nout=1  and  batch=0  HCubatureJL  Uses HCubature.jl Requires  batch=0  VEGAS  Uses MonteCarloIntegration.jl Requires  nout=1  CubatureJLh  h-Cubature from Cubature.jl Requires  using IntegralsCubature  CubatureJLp  p-Cubature from Cubature.jl Requires  using IntegralsCubature  CubaVegas  Vegas from Cuba.jl Requires  using IntegralsCuba  CubaSUAVE  SUAVE from Cuba.jl Requires  using IntegralsCuba  CubaDivonne  Divonne from Cuba.jl Requires  using IntegralsCuba  CubaCuhre  Cuhre from Cuba.jl Requires  using IntegralsCuba "},{"doctype":"documentation","id":"references/PolyChaos.GaussMeasure","title":"GaussMeasure","text":""},{"doctype":"documentation","id":"references/SciMLBase.numargs","title":"numargs","text":"DocStringExtensions.MethodSignatures Returns the number of arguments of  f  for the method which has the most arguments"},{"doctype":"documentation","id":"references/NeuralPDE.get_loss_function","title":"get_loss_function","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.AbstractTensorProductLayer","title":"AbstractTensorProductLayer","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractConstantLagDDEProblem","title":"AbstractConstantLagDDEProblem","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/ModelingToolkit.SDEProblemExpr","title":"SDEProblemExpr","text":"Generates a Julia expression for constructing an ODEProblem from an ODESystem and allows for automatically symbolically calculating numerical enhancements"},{"doctype":"documentation","id":"references/LinearSolve.SVDFactorization","title":"SVDFactorization","text":""},{"doctype":"documentation","id":"references/SciMLOperators.TensorProductOperator","title":"TensorProductOperator","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractSDEIntegrator","title":"AbstractSDEIntegrator","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.torn_system_jacobian_sparsity","title":"torn_system_jacobian_sparsity","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.ExpMethodGeneric","title":"ExpMethodGeneric","text":"Generic exponential implementation of the method  ExpMethodHigham2005  for any exp argument  x   for which the functions  LinearAlgebra.opnorm           and    including addition with UniformScaling objects are defined The type  T  is used to adjust the number of terms used in the Pade approximants at compile time See The Scaling and Squaring Method for the Matrix Exponential Revisited by Higham Nicholas J in 2005 for algorithm details"},{"doctype":"documentation","id":"references/SciMLBase.interpolant!","title":"interpolant!","text":"Hairer Norsett Wanner Solving Ordinary Differential Euations I  Nonstiff Problems Page 190 Hermite Interpolation Hermite Interpolation Hermite Interpolation Hermite Interpolation Linear Interpolation Linear Interpolation Constant Interpolation Constant Interpolation"},{"doctype":"documentation","id":"references/DiffEqSensitivity.ZygoteAdjoint","title":"ZygoteAdjoint","text":"ZygoteAdjoint  AbstractAdjointSensitivityAlgorithm An implementation of discrete adjoint sensitivity analysis using the Zygote.jl source-to-source AD directly on the differential equation solver Constructor SciMLProblem Support Currently fails on almost every solver"},{"doctype":"documentation","id":"references/Catalyst.find_species_in_rate!","title":"find_species_in_rate!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.bound_inputs","title":"bound_inputs","text":"Return inputs that are bound within the system i.e internal inputs See also  bound_inputs   unbound_inputs   bound_outputs   unbound_outputs"},{"doctype":"documentation","id":"references/Integrals.ReverseDiffVJP","title":"ReverseDiffVJP","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractOptimizationProblem","title":"AbstractOptimizationProblem","text":"DocStringExtensions.TypeDefinition Base for types which define equations for optimization"},{"doctype":"documentation","id":"references/Surrogates.LinearSurrogate","title":"LinearSurrogate","text":"LinearSurrogate(x,y,lb,ub Builds a linear surrogate using GLM.jl"},{"doctype":"documentation","id":"references/DiffEqFlux.NeuralODE","title":"NeuralODE","text":"model tspan alg nothing args kwargs model tspan alg nothing args sensealg autojacvec kwargs Constructs a continuous-time recurrant neural network also known as a neural ordinary differential equation neural ODE with a fast gradient calculation via adjoints 1 At a high level this corresponds to solving the forward differential equation using a second differential equation that propagates the derivatives of the loss backwards in time Arguments model  A Chain or FastChain neural network that defines the ̇x tspan  The timespan to be solved on alg  The algorithm used to solve the ODE Defaults to  nothing  i.e the default algorithm from DifferentialEquations.jl sensealg  The choice of differentiation algorthm used in the backpropogation Defaults to an adjoint method and with  FastChain  it defaults to utilizing a tape-compiled ReverseDiff vector-Jacobian product for extra efficiency Seee the  Local Sensitivity Analysis  documentation for more details kwargs  Additional arguments splatted to the ODE solver See the  Common Solver Arguments  documentation for more details References 1 Pontryagin Lev Semenovich Mathematical theory of optimal processes CRC press 1987"},{"doctype":"documentation","id":"references/ModelingToolkit.push_vars!","title":"push_vars!","text":""},{"doctype":"documentation","id":"references/Surrogates.linearRadial","title":"linearRadial","text":""},{"doctype":"documentation","id":"references/PolyChaos.getentry","title":"getentry","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.compute_coeffs","title":"compute_coeffs","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.Stegr","title":"Stegr","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.NeuralCDDE","title":"NeuralCDDE","text":"model tspan lags alg nothing args sensealg kwargs model tspan lags alg nothing args sensealg kwargs Constructs a neural delay differential equation neural DDE with constant delays Arguments model  A Chain or FastChain neural network that defines the derivative function Should take an input of size  x;x(t-lag_1);...;x(t-lag_n  and produce and output shaped like  x  tspan  The timespan to be solved on hist  Defines the history function  h(t  for values before the start of the integration lags  Defines the lagged values that should be utilized in the neural network alg  The algorithm used to solve the ODE Defaults to  nothing  i.e the default algorithm from DifferentialEquations.jl sensealg  The choice of differentiation algorthm used in the backpropogation Defaults to using reverse-mode automatic differentiation via Tracker.jl kwargs  Additional arguments splatted to the ODE solver See the  Common Solver Arguments  documentation for more details"},{"doctype":"documentation","id":"references/DiffEqSensitivity.copy_to_integrator!","title":"copy_to_integrator!","text":""},{"doctype":"document","id":"SciMLBase/interfaces/Common_Keywords.md","title":"Common Keyword Arguments","text":"Common Keyword Arguments The following defines the keyword arguments which are meant to be preserved throughout all of the SciMLProblem cases where applicable Default Algorithm Hinting To help choose the default algorithm the keyword argument  alg_hints  is provided to  solve   alg_hints  is a  Vector{Symbol  which describe the problem at a high level to the solver The options are This functionality is derived via the benchmarks in  SciMLBenchmarks.jl Currently this is only implemented for the differential equation solvers Output Control These arguments control the output behavior of the solvers It defaults to maximum output to give the best interactive user experience but can be reduced all the way to only saving the solution at the final timepoint The following options are all related to output control See the Examples section at the end of this page for some example usage dense  Denotes whether to save the extra pieces required for dense continuous output Default is  save_everystep  isempty(saveat  for algorithms which have the ability to produce dense output i.e by default it's  true  unless the user has turned off saving on steps or has chosen a  saveat  value If  dense=false  the solution still acts like a function and  sol(t  is a linear interpolation between the saved time points saveat  Denotes specific times to save the solution at during the solving phase The solver will save at each of the timepoints in this array in the most efficient manner available to the solver If only  saveat  is given then the arguments  save_everystep  and  dense  are  false  by default If  saveat  is given a number then it will automatically expand to  tspan[1]:saveat:tspan[2  For methods where interpolation is not possible  saveat  may be equivalent to  tstops  The default value is    save_idxs  Denotes the indices for the components of the equation to save Defaults to saving all indices For example if you are solving a 3-dimensional ODE and given  save_idxs  1 3  only the first and third components of the solution will be outputted Notice that of course in this case the outputed solution will be two-dimensional tstops  Denotes  extra  times that the timestepping algorithm must step to This should be used to help the solver deal with discontinuities and singularities since stepping exactly at the time of the discontinuity will improve accuracy If a method cannot change timesteps fixed timestep multistep methods then  tstops  will use an interpolation matching the behavior of  saveat  If a method cannot change timesteps and also cannot interpolate then  tstops  must be a multiple of  dt  or else an error will be thrown Default is    d_discontinuities  Denotes locations of discontinuities in low order derivatives This will force FSAL algorithms which assume derivative continuity to re-evaluate the derivatives at the point of discontinuity The default is    save_everystep  Saves the result at every step Default is true if  isempty(saveat  save_on  Denotes whether intermediate solutions are saved This overrides the settings of  dense   saveat  and  save_everystep  and is used by some applicatioins to manually turn off saving temporarily Everyday use of the solvers should leave this unchanged Defaults to  true  save_start  Denotes whether the initial condition should be included in the solution type as the first timepoint Defaults to  true  save_end  Denotes whether the final timepoint is forced to be saved regardless of the other saving settings Defaults to  true  initialize_save  Denotes whether to save after the callback initialization phase when  u_modified=true  Defaults to  true  Note that  dense  requires  save_everystep=true  and  saveat=false  Stepsize Control These arguments control the timestepping routines Basic Stepsize Control adaptive  Turns on adaptive timestepping for appropriate methods Default is true abstol  Absolute tolerance in adaptive timestepping This is the tolerance on local error estimates not necessarily the global error though these quantities are related reltol  Relative tolerance in adaptive timestepping  This is the tolerance on local error estimates not necessarily the global error though these quantities are related dt  Sets the initial stepsize This is also the stepsize for fixed timestep methods Defaults to an automatic choice if the method is adaptive dtmax  Maximum dt for adaptive timestepping Defaults are package-dependent dtmin  Minimum dt for adaptive timestepping Defaults are package-dependent Fixed Stepsize Usage Note that if a method does not have adaptivity the following rules apply If  dt  is set then the algorithm will step with size  dt  each iteration If  tstops  and  dt  are both set then the algorithm will step with either a size  dt  or use a smaller step to hit the  tstops  point If  tstops  is set without  dt  then the algorithm will step directly to each value in  tstops If neither  dt  nor  tstops  are set the solver will throw an error Memory Optimizations alias_u0  allows the solver to alias the initial condition array that is contained in the problem struct Defaults to false cache  pass a solver cache to decrease the construction time This is not implemented for any of the problem interfaces at this moment Miscellaneous maxiters  Maximum number of iterations before stopping callback  Specifies a callback function that is called between iterations verbose  Toggles whether warnings are thrown when the solver exits early Defaults to true Progress Monitoring These arguments control the usage of the progressbar in the logger progress  Turns on/off the Juno progressbar Default is false progress_steps  Numbers of steps between updates of the progress bar Default is 1000 progress_name  Controls the name of the progressbar Default is the name of the problem type progress_message  Controls the message with the progressbar Defaults to showing  dt   t  the maximum of  u  The progress bars all use the Julia Logging interface in order to be generic to the IDE or programming tool that is used For more information on how this is all put together see  this discussion  Error Calculations If you are using the test problems i.e  SciMLFunction s where  f.analytic  is defined then options control the errors which are calculated By default any cheap error estimates are always calculated Extra keyword arguments include timeseries_errors dense_errors for specifying more expensive errors Automatic Differentiation Control See the Automatic Differentiation page for a full description of  sensealg   sensealg"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.accept_step!","title":"accept_step!","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.isdifference","title":"isdifference","text":""},{"doctype":"documentation","id":"references/LinearSolve.do_factorization","title":"do_factorization","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.check_variables","title":"check_variables","text":""},{"doctype":"documentation","id":"references/Catalyst.recursive_find_reactants!","title":"recursive_find_reactants!","text":""},{"doctype":"documentation","id":"references/SciMLBase.has_ldiv","title":"has_ldiv","text":""},{"doctype":"documentation","id":"references/GlobalSensitivity.Sobol","title":"Sobol","text":""},{"doctype":"documentation","id":"references/Surrogates._coeff_1d","title":"_coeff_1d","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractODEProblem","title":"AbstractODEProblem","text":"DocStringExtensions.TypeDefinition Base for types which define ODE problems"},{"doctype":"documentation","id":"references/ModelingToolkit.renamespace","title":"renamespace","text":""},{"doctype":"documentation","id":"references/NonlinearSolve.scalar_nlsolve_ad","title":"scalar_nlsolve_ad","text":""},{"doctype":"documentation","id":"references/Catalyst.NetworkProperties","title":"NetworkProperties","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractSDDEFunction","title":"AbstractSDDEFunction","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/Catalyst.make_reaction_system","title":"make_reaction_system","text":""},{"doctype":"documentation","id":"references/Surrogates.VariableFidelitySurrogate","title":"VariableFidelitySurrogate","text":""},{"doctype":"documentation","id":"references/SymbolicNumericIntegration.is_var","title":"is_var","text":""},{"doctype":"documentation","id":"references/Catalyst.error_if_constraints","title":"error_if_constraints","text":""},{"doctype":"documentation","id":"references/PolyChaos.multi2uni","title":"multi2uni","text":""},{"doctype":"documentation","id":"references/DiffEqFlux.AugmentedNDELayer","title":"AugmentedNDELayer","text":"nde adim Int Constructs an Augmented Neural Differential Equation Layer Arguments nde  Any Neural Differential Equation Layer adim  The number of dimensions the initial conditions should be lifted References 1 Dupont Emilien Arnaud Doucet and Yee Whye Teh Augmented neural ODEs In Proceedings of the 33rd International Conference on Neural Information Processing Systems pp 3140-3150 2019"},{"doctype":"document","id":"NonlinearSolve/index.md","title":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","text":"Pkg Pkg add NonlinearSolve.jl High-Performance Unified Nonlinear Solvers NonlinearSolve.jl is a unified interface for the nonlinear solving packages of Julia It includes its own high-performance nonlinear solvers which include the ability to swap out to fast direct and iterative linear solvers along with the ability to use sparse automatic differentiation for Jacobian construction and Jacobian-vector products It interfaces with other packages of the Julia ecosystem to make it easy to test alternative solver packages and pass small types to control algorithm swapping It also interfaces with the  ModelingToolkit.jl  world of symbolic modeling to allow for automatically generating high-performance code Performance is key the current methods are made to be highly performant on scalar and statically sized small problems with options for large-scale systems If you run into any performance issues please file an issue Note that this package is distinct from  SciMLNLSolve.jl  Consult the NonlinearSystemSolvers  nonlinearsystemsolvers page for information on how to import solvers from different packages Installation To install NonlinearSolve.jl use the Julia package manager Contributing Please refer to the  SciML ColPrac Contributor's Guide on Collaborative Practices for Community Packages  for guidance on PRs issues and other matters relating to contributing to ModelingToolkit There are a few community forums the diffeq-bridged channel in the  Julia Slack JuliaDiffEq  on Gitter on the Julia Discourse forums look for the  modelingtoolkit tag see also  SciML Community page Roadmap The current algorithms should support automatic differentiation though improved adjoint overloads are planned to be added in the current update which will make use of the  f(u,p  form Future updates will include standard methods for larger scale nonlinear solving like Newton-Krylov methods"},{"doctype":"documentation","id":"references/PolyChaos.computeSP","title":"computeSP","text":"Univariate Multivariate Computes the scalar product langle phi_{a_1},\\phi_{a_2},\\cdots,\\phi_{a_n rangle where  n  length(a  This requires to provide the recurrence coefficients  α,β  and the quadrature rule  nodes,weights  as well as the multi-index  ind  If provided via the keyword  issymmetric  symmetry of the weight function is exploited All computations of the multivariate scalar products resort back to efficient computations of the univariate scalar products Mathematically this follows from Fubini's theorem The function is dispatched to facilitate its use with  AbstractOrthoPoly  and its quadrature rule  Quad  Note Zero entries of  a  are removed automatically to simplify computations which follows from langle phi_i phi_j phi_0,\\cdots,\\phi_0 rangle  langle phi_i phi_j rangle because  phi_0  1  It is checked whether enough quadrature points are supplied to solve the integral exactly"},{"doctype":"documentation","id":"references/DiffEqOperators.BoundaryPaddedMatrix","title":"BoundaryPaddedMatrix","text":""},{"doctype":"documentation","id":"references/SciMLOperators.AddedOperator","title":"AddedOperator","text":"Lazy operator addition"},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.joint_density_function","title":"joint_density_function","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.VariableDisturbance","title":"VariableDisturbance","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.Gradient","title":"Gradient","text":""},{"doctype":"documentation","id":"references/PolyChaos.w_uniform01","title":"w_uniform01","text":""},{"doctype":"document","id":"GlobalSensitivity/methods/fractional.md","title":"Fractional Factorial Method","text":"Test f X X X X X X res1 f num_params N Fractional Factorial Method FractionalFactorial  does not have any keyword arguments Method Details Fractional Factorial method creates a design matrix by utilising Hadamard Matrix and uses it run simulations of the input model The main effects are then evaluated by dot product between the contrast for the parameter and the vector of simulation results The corresponding main effects and variance i.e square of the main effects are returned as results for Fractional Factorial method API Example"},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.solve_equation","title":"solve_equation","text":""},{"doctype":"documentation","id":"references/PolyChaos.Beta01Measure","title":"Beta01Measure","text":""},{"doctype":"documentation","id":"references/SciMLBase.DynamicalODEProblem","title":"DynamicalODEProblem","text":"f v0 u0 tspan p kwargs f1 f2 v0 u0 tspan p kwargs Defines an dynamical ordinary differential equation ODE problem Documentation Page https://diffeq.sciml.ai/stable/types/dynamical_types Dynamical ordinary differential equations such as those arising from the definition of a Hamiltonian system or a second order ODE have a special structure that can be utilized in the solution of the differential equation On this page we describe how to define second order differential equations for their efficient numerical solution Mathematical Specification of a Dynamical ODE Problem These algorithms require a Partitioned ODE of the form frac{dv}{dt  f_1(u,t \n\\frac{du}{dt  f_2(v  This is a Partitioned ODE partitioned into two groups so the functions should be specified as  f1(dv,v,u,p,t  and  f2(du,v,u,p,t  in the inplace form where  f1  is independent of  v  unless specified by the solver and  f2  is independent of  u  and  t  This includes discretizations arising from  SecondOrderODEProblem s where the velocity is not used in the acceleration function and Hamiltonians where the potential is or can be time-dependent but the kinetic energy is only dependent on  v  Note that some methods assume that the integral of  f2  is a quadratic form That means that  f2=v'*M*v  i.e  int f_2  frac{1}{2 m v^2  giving  du  v  This is equivalent to saying that the kinetic energy is related to  v^2  The methods which require this assumption will lose accuracy if this assumption is violated Methods listed make note of this requirement with Requires quadratic kinetic energy Constructor Defines the ODE with the specified functions  isinplace  optionally sets whether the function is inplace or not This is determined automatically but not inferred Parameters are optional and if not given then a  NullParameters  singleton will be used which will throw nice errors if you try to index non-existent parameters Any extra keyword arguments are passed on to the solvers For example if you set a  callback  in the problem then that  callback  will be added in every solve call Fields f1  and  f2  The functions in the ODE v0  and  u0  The initial conditions tspan  The timespan for the problem p  The parameters for the problem Defaults to  NullParameters kwargs  The keyword arguments passed onto the solves Define a dynamical ODE function from a  DynamicalODEFunction "},{"doctype":"documentation","id":"references/LinearSolve.UMFPACKFactorization","title":"UMFPACKFactorization","text":""},{"doctype":"documentation","id":"references/LinearSolve.IterativeSolversJL_GMRES","title":"IterativeSolversJL_GMRES","text":""},{"doctype":"documentation","id":"references/SciMLBase.DAESolution","title":"DAESolution","text":"DocStringExtensions.TypeDefinition Representation of the solution to an differential-algebraic equation defined by an DAEProblem DESolution Interface For more information on interacting with  DESolution  types check out the Solution Handling page of the DifferentialEquations.jl documentation https://diffeq.sciml.ai/stable/basics/solution Fields u  the representation of the DAE solution Given as an array of solutions where  u[i  corresponds to the solution at time  t[i  It is recommended in most cases one does not access  sol.u  directly and instead use the array interface described in the Solution Handling page of the DifferentialEquations.jl documentation du  the representation fo the derivatives of the DAE solution t  the time points corresponding to the saved values of the DAE solution prob  the original DAEProblem that was solved alg  the algorithm type used by the solver destats  statistics of the solver such as the number of function evaluations required number of Jacobians computed and more retcode  the return code from the solver Used to determine whether the solver solved successfully  sol.retcode  Success  whether it terminated due to a user-defined callback  sol.retcode  Terminated  or whether it exited due to an error For more details see the return code section of the DifferentialEquations.jl documentation"},{"doctype":"documentation","id":"references/SciMLBase.TimeChoiceIterator","title":"TimeChoiceIterator","text":""},{"doctype":"documentation","id":"references/Catalyst.assemble_drift","title":"assemble_drift","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.find_pivot_any","title":"find_pivot_any","text":""},{"doctype":"documentation","id":"references/Catalyst.@unpack_Subgraph","title":"@unpack_Subgraph","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.toparam","title":"toparam","text":"Maps the variable to a paramter Transform all instances of  varibales  in  ctrls  appearing as states and in equations of  sys  with similarly named  parameters  This allows  structural_simplify sys in the presence unbound inputs"},{"doctype":"documentation","id":"references/ModelingToolkit._merge","title":"_merge","text":""},{"doctype":"documentation","id":"references/DiffEqOperators.cross_product","title":"cross_product","text":""},{"doctype":"documentation","id":"references/Catalyst.get_indep_sts","title":"get_indep_sts","text":""},{"doctype":"document","id":"Catalyst/tutorials/generating_reactions_programmatically.md","title":"Smoluchowski Coagulation Equation","text":"LinearAlgebra DiffEqBase DiffEqJump Plots SpecialFunctions N Vₒ π Nₒ Vₒ uₒ V uₒ Nₒ integ x Int floor x n integ N nr N n n n n n i N push! integ i i integ i vcat vᵢ vⱼ volᵢ Vₒ vᵢ volⱼ Vₒ vⱼ sum_vᵢvⱼ vᵢ vⱼ i i B kv B volᵢ volⱼ V i C kv fill C V nr t k nr X N t pars Pair collect k kv i tspan i tspan u₀ zeros Int64 N u₀ uₒ u₀map Pair collect X u₀ rx n nr vᵢ n vⱼ n push! rx k n X vᵢ n X sum_vᵢvⱼ n push! rx k n X vᵢ n X vⱼ n X sum_vᵢvⱼ n rs rx t collect X collect k jumpsys convert rs dprob jumpsys u₀map tspan pars jprob JumpProblem jumpsys dprob Direct save_positions jsol jprob SSAStepper saveat tspan v_res t jsol t sol zeros length v_res length t i ϕ exp B Nₒ Vₒ t j v_res sol j Nₒ ϕ j ϕ j gamma j exp j ϕ i ϕ C Nₒ t j v_res sol j Nₒ ϕ j ϕ j default lw xlabel scatter ϕ jsol t uₒ label markercolor blue plot! ϕ sol Nₒ line dot blue label scatter! ϕ jsol t uₒ label markercolor orange plot! ϕ sol Nₒ line dot orange label scatter! ϕ jsol t uₒ label markercolor purple plot! ϕ sol Nₒ line dot purple label ylabel Smoluchowski Coagulation Equation This tutorial shows how to programmatically construct a  ReactionSystem  corresponding to the chemistry underlying the  Smoluchowski coagulation model  using  ModelingToolkit  Catalyst  A jump process version of the model is then constructed from the  ReactionSystem  and compared to the model's analytical solution obtained by the  method of Scott  see also  3  The Smoluchowski coagulation equation describes a system of reactions in which monomers may collide to form dimers monomers and dimers may collide to form trimers and so on This models a variety of chemical/physical processes including polymerization and flocculation We begin by importing some necessary packages Suppose the maximum cluster size is  N  We assume an initial concentration of monomers  Nₒ  and let  uₒ  denote the initial number of monomers in the system We have  nr  total reactions and label by  V  the bulk volume of the system which plays an important role in the calculation of rate laws since we have bimolecular reactions Our basic parameters are then The  Smoluchowski coagulation equation  Wikipedia page illustrates the set of possible reactions that can occur We can easily enumerate the  pair s of multimer reactants that can combine when allowing a maximal cluster size of  N  monomers We initialize the volumes of the reactant multimers as  volᵢ  and  volⱼ We next specify the rates i.e kernel at which reactants collide to form products For simplicity we allow a user-selected additive kernel or constant kernel The constants B  and  C  are adopted from Scott's paper  2 We'll store the reaction rates in  pars  as  Pair s and set the initial condition that only monomers are present at  t=0  in  u₀map  Here we generate the reactions programmatically We systematically create Catalyst  Reaction s for each possible reaction shown in the figure on  Wikipedia  When  vᵢ[n  vⱼ[n  we set the stoichiometric coefficient of the reactant multimer to two We now convert the  ReactionSystem  into a  ModelingToolkit.JumpSystem  and solve it using Gillespie's direct method For details on other possible solvers SSAs see the  DifferentialEquations.jl  documentation Lets check the results for the first three polymers/cluster sizes We compare to the analytical solution for this system For the  additive kernel  we find additive_kernel Sources https://en.wikipedia.org/wiki/Smoluchowski_coagulation_equation Scott W T 1968 Analytic Studies of Cloud Droplet Coalescence I Journal of Atmospheric Sciences 25(1 54-65 Retrieved Feb 18 2021 from https://journals.ametsoc.org/view/journals/atsc/25/1/1520-0469_1968_025_0054_asocdc_2_0_co_2.xml Ian J Laurenzi John D Bartels Scott L Diamond A General Algorithm for Exact Simulation of Multicomponent Aggregation Processes Journal of Computational Physics Volume 177 Issue 2 2002 Pages 418-449 ISSN 0021-9991 https://doi.org/10.1006/jcph.2002.7017"},{"doctype":"documentation","id":"references/SciMLBase.has_invW","title":"has_invW","text":""},{"doctype":"documentation","id":"references/RecursiveArrayTools.__parameterless_type","title":"__parameterless_type","text":""},{"doctype":"documentation","id":"references/Surrogates._hinge_mirror","title":"_hinge_mirror","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.modelingtoolkitize","title":"modelingtoolkitize","text":"DocStringExtensions.TypedMethodSignatures Generate  ODESystem  dependent variables and parameters from an  ODEProblem  DocStringExtensions.TypedMethodSignatures Generate  SDESystem  dependent variables and parameters from an  SDEProblem  DocStringExtensions.TypedMethodSignatures Generate  OptimizationSystem  dependent variables and parameters from an  OptimizationProblem  DocStringExtensions.TypedMethodSignatures Generate  NonlinearSystem  dependent variables and parameters from an  NonlinearProblem "},{"doctype":"documentation","id":"references/SciMLBase.SENSITIVITY_INTERP_MESSAGE","title":"SENSITIVITY_INTERP_MESSAGE","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.get_jac","title":"get_jac","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.readable_code","title":"readable_code","text":""},{"doctype":"documentation","id":"references/SciMLBase.UDerivativeWrapper","title":"UDerivativeWrapper","text":""},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.REAL_WHITE_NOISE_DIST","title":"REAL_WHITE_NOISE_DIST","text":""},{"doctype":"documentation","id":"references/LinearSolve.IterativeSolversJL","title":"IterativeSolversJL","text":""},{"doctype":"documentation","id":"references/GlobalSensitivity._unskew_S1","title":"_unskew_S1","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.has_bcs","title":"has_bcs","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.StructuralTransformations.substitution_graph","title":"substitution_graph","text":""},{"doctype":"documentation","id":"references/Integrals.scale_x","title":"scale_x","text":""},{"doctype":"documentation","id":"references/LinearSolve.KrylovJL_BICGSTAB","title":"KrylovJL_BICGSTAB","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.alias_elimination","title":"alias_elimination","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.bareiss_update_virtual_colswap_mtk!","title":"bareiss_update_virtual_colswap_mtk!","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.@diagview","title":"@diagview","text":""},{"doctype":"documentation","id":"references/MethodOfLines.AbstractGrid","title":"AbstractGrid","text":""},{"doctype":"documentation","id":"references/SciMLBase.AbstractPDEProblem","title":"AbstractPDEProblem","text":"DocStringExtensions.TypeDefinition Base for types which define PDE problems"},{"doctype":"documentation","id":"references/SciMLBase.isinplace","title":"isinplace","text":"Check whether a function operates in place by comparing its number of arguments to the expected number The second parameter is optional if  f  is an  AbstractDiffEqFunction   AbstractDiffEqFunction See also  numargs   numargs Determine whether the function of the given problem operates in place or not"},{"doctype":"document","id":"NonlinearSolve/basics/NonlinearFunctions.md","title":"[NonlinearFunctions and Jacobian Types]( nonlinearfunctions)","text":"NonlinearFunctions and Jacobian Types  nonlinearfunctions The SciML ecosystem provides an extensive interface for declaring extra functions associated with the differential equation's data In traditional libraries there is usually only one option the Jacobian However we allow for a large array of pre-computed functions to speed up the calculations This is offered via the  NonlinearFunction  types which can be passed to the problems Function Type Definitions"},{"doctype":"documentation","id":"references/DiffEqSensitivity.FakeIntegrator","title":"FakeIntegrator","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.NILSAS","title":"NILSAS","text":"nseg nstep M nothing rng Xorshifts Xoroshiro128Plus rand UInt64 adjoint_sensealg chunk_size autodiff Val central autojacvec autodiff g nothing NILSAS  AbstractShadowingSensitivityAlgorithm An implementation of the adjoint-mode continuous  non-intrusive adjoint least squares shadowing  method  NILSAS  allows for computing sensitivities of long-time averaged quantities with respect to the parameters of an  ODEProblem  by constraining the computation to the unstable subspace  NILSAS  employs SciMLSensitivity.jl's continuous adjoint sensitivity methods on each segment to compute homogenous and inhomogenous adjoint solutions To avoid an exponential blow-up of the adjoint solutions the trajectory should be divided into sufficiently small segments where the adjoint solutions are rescaled on the interfaces The computational and memory cost of NILSAS scale with the number of unstable adjoint Lyapunov exponents instead of the number of states as in the LSS method  NILSAS  avoids the explicit construction of the Jacobian at each time step and thus should generally be preferred for large system sizes over  AdjointLSS   NILSAS  is favourable over  NILSS  for many parameters because NILSAS computes the gradient with respect to multiple parameters with negligible additional cost Constructor Arguments nseg  Number of segments on full time interval on the attractor nstep  number of steps on each segment M  number of homogenous adjoint solutions This number must be bigger or equal than the number of positive adjoint Lyapunov exponents Default is  nothing  Keyword Arguments rng  Pseudo random number generator Used for initializing the terminate conditions of the homogenous adjoint states  w  Default is  Xorshifts.Xoroshiro128Plus(rand(UInt64  adjoint_sensealg  Continuous adjoint sensitivity method to compute homogenous and inhomogenous adjoint solutions on each segment Default is  BacksolveAdjoint  autodiff  Use automatic differentiation for constructing the Jacobian if the Jacobian needs to be constructed  Defaults to  true  chunk_size  Chunk size for forward-mode differentiation if full Jacobians are built  autojacvec=false  and  autodiff=true  Default is  0  for automatic choice of chunk size diff_type  The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with  autodiff=false  autojacvec  Calculate the vector-Jacobian product  J'*v  via automatic differentiation with special seeding The default is  true  The total set of choices are false  the Jacobian is constructed via FiniteDiff.jl true  the Jacobian is constructed via ForwardDiff.jl TrackerVJP  Uses Tracker.jl for the vjp ZygoteVJP  Uses Zygote.jl for the vjp EnzymeVJP  Uses Enzyme.jl for the vjp ReverseDiffVJP(compile=false  Uses ReverseDiff.jl for the vjp  compile  is a boolean for whether to precompile the tape which should only be done if there are no branches  if  or  while  statements in the  f  function g  instantaneous objective function of the long-time averaged objective SciMLProblem Support This  sensealg  only supports  ODEProblem s This  sensealg  does not support events callbacks This  sensealg  assumes that the objective is a long-time averaged quantity and ergodic i.e the time evolution of the system behaves qualitatively the same over infinite time independent of the specified initial conditions such that only the sensitivity with respect to the parameters is of interest References Ni A and Talnikar C Adjoint sensitivity analysis on chaotic dynamical systems by Non-Intrusive Least Squares Adjoint Shadowing NILSAS Journal of Computational Physics 395 690-709 2019"},{"doctype":"documentation","id":"references/MethodOfLines.isupper","title":"isupper","text":""},{"doctype":"document","id":"DiffEqOperators/nonlinear_derivatives/nonlinear_diffusion.md","title":"Nonlinear Diffusion","text":"second_differential_order Int first_differential_order Int approx_order Int p AbstractVector T q AbstractVector T dx Union T AbstractVector T Real nknots Int T Real N du AbstractVector T second_differential_order Int first_differential_order Int approx_order Int p AbstractVector T q AbstractVector T dx Union T AbstractVector T Real nknots Int T Real N Nonlinear Diffusion This function handles expressions of the form  ðₙ(D(ðₘu  where  n,m  0  and  D  is a function of  u  i.e they vary as  u(x,t  and  D(u  The expansion can be carried out via  general Leibniz rule  A boundary condition operator  bc  is first operated on  u  resulting in a boundary padded vector  bc*u  Since  D  is a function of  u  its discrete values can be obtained at grid points once  u  has been padded  After producing these two functions in the grid range we can expand the given expression via binomial expansion through the  nonlinear_diffusion  and  nonlinear_diffusion  functions and produce the final discretized derivatives Expressions for general Leibnuz rule with varying m The functions implicitly put the  CenteredDifference  operator to use for computing derivates of various orders e.g  uᵏ  CenteredDifference(k,approx_order,dx,nknots)*u  helping us generate a symmetric discretization The two functions differ in terms of memory allocation since the non   one will allocate memory to the output whereas the    one can be used for non-allocating applications Functions The two functions are as follows  Arguments  du   an input  AbstractVector  similar to  u  to store the final discretized expression second_differential_order   the overall order of derivative on the expression n first_differential_order   the inner order of derivative to discretize for  u  m approx_order   the order of the discretization in terms of O(dx^order p   boundary padded  D  q   boundary padded  u  obtained by  bc*u  dx  spacing of the discretization If  dx  is a  Number  the discretization is uniform If  dx  is an array then the discretization is non-uniform nknots   the length of discretization in the direction of operator"},{"doctype":"documentation","id":"references/Surrogates.PolyChaosStructure","title":"PolyChaosStructure","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.BipartiteGraph","title":"BipartiteGraph","text":"ne srcverts depverts fadjlist badjlist bg fadjlist badjlist DocStringExtensions.TypeDefinition A bipartite graph representation between two possibly distinct sets of vertices source and dependencies Maps source vertices labelled  1:N₁  to vertices on which they depend labelled  1:N₂  Fields DocStringExtensions.TypeFields(false Example DocStringExtensions.MethodSignatures Build an empty  BipartiteGraph  with  nsrcs  sources and  ndsts  destinations"},{"doctype":"documentation","id":"references/SciMLBase.sensitivity_solution","title":"sensitivity_solution","text":""},{"doctype":"document","id":"ModelingToolkit/tutorials/nonlinear.md","title":"Modeling Nonlinear Systems","text":"Modeling Nonlinear Systems In this example we will go one step deeper and showcase the direct function generation capabilities in ModelingToolkit.jl to build nonlinear systems Let's say we wanted to solve for the steady state of the previous ODE This is the nonlinear system defined by where the derivatives are zero We use unknown variables for our nonlinear system We can similarly ask to generate the  NonlinearProblem  with the analytical Jacobian function"},{"doctype":"documentation","id":"references/SciMLBase.set_reltol!","title":"set_reltol!","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.ConditionUWrapper","title":"ConditionUWrapper","text":""},{"doctype":"documentation","id":"references/DiffEqSensitivity.ForwardLSSProblem","title":"ForwardLSSProblem","text":""},{"doctype":"document","id":"PoissonRandom/pois_rand.md","title":"Poisson Random API","text":"Poisson Random API"},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.SRC","title":"SRC","text":""},{"doctype":"documentation","id":"references/SciMLBase.observed","title":"observed","text":""},{"doctype":"documentation","id":"references/ExponentialUtilities.RHO_V","title":"RHO_V","text":""},{"doctype":"documentation","id":"references/ModelingToolkit.BipartiteGraphs.nsrcs","title":"nsrcs","text":""},{"doctype":"documentation","id":"references/SciMLBase.DEFAULT_REDUCTION","title":"DEFAULT_REDUCTION","text":""},{"doctype":"documentation","id":"references/SciMLOperators.AbstractSciMLLinearOperator","title":"AbstractSciMLLinearOperator","text":"DocStringExtensions.TypeDefinition"},{"doctype":"documentation","id":"references/SciMLBase.has_destats","title":"has_destats","text":""},{"doctype":"documentation","id":"references/Catalyst.get_constraints","title":"get_constraints","text":"Return the current constraint subsystem if none is defined will return  nothing "},{"doctype":"documentation","id":"references/Catalyst.addparam!","title":"addparam!","text":"Given a  ReactionSystem  add the parameter corresponding to the variable  p  to the network if it is not already defined Returns the integer id of the parameter within the system disablechecks  will disable checking for whether the passed in variable is already defined which is useful when adding many new variables to the system  Do not disable checks  unless you are sure the passed in variable is a new variable as this will potentially leave the system in an undefined state Given a  ReactionSystem  add the parameter corresponding to the variable  p  to the network if it is not already defined Returns the integer id of the parameter within the system disablechecks  will disable checking for whether the passed in variable is already defined which is useful when adding many new variables to the system  Do not disable checks  unless you are sure the passed in variable is a new variable as this will potentially leave the system in an undefined state"},{"doctype":"document","id":"LinearSolve/advanced/developing.md","title":"Developing New Linear Solvers","text":"MyLUFactorization P alg MyLUFactorization A b u Pl Pr maxiters abstol reltol verbose lu! convert AbstractMatrix A cache alg MyLUFactorization kwargs cache isfresh A convert AbstractMatrix A fact lu! A cache cache fact y ldiv! cache u cache cacheval cache b alg y nothing cache alg MyLUFactorization A b u Pl Pr maxiters abstol reltol verbose Developing New Linear Solvers Developing new or custom linear solvers for the SciML interface can be done in one of two ways You can either create a completely new set of dispatches for  init  and  solve  You can extend LinearSolve.jl's internal mechanisms For developer ease we highly recommend 2 as that will automatically make the caching API work Thus this is the documentation for how to do that Developing New Linear Solvers with LinearSolve.jl Primitives Let's create a new wrapper for a simple LU-factorization which uses only the basic machinery A simplified version is The way this works is as follows LinearSolve.jl has a  LinearCache  that everything shares this is what gives most of the ease of use However many algorithms need to cache their own things and so there's one value  cacheval  that is for the algorithms to modify The function is what is called at  init  time to create the first  cacheval  Note that this should match the type of the cache later used in  solve  as many algorithms like those in OrdinaryDiffEq.jl expect type-groundedness in the linear solver definitions While there are cheaper ways to obtain this type for LU factorizations specifically  ArrayInterfaceCore.lu_instance(A  for a demonstration this just performs an LU-factorization to get an  LU{T Matrix{T  which it puts into the  cacheval  so its typed for future use After the  init_cacheval  the only thing left to do is to define  SciMLBase.solve(cache::LinearCache alg::MyLUFactorization  Many algorithms may use a lazy matrix-free representation of the operator  A  Thus if the algorithm requires a concrete matrix like LU-factorization does the algorithm should  convert(AbstractMatrix,cache.A  The flag  cache.isfresh  states whether  A  has changed since the last  solve  Since we only need to factorize when  A  is new the factorization part of the algorithm is done in a  if cache.isfresh   cache  set_cacheval(cache fact  puts the new factorization into the cache so it's updated for future solves Then  y  ldiv!(cache.u cache.cacheval cache.b  performs the solve and a linear solution is returned via  SciMLBase.build_linear_solution(alg,y,nothing,cache "},{"doctype":"documentation","id":"references/DiffEqNoiseProcess.linear_interpolant","title":"linear_interpolant","text":""},{"doctype":"documentation","id":"references/Optimization.check_pkg_version","title":"check_pkg_version","text":""},{"doctype":"documentation","id":"references/Catalyst.ExprValues","title":"ExprValues","text":""},{"doctype":"documentation","id":"references/SciMLBase.ODEFunction","title":"ODEFunction","text":"iip recompile f mass_matrix I analytic nothing tgrad nothing jac nothing jvp nothing vjp nothing jac_prototype nothing sparsity jac_prototype paramjac nothing syms nothing indepsym nothing colorvec nothing LinearAlgebra f du u p t du t u jac J u p t J t J t J jp Diagonal zeros fun f jac jac jac_prototype jp f du u p t du u u u du u u u f_jac J u p t J u J u J u J u nothing ff f jac f_jac prob ff ones ODEFunction  AbstractODEFunction A representation of an ODE function  f  defined by M frac{du}{dt  f(u,p,t and all of its related functions such as the Jacobian of  f  its gradient with respect to time and more For all cases  u0  is the initial condition  p  are the parameters and  t  is the independent variable Constructor Note that only the function  f  itself is required This function should be given as  f!(du,u,p,t  or  du  f(u,p,t  See the section on  iip  for more details on in-place vs out-of-place handling All of the remaining functions are optional for improving or accelerating  the usage of  f  These include mass_matrix  the mass matrix  M  represented in the ODE function Can be used to determine that the equation is actually a differential-algebraic equation DAE if  M  is singular Note that in this case special solvers are required see the DAE solver page for more details https://diffeq.sciml.ai/stable/solvers/dae_solve Must be an AbstractArray or an AbstractSciMLOperator analytic(u0,p,t  used to pass an analytical solution function for the analytical  solution of the ODE Generally only used for testing and development of the solvers tgrad(dT,u,p,t  or dT=tgrad(u,p,t returns  frac{\\partial f(u,p,t)}{\\partial t jac(J,u,p,t  or  J=jac(u,p,t  returns  frac{df}{du jvp(Jv,v,u,p,t  or  Jv=jvp(v,u,p,t  returns the directional derivative frac{df}{du v vjp(Jv,v,u,p,t  or  Jv=vjp(v,u,p,t  returns the adjoint derivative frac{df}{du}^\\ast v jac_prototype  a prototype matrix matching the type that matches the Jacobian For example if the Jacobian is tridiagonal then an appropriately sized  Tridiagonal  matrix can be used as the prototype and integrators will specialize on this structure where possible Non-structured sparsity patterns should use a  SparseMatrixCSC  with a correct sparsity pattern for the Jacobian The default is  nothing  which means a dense Jacobian paramjac(pJ,u,p,t  returns the parameter Jacobian  frac{df}{dp  syms  the symbol names for the elements of the equation This should match  u0  in size For example if  u0  0.0,1.0  and  syms  x y  this will apply a canonical naming to the values allowing  sol[:x  in the solution and automatically naming values in plots indepsym  the canonical naming for the independent variable Defaults to nothing which internally uses  t  as the representation in any plots colorvec  a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the  jac_prototype  This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern Defaults to  nothing  which means a color vector will be internally computed on demand when required The cost of this operation is highly dependent on the sparsity pattern iip In-Place vs Out-Of-Place iip  is the the optional boolean for determining whether a given function is written to be used in-place or out-of-place In-place functions are  f!(du,u,p,t  where the return is ignored and the result is expected to be mutated into the value of  du  Out-of-place functions are  du=f(u,p,t  Normally this is determined automatically by looking at the method table for  f  and seeing the maximum number of arguments in available dispatches For this reason the constructor  ODEFunction(f  generally works but is type-unstable However for type-stability or to enforce correctness this option is passed via  ODEFunction{true}(f  recompile Controlling Compilation and Specialization The  recompile  parameter controls whether the ODEFunction will fully specialize on the  typeof(f  This causes recompilation of the solver for each new  f  function but gives the maximum compiler information and runtime speed By default  recompile  true  If  recompile  false  the  ODEFunction  uses  Any  type parameters for each of the functions allowing for the reuse of compilation caches but adding a dynamic dispatch at the  f  call sites potentially leading to runtime regressions Overriding the  true  default is done by passing a second type parameter after  iip  for example  ODEFunction{true,false}(f  is an in-place function with no recompilation specialization Fields The fields of the ODEFunction type directly match the names of the inputs More Details on Jacobians The following example creates an inplace  ODEFunction  whose jacobian is a  Diagonal  Note that the integrators will always make a deep copy of  fun.jac_prototype  so there's no worry of aliasing In general the jacobian prototype can be anything that has  mul  defined in particular sparse matrices or custom lazy types that support  mul  A special case is when the  jac_prototype  is a  AbstractDiffEqLinearOperator  in which case you do not need to supply  jac  as it is automatically set to  update_coefficients  Refer to the AbstractSciMLOperators documentation for more information on setting up time/parameter dependent operators Examples Declaring Explicit Jacobians for ODEs The most standard case declaring a function for a Jacobian is done by overloading the function  f(du,u,p,t  with an in-place updating function for the Jacobian  f_jac(J,u,p,t  where the value type is used for dispatch For example take the LotkaVolterra model To declare the Jacobian we simply add the dispatch Then we can supply the Jacobian with our ODE as and use this in an  ODEProblem  Symbolically Generating the Functions See the  modelingtoolkitize  function from  ModelingToolkit.jl  for automatically symbolically generating the Jacobian and more from the  numerically-defined functions"},{"doctype":"documentation","id":"references/DiffEqFlux.initial_params","title":"initial_params","text":""}]