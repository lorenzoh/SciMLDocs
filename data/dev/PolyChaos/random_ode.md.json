{"attributes":{"backlinks":[],"path":"/Users/lorenz/.julia/packages/PolyChaos/THVqe/docs/src/random_ode.md","title":"Galerkin-based Solution of Random Differential Equation"},"tag":"document","children":[{"attributes":{},"tag":"md","children":[{"attributes":{"lang":"@setup mysetup"},"tag":"codeblock","children":["using PolyChaos, DifferentialEquations\nx0 = 2.0\nμ, σ = -0.5, 0.05\ntend, Δt = 3.0, 0.01\nusing PolyChaos\nL, Nrec = 6, 40\nopq = GaussOrthoPoly(L; Nrec=Nrec, addQuadrature=true)\nusing DifferentialEquations\n\na = [ convert2affinePCE(μ, σ, opq); zeros(Float64,L-1) ] # PCE coefficients of a\nxinit = [ x0; zeros(Float64,L) ] # PCE coefficients of initial condition\n\nt2 = Tensor(2, opq); # \\langle \\phi_i, \\phi_j \\rangle\nt3 = Tensor(3, opq); # \\langle \\phi_i \\phi_j, \\phi_k \\rangle\n\n# Galerkin-projected random differential equation\nfunction ODEgalerkin(du,u,p,t)\n   du[:] = [ sum( p[j+1]*u[k+1]*t3.get([j,k,m])/t2.get([m,m]) for j=0:L for k=0:L) for m=0:L ]\nend\n\nprobgalerkin = ODEProblem(ODEgalerkin,xinit,(0,tend),a)\nsolgalerkin = solve(probgalerkin;saveat=0:Δt:tend)\nt, x = solgalerkin.t, solgalerkin.u;\n# an advantage of PCE is that moments can be computed from the PCE coefficients alone; no sampling required\nmean_pce = [ mean(x_, opq) for x_ in x]  \nstd_pce = [ std(x_, opq) for x_ in x]\nusing Statistics\nNsmpl = 5000\nξ = sampleMeasure(Nsmpl,opq)     # sample from Gaussian measure; effectively randn() here    \nasmpl = evaluatePCE(a,ξ,opq)     # sample random variable with PCE coefficients a; effectively μ + σ*randn() here\n# or: asmpl = samplePCE(Nsmpl,a,opq)\nxmc = [ solve(ODEProblem((u,p,t)->aa*u,x0,(0,tend));saveat=0:Δt:tend).u for aa in asmpl]\nxmc = hcat(xmc...);\n[ mean(xmc,dims=2)-mean_pce std(xmc,dims=2)-std_pce]\nlogx_pce = [ log.(evaluatePCE(x[i],ξ,opq)) for i=1:length(t)]\n[mean.(logx_pce)-(log(x0) .+ μ*t) std.(logx_pce)-σ*t ]\n"],"type":"node"},{"attributes":{},"tag":"h1","children":["Galerkin-based Solution of Random Differential Equation"],"type":"node"},{"attributes":{},"tag":"p","children":["This tutorial demonstrates how random differential equations can be solved using polynomial chaos expansions (PCE)."],"type":"node"},{"attributes":{},"tag":"h2","children":["Theory"],"type":"node"},{"attributes":{},"tag":"p","children":["A random differential equation is an ordinary differential equation that has random parameters, hence its solution is itself a (time-varying) random variable. Perhaps the simplest non-trivial example is the following scalar, linear ordinary differential equation"],"type":"node"},{"attributes":{},"tag":"mathblock","children":["\\dot{x}(t) = a x(t), \\quad x(0) = x_{0},"],"type":"node"},{"attributes":{},"tag":"p","children":["where ",{"attributes":{},"tag":"math","children":["a"],"type":"node"}," is the realization of a Gaussian random variable ",{"attributes":{},"tag":"math","children":["\\mathsf{a} \\sim \\mathcal{N}(\\mu, \\sigma^2)"],"type":"node"}," with mean ",{"attributes":{},"tag":"math","children":["\\mu"],"type":"node"}," and variance ",{"attributes":{},"tag":"math","children":["\\sigma^2"],"type":"node"},". Arguably, for every realization ",{"attributes":{},"tag":"math","children":["a"],"type":"node"}," we can solve the differential equation and obtain"],"type":"node"},{"attributes":{},"tag":"mathblock","children":["x(t) = x_0 \\mathrm{e}^{a t},"],"type":"node"},{"attributes":{},"tag":"p","children":["from which we find that"],"type":"node"},{"attributes":{},"tag":"mathblock","children":["\\ln (x(t)) = \\ln (x_0) + at \\sim \\mathcal{N}(\\ln(x_0) + \\mu t, (\\sigma t)^2)."],"type":"node"},{"attributes":{},"tag":"p","children":["In other words, the logarithm of the solution is normally distributed (so-called ",{"attributes":{"href":"https://en.wikipedia.org/wiki/Log-normal_distribution","title":""},"tag":"a","children":["log-normal distribution"],"type":"node"},")."],"type":"node"},{"attributes":{},"tag":"p","children":["We'd like to obtain this result numerically with the help of PCE. The first step is to define the (truncated) PCE for the random variable ",{"attributes":{},"tag":"math","children":["\\mathsf{a}"],"type":"node"}],"type":"node"},{"attributes":{},"tag":"mathblock","children":["\\mathsf{a} = \\sum_{i=0}^{L} a_i \\phi_i,"],"type":"node"},{"attributes":{},"tag":"p","children":["where ",{"attributes":{},"tag":"math","children":["a_i"],"type":"node"}," are the so-called PCE coefficients, and ",{"attributes":{},"tag":"math","children":["\\phi_i"],"type":"node"}," are the orthogonal basis polynomials. As the solution to the random differential equation is itself a random variable, we treat ",{"attributes":{},"tag":"math","children":["x(t)"],"type":"node"}," as the realization of the random variable ",{"attributes":{},"tag":"math","children":["\\mathsf{x}(t)"],"type":"node"},", and define its PCE"],"type":"node"},{"attributes":{},"tag":"mathblock","children":["\\mathsf{x}(t) = \\sum_{i=0}^{L} x_i(t) \\phi_i."],"type":"node"},{"attributes":{},"tag":"p","children":["The question is how to obtain the unknown PCE coefficients ",{"attributes":{},"tag":"math","children":["x_i(t)"],"type":"node"}," from the known PCE coefficients ",{"attributes":{},"tag":"math","children":["a_i"],"type":"node"}," relative to the orthogonal basis polynomials ",{"attributes":{},"tag":"math","children":["\\phi_i"],"type":"node"},". This can be done using Galerkin projection, which is nothing else than projecting onto the orthogonal basis. Think of a three-dimensional space, in which you have placed some three-dimensional object. If you know project the silhouett of the object onto every axis of the three-dimensional space, then you are doing a Galerkin projection. With PCE the concept is equivalent, but the imagination has a harder time. The first step for Galerkin projection is to insert the PCEs"],"type":"node"},{"attributes":{},"tag":"mathblock","children":["\\sum_{i=0}^{L} \\dot{x}_i(t) \\phi_i = \\sum_{j=0}^{L} a_j \\phi_j \\sum_{k=0}^{L} x_k(t) \\phi_k;"],"type":"node"},{"attributes":{},"tag":"p","children":["the second step is to project onto every basis polynomial ",{"attributes":{},"tag":"math","children":["\\phi_m"],"type":"node"}," for ",{"attributes":{},"tag":"math","children":["m = 0, 1, \\dots, L"],"type":"node"},", and to exploit orthogonality of the basis. This gives"],"type":"node"},{"attributes":{},"tag":"mathblock","children":["\\dot{x}_m(t) \\langle \\phi_m, \\phi_m \\rangle = \\sum_{j=0}^{L} \\sum_{k=0}^{L} a_j x_k(t) \\langle \\phi_l \\phi_k, \\phi_m \\rangle \\quad m = 0, 1, \\dots, L."],"type":"node"},{"attributes":{},"tag":"p","children":["Of course, the initial condition must not be forgotten:"],"type":"node"},{"attributes":{},"tag":"mathblock","children":["x_0(0) = x_0, \\quad x_m(0) = 0 \\quad m = 1, \\dots, L."],"type":"node"},{"attributes":{},"tag":"p","children":["If we can solve this enlarged system of ordinary random differential equations, we can reconstruct the analytic solution."],"type":"node"},{"attributes":{},"tag":"h2","children":["Practice"],"type":"node"},{"attributes":{},"tag":"p","children":["We begin by defining the random differential equation"],"type":"node"},{"attributes":{"lang":"@example mysetup"},"tag":"codeblock","children":["x0 = 2.0\nμ, σ = -0.5, 0.05\ntend, Δt = 3.0, 0.01\n"],"type":"node"},{"attributes":{},"tag":"p","children":["Next, we define an orthogonal basis (and its quadrature rule) relative to the Gaussian measure using ",{"attributes":{},"tag":"code","children":["PolyChaos"],"type":"node"},". We choose a maximum degree of ",{"attributes":{},"tag":"code","children":["L"],"type":"node"},"."],"type":"node"},{"attributes":{"lang":"@example mysetup"},"tag":"codeblock","children":["using PolyChaos\nL, Nrec = 6, 40\nopq = GaussOrthoPoly(L; Nrec=Nrec, addQuadrature=true)\n"],"type":"node"},{"attributes":{},"tag":"p","children":["Now we can define the PCE for ",{"attributes":{},"tag":"math","children":["\\mathsf{a}"],"type":"node"}," and solve the Galerkin-projected ordinary differential equation using ",{"attributes":{},"tag":"code","children":["DifferentialEquations.jl"],"type":"node"},"."],"type":"node"},{"attributes":{"lang":"@example mysetup"},"tag":"codeblock","children":["using DifferentialEquations\n\na = [ convert2affinePCE(μ, σ, opq); zeros(Float64,L-1) ] # PCE coefficients of a\nxinit = [ x0; zeros(Float64,L) ] # PCE coefficients of initial condition\n\nt2 = Tensor(2, opq); # \\langle \\phi_i, \\phi_j \\rangle\nt3 = Tensor(3, opq); # \\langle \\phi_i \\phi_j, \\phi_k \\rangle\n\n# Galerkin-projected random differential equation\nfunction ODEgalerkin(du,u,p,t)\n   du[:] = [ sum( p[j+1]*u[k+1]*t3.get([j,k,m])/t2.get([m,m]) for j=0:L for k=0:L) for m=0:L ]\nend\n\nprobgalerkin = ODEProblem(ODEgalerkin,xinit,(0,tend),a)\nsolgalerkin = solve(probgalerkin;saveat=0:Δt:tend)\nt, x = solgalerkin.t, solgalerkin.u;\n"],"type":"node"},{"attributes":{},"tag":"p","children":["For later purposes we compute the expected value and the standard deviation at all time instants using PCE."],"type":"node"},{"attributes":{"lang":"@example mysetup"},"tag":"codeblock","children":["# an advantage of PCE is that moments can be computed from the PCE coefficients alone; no sampling required\nmean_pce = [ mean(x_, opq) for x_ in x]  \nstd_pce = [ std(x_, opq) for x_ in x]\n"],"type":"node"},{"attributes":{},"tag":"p","children":["We compare the solution from PCE to a Monte-Carlo-based solution. That means to solve the ordinary differential equation for many samples of ",{"attributes":{},"tag":"math","children":["\\mathsf{a}"],"type":"node"},". We first sample from the measure using ",{"attributes":{},"tag":"code","children":["sampleMeasure"],"type":"node"},", and then generate samples of ",{"attributes":{},"tag":"math","children":["\\mathsf{a}"],"type":"node"}," using ",{"attributes":{},"tag":"code","children":["evaluatePCE"],"type":"node"},". After that we solve the ODE and store the results in ",{"attributes":{},"tag":"code","children":["xmc"],"type":"node"},"."],"type":"node"},{"attributes":{"lang":"@example mysetup"},"tag":"codeblock","children":["using Statistics\nNsmpl = 5000\nξ = sampleMeasure(Nsmpl, opq)     # sample from Gaussian measure; effectively randn() here    \nasmpl = evaluatePCE(a, ξ, opq)     # sample random variable with PCE coefficients a; effectively μ + σ*randn() here\n# or: asmpl = samplePCE(Nsmpl,a,opq)\nxmc = [ solve(ODEProblem((u,p,t)->aa*u,x0,(0,tend));saveat=0:Δt:tend).u for aa in asmpl]\nxmc = hcat(xmc...);\n"],"type":"node"},{"attributes":{},"tag":"p","children":["Now we can compare the Monte Carlo mean and standard deviation to the expression from PCE for every time instant."],"type":"node"},{"attributes":{"lang":"@example mysetup"},"tag":"codeblock","children":["[ mean(xmc,dims=2)-mean_pce std(xmc,dims=2)-std_pce]\n"],"type":"node"},{"attributes":{},"tag":"p","children":["Clearly, the accuracy of PCE deteriorates over time. Possible remedies are to increase the dimension of PCE, and to tweak the tolerances of the integrator."],"type":"node"},{"attributes":{},"tag":"p","children":["Finally, we compare whether the samples follow a log-normal distribution, and compare the result to the analytic mean and standard deviation."],"type":"node"},{"attributes":{"lang":"@example mysetup"},"tag":"codeblock","children":["logx_pce = [ log.(evaluatePCE(x_,ξ,opq)) for x_ in x]\n[ mean.(logx_pce)-(log(x0) .+ μ*t) std.(logx_pce)-σ*t ]\n"],"type":"node"}],"type":"node"}],"type":"node"}